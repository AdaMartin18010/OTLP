# 30. æ€§èƒ½ä¼˜åŒ–æŒ‡å—

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0.0  
**åˆ›å»ºæ—¥æœŸ**: 2025å¹´10æœˆ7æ—¥  
**ä½œè€…**: OTLPç³»ç»Ÿåˆ†æå›¢é˜Ÿ  
**æ‰€å±éƒ¨åˆ†**: ç¬¬åéƒ¨åˆ† - å®è·µæ¡ˆä¾‹ä¸æœ€ä½³å®è·µ

---

## ğŸ“‹ ç›®å½•

- [30. æ€§èƒ½ä¼˜åŒ–æŒ‡å—](#30-æ€§èƒ½ä¼˜åŒ–æŒ‡å—)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [æ¦‚è¿°](#æ¦‚è¿°)
    - [ä¼˜åŒ–ç›®æ ‡](#ä¼˜åŒ–ç›®æ ‡)
    - [ä¼˜åŒ–åŸåˆ™](#ä¼˜åŒ–åŸåˆ™)
  - [SDKæ€§èƒ½ä¼˜åŒ–](#sdkæ€§èƒ½ä¼˜åŒ–)
    - [1. æ‰¹é‡å¤„ç†](#1-æ‰¹é‡å¤„ç†)
    - [2. å¯¹è±¡æ± åŒ–](#2-å¯¹è±¡æ± åŒ–)
    - [3. é›¶æ‹·è´ä¼˜åŒ–](#3-é›¶æ‹·è´ä¼˜åŒ–)
    - [4. é‡‡æ ·ä¼˜åŒ–](#4-é‡‡æ ·ä¼˜åŒ–)
  - [Collectoræ€§èƒ½ä¼˜åŒ–](#collectoræ€§èƒ½ä¼˜åŒ–)
    - [1. å¹¶å‘å¤„ç†](#1-å¹¶å‘å¤„ç†)
    - [2. å†…å­˜ä¼˜åŒ–](#2-å†…å­˜ä¼˜åŒ–)
    - [3. æ‰¹é‡å¯¼å‡ºä¼˜åŒ–](#3-æ‰¹é‡å¯¼å‡ºä¼˜åŒ–)
  - [å­˜å‚¨æ€§èƒ½ä¼˜åŒ–](#å­˜å‚¨æ€§èƒ½ä¼˜åŒ–)
    - [1. ClickHouseä¼˜åŒ–](#1-clickhouseä¼˜åŒ–)
    - [2. æ‰¹é‡å†™å…¥ä¼˜åŒ–](#2-æ‰¹é‡å†™å…¥ä¼˜åŒ–)
  - [æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–](#æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–)
    - [1. æŸ¥è¯¢ä¼˜åŒ–](#1-æŸ¥è¯¢ä¼˜åŒ–)
    - [2. ç´¢å¼•ä¼˜åŒ–](#2-ç´¢å¼•ä¼˜åŒ–)
  - [ç½‘ç»œæ€§èƒ½ä¼˜åŒ–](#ç½‘ç»œæ€§èƒ½ä¼˜åŒ–)
    - [1. HTTP/2ä¸gRPC](#1-http2ä¸grpc)
    - [2. å‹ç¼©ä¼˜åŒ–](#2-å‹ç¼©ä¼˜åŒ–)
  - [æ€§èƒ½ç›‘æ§ä¸è¯Šæ–­](#æ€§èƒ½ç›‘æ§ä¸è¯Šæ–­)
    - [1. æ€§èƒ½æŒ‡æ ‡](#1-æ€§èƒ½æŒ‡æ ‡)
    - [2. æ€§èƒ½å‰–æ](#2-æ€§èƒ½å‰–æ)
  - [æ€§èƒ½æµ‹è¯•](#æ€§èƒ½æµ‹è¯•)
    - [1. åŸºå‡†æµ‹è¯•](#1-åŸºå‡†æµ‹è¯•)
    - [2. æ€§èƒ½ç›®æ ‡](#2-æ€§èƒ½ç›®æ ‡)
  - [æ€»ç»“](#æ€»ç»“)

---

## æ¦‚è¿°

### ä¼˜åŒ–ç›®æ ‡

1. **ä½å»¶è¿Ÿ**: è¿½è¸ªå¼€é”€<1ms
2. **é«˜åå**: æ”¯æŒ100ä¸‡+ spans/s
3. **ä½èµ„æºæ¶ˆè€—**: CPU<5%, å†…å­˜<500MB
4. **å¯æ‰©å±•æ€§**: çº¿æ€§æ‰©å±•èƒ½åŠ›

### ä¼˜åŒ–åŸåˆ™

- **æµ‹é‡å…ˆè¡Œ**: å…ˆæµ‹é‡å†ä¼˜åŒ–
- **æ‰¾åˆ°ç“¶é¢ˆ**: è¯†åˆ«çœŸæ­£çš„ç“¶é¢ˆ
- **æƒè¡¡å–èˆ**: å¹³è¡¡æ€§èƒ½ä¸åŠŸèƒ½
- **æŒç»­ä¼˜åŒ–**: å»ºç«‹æŒç»­ä¼˜åŒ–æœºåˆ¶

---

## SDKæ€§èƒ½ä¼˜åŒ–

### 1. æ‰¹é‡å¤„ç†

```go
// é«˜æ€§èƒ½æ‰¹å¤„ç†å™¨
type HighPerformanceBatchProcessor struct {
    queue       chan sdktrace.ReadOnlySpan
    batch       []sdktrace.ReadOnlySpan
    batchSize   int
    timeout     time.Duration
    exporter    sdktrace.SpanExporter
    stopCh      chan struct{}
    wg          sync.WaitGroup
}

func NewHighPerformanceBatchProcessor(
    exporter sdktrace.SpanExporter,
    opts ...BatchOption,
) *HighPerformanceBatchProcessor {
    bp := &HighPerformanceBatchProcessor{
        queue:     make(chan sdktrace.ReadOnlySpan, 10000), // å¤§ç¼“å†²åŒº
        batch:     make([]sdktrace.ReadOnlySpan, 0, 512),
        batchSize: 512,
        timeout:   1 * time.Second,
        exporter:  exporter,
        stopCh:    make(chan struct{}),
    }
    
    // åº”ç”¨é€‰é¡¹
    for _, opt := range opts {
        opt(bp)
    }
    
    // å¯åŠ¨æ‰¹å¤„ç†åç¨‹
    bp.wg.Add(1)
    go bp.processBatch()
    
    return bp
}

func (bp *HighPerformanceBatchProcessor) processBatch() {
    defer bp.wg.Done()
    
    ticker := time.NewTicker(bp.timeout)
    defer ticker.Stop()
    
    for {
        select {
        case <-bp.stopCh:
            // åˆ·æ–°å‰©ä½™æ•°æ®
            bp.flush()
            return
            
        case span := <-bp.queue:
            bp.batch = append(bp.batch, span)
            
            // è¾¾åˆ°æ‰¹å¤§å°ï¼Œç«‹å³å‘é€
            if len(bp.batch) >= bp.batchSize {
                bp.flush()
            }
            
        case <-ticker.C:
            // è¶…æ—¶ï¼Œå‘é€å½“å‰æ‰¹æ¬¡
            if len(bp.batch) > 0 {
                bp.flush()
            }
        }
    }
}

func (bp *HighPerformanceBatchProcessor) flush() {
    if len(bp.batch) == 0 {
        return
    }
    
    // å¼‚æ­¥å¯¼å‡ºï¼ˆä¸é˜»å¡ï¼‰
    batch := bp.batch
    bp.batch = make([]sdktrace.ReadOnlySpan, 0, bp.batchSize)
    
    go func() {
        ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
        defer cancel()
        
        if err := bp.exporter.ExportSpans(ctx, batch); err != nil {
            log.Printf("Failed to export batch: %v", err)
            metrics.ExportErrors.Inc()
        } else {
            metrics.ExportedSpans.Add(float64(len(batch)))
        }
    }()
}

// OnStartå®ç°ï¼ˆå¿«é€Ÿè·¯å¾„ï¼‰
func (bp *HighPerformanceBatchProcessor) OnStart(
    parent context.Context,
    s sdktrace.ReadWriteSpan,
) {
    // ä¸åšä»»ä½•å¤„ç†ï¼Œå‡å°‘å¼€é”€
}

// OnEndå®ç°ï¼ˆå¿«é€Ÿå…¥é˜Ÿï¼‰
func (bp *HighPerformanceBatchProcessor) OnEnd(s sdktrace.ReadOnlySpan) {
    // éé˜»å¡å…¥é˜Ÿ
    select {
    case bp.queue <- s:
        // æˆåŠŸå…¥é˜Ÿ
    default:
        // é˜Ÿåˆ—æ»¡ï¼Œä¸¢å¼ƒï¼ˆè®°å½•æŒ‡æ ‡ï¼‰
        metrics.DroppedSpans.Inc()
    }
}
```

### 2. å¯¹è±¡æ± åŒ–

```go
// Spanå¯¹è±¡æ± 
var spanPool = sync.Pool{
    New: func() interface{} {
        return &SpanData{
            Attributes: make(map[string]interface{}, 16),
            Events:     make([]Event, 0, 4),
            Links:      make([]Link, 0, 2),
        }
    },
}

// è·å–Spanå¯¹è±¡
func acquireSpan() *SpanData {
    return spanPool.Get().(*SpanData)
}

// é‡Šæ”¾Spanå¯¹è±¡
func releaseSpan(span *SpanData) {
    // é‡ç½®å¯¹è±¡
    span.Reset()
    // æ”¾å›æ± ä¸­
    spanPool.Put(span)
}

// SpanDataé‡ç½®æ–¹æ³•
func (sd *SpanData) Reset() {
    sd.TraceID = ""
    sd.SpanID = ""
    sd.ParentSpanID = ""
    sd.Name = ""
    sd.StartTime = time.Time{}
    sd.EndTime = time.Time{}
    sd.Status = StatusUnset
    
    // æ¸…ç©ºmapï¼ˆä¿ç•™å®¹é‡ï¼‰
    for k := range sd.Attributes {
        delete(sd.Attributes, k)
    }
    
    // æ¸…ç©ºsliceï¼ˆä¿ç•™å®¹é‡ï¼‰
    sd.Events = sd.Events[:0]
    sd.Links = sd.Links[:0]
}

// å­—èŠ‚ç¼“å†²æ± 
var bufferPool = sync.Pool{
    New: func() interface{} {
        return new(bytes.Buffer)
    },
}

// åºåˆ—åŒ–æ—¶ä½¿ç”¨ç¼“å†²æ± 
func serializeSpan(span *SpanData) ([]byte, error) {
    buf := bufferPool.Get().(*bytes.Buffer)
    defer func() {
        buf.Reset()
        bufferPool.Put(buf)
    }()
    
    encoder := json.NewEncoder(buf)
    if err := encoder.Encode(span); err != nil {
        return nil, err
    }
    
    // æ‹·è´æ•°æ®ï¼ˆå› ä¸ºbufä¼šè¢«é‡ç”¨ï¼‰
    data := make([]byte, buf.Len())
    copy(data, buf.Bytes())
    
    return data, nil
}
```

### 3. é›¶æ‹·è´ä¼˜åŒ–

```go
// é›¶æ‹·è´Spanå¤„ç†
type ZeroCopySpanProcessor struct {
    buffer *RingBuffer
}

// ç¯å½¢ç¼“å†²åŒºï¼ˆæ— é”ï¼‰
type RingBuffer struct {
    data     []unsafe.Pointer
    size     int
    mask     int
    writePos atomic.Uint64
    readPos  atomic.Uint64
}

func NewRingBuffer(size int) *RingBuffer {
    // ç¡®ä¿sizeæ˜¯2çš„å¹‚
    if size&(size-1) != 0 {
        panic("size must be power of 2")
    }
    
    return &RingBuffer{
        data: make([]unsafe.Pointer, size),
        size: size,
        mask: size - 1,
    }
}

// å†™å…¥ï¼ˆæ— é”ï¼‰
func (rb *RingBuffer) Write(ptr unsafe.Pointer) bool {
    for {
        writePos := rb.writePos.Load()
        readPos := rb.readPos.Load()
        
        // æ£€æŸ¥æ˜¯å¦æ»¡
        if writePos-readPos >= uint64(rb.size) {
            return false // ç¼“å†²åŒºæ»¡
        }
        
        // CASæ›´æ–°å†™ä½ç½®
        if rb.writePos.CompareAndSwap(writePos, writePos+1) {
            // å†™å…¥æ•°æ®
            idx := int(writePos) & rb.mask
            atomic.StorePointer(&rb.data[idx], ptr)
            return true
        }
    }
}

// è¯»å–ï¼ˆæ— é”ï¼‰
func (rb *RingBuffer) Read() unsafe.Pointer {
    for {
        readPos := rb.readPos.Load()
        writePos := rb.writePos.Load()
        
        // æ£€æŸ¥æ˜¯å¦ç©º
        if readPos >= writePos {
            return nil // ç¼“å†²åŒºç©º
        }
        
        // CASæ›´æ–°è¯»ä½ç½®
        if rb.readPos.CompareAndSwap(readPos, readPos+1) {
            // è¯»å–æ•°æ®
            idx := int(readPos) & rb.mask
            return atomic.LoadPointer(&rb.data[idx])
        }
    }
}

// ä½¿ç”¨é›¶æ‹·è´å¤„ç†
func (zcp *ZeroCopySpanProcessor) OnEnd(s sdktrace.ReadOnlySpan) {
    // ç›´æ¥ä¼ é€’æŒ‡é’ˆï¼ˆé¿å…æ‹·è´ï¼‰
    ptr := unsafe.Pointer(&s)
    
    if !zcp.buffer.Write(ptr) {
        metrics.DroppedSpans.Inc()
    }
}
```

### 4. é‡‡æ ·ä¼˜åŒ–

```go
// å¿«é€Ÿé‡‡æ ·å™¨ï¼ˆä½è¿ç®—ï¼‰
type FastSampler struct {
    rate     float64
    threshold uint64
}

func NewFastSampler(rate float64) *FastSampler {
    return &FastSampler{
        rate:     rate,
        threshold: uint64(rate * float64(math.MaxUint64)),
    }
}

// åŸºäºTraceIDçš„ç¡®å®šæ€§é‡‡æ ·ï¼ˆæå¿«ï¼‰
func (fs *FastSampler) ShouldSample(
    p sdktrace.SamplingParameters,
) sdktrace.SamplingResult {
    // ä½¿ç”¨TraceIDçš„ä½64ä½
    traceIDLow := binary.BigEndian.Uint64(p.TraceID[8:])
    
    // ç®€å•æ¯”è¾ƒï¼ˆé¿å…æµ®ç‚¹è¿ç®—ï¼‰
    if traceIDLow <= fs.threshold {
        return sdktrace.SamplingResult{
            Decision: sdktrace.RecordAndSample,
        }
    }
    
    return sdktrace.SamplingResult{
        Decision: sdktrace.Drop,
    }
}

// è‡ªé€‚åº”é‡‡æ ·ï¼ˆåŸºäºä»¤ç‰Œæ¡¶ï¼‰
type AdaptiveSampler struct {
    targetRate float64
    bucket     *TokenBucket
    mu         sync.RWMutex
}

func (as *AdaptiveSampler) ShouldSample(
    p sdktrace.SamplingParameters,
) sdktrace.SamplingResult {
    // å°è¯•è·å–ä»¤ç‰Œ
    if as.bucket.TryAcquire() {
        return sdktrace.SamplingResult{
            Decision: sdktrace.RecordAndSample,
        }
    }
    
    return sdktrace.SamplingResult{
        Decision: sdktrace.Drop,
    }
}

// ä»¤ç‰Œæ¡¶ï¼ˆé«˜æ€§èƒ½ï¼‰
type TokenBucket struct {
    capacity  int64
    tokens    atomic.Int64
    rate      int64 // tokens per second
    lastRefill atomic.Int64 // unix nano
}

func (tb *TokenBucket) TryAcquire() bool {
    // è¡¥å……ä»¤ç‰Œ
    tb.refill()
    
    // å°è¯•è·å–ä»¤ç‰Œ
    for {
        tokens := tb.tokens.Load()
        if tokens <= 0 {
            return false
        }
        
        if tb.tokens.CompareAndSwap(tokens, tokens-1) {
            return true
        }
    }
}

func (tb *TokenBucket) refill() {
    now := time.Now().UnixNano()
    last := tb.lastRefill.Load()
    
    if now <= last {
        return
    }
    
    // CASæ›´æ–°æ—¶é—´æˆ³
    if !tb.lastRefill.CompareAndSwap(last, now) {
        return
    }
    
    // è®¡ç®—åº”è¯¥è¡¥å……çš„ä»¤ç‰Œæ•°
    elapsed := now - last
    tokensToAdd := elapsed * tb.rate / 1e9
    
    // è¡¥å……ä»¤ç‰Œï¼ˆä¸è¶…è¿‡å®¹é‡ï¼‰
    for {
        tokens := tb.tokens.Load()
        newTokens := tokens + tokensToAdd
        if newTokens > tb.capacity {
            newTokens = tb.capacity
        }
        
        if tb.tokens.CompareAndSwap(tokens, newTokens) {
            break
        }
    }
}
```

---

## Collectoræ€§èƒ½ä¼˜åŒ–

### 1. å¹¶å‘å¤„ç†

```go
// å¹¶å‘Processor
type ConcurrentProcessor struct {
    workers    int
    queue      chan *SpanBatch
    processors []Processor
    wg         sync.WaitGroup
}

func NewConcurrentProcessor(workers int, processor Processor) *ConcurrentProcessor {
    cp := &ConcurrentProcessor{
        workers:    workers,
        queue:      make(chan *SpanBatch, workers*2),
        processors: make([]Processor, workers),
    }
    
    // ä¸ºæ¯ä¸ªworkeråˆ›å»ºç‹¬ç«‹çš„processorå®ä¾‹ï¼ˆé¿å…é”ç«äº‰ï¼‰
    for i := 0; i < workers; i++ {
        cp.processors[i] = processor.Clone()
    }
    
    // å¯åŠ¨workers
    for i := 0; i < workers; i++ {
        cp.wg.Add(1)
        go cp.worker(i)
    }
    
    return cp
}

func (cp *ConcurrentProcessor) worker(id int) {
    defer cp.wg.Done()
    
    processor := cp.processors[id]
    
    for batch := range cp.queue {
        // å¤„ç†æ‰¹æ¬¡
        for _, span := range batch.Spans {
            processor.Process(span)
        }
        
        // é‡Šæ”¾æ‰¹æ¬¡
        releaseBatch(batch)
    }
}

// æäº¤æ‰¹æ¬¡ï¼ˆéé˜»å¡ï¼‰
func (cp *ConcurrentProcessor) Submit(batch *SpanBatch) bool {
    select {
    case cp.queue <- batch:
        return true
    default:
        return false
    }
}
```

### 2. å†…å­˜ä¼˜åŒ–

```go
// å†…å­˜é™åˆ¶å™¨
type MemoryLimiter struct {
    maxMemory     uint64
    checkInterval time.Duration
    currentMemory atomic.Uint64
    dropRatio     atomic.Uint64 // ä¸¢å¼ƒæ¯”ä¾‹ï¼ˆ0-100ï¼‰
}

func NewMemoryLimiter(maxMemory uint64) *MemoryLimiter {
    ml := &MemoryLimiter{
        maxMemory:     maxMemory,
        checkInterval: 1 * time.Second,
    }
    
    // å¯åŠ¨ç›‘æ§
    go ml.monitor()
    
    return ml
}

func (ml *MemoryLimiter) monitor() {
    ticker := time.NewTicker(ml.checkInterval)
    defer ticker.Stop()
    
    var m runtime.MemStats
    
    for range ticker.C {
        runtime.ReadMemStats(&m)
        
        currentMem := m.Alloc
        ml.currentMemory.Store(currentMem)
        
        // è®¡ç®—ä½¿ç”¨ç‡
        usage := float64(currentMem) / float64(ml.maxMemory)
        
        // åŠ¨æ€è°ƒæ•´ä¸¢å¼ƒæ¯”ä¾‹
        if usage > 0.9 {
            // å†…å­˜ä½¿ç”¨è¶…è¿‡90%ï¼Œä¸¢å¼ƒ50%
            ml.dropRatio.Store(50)
            
            // è§¦å‘GC
            runtime.GC()
        } else if usage > 0.8 {
            // å†…å­˜ä½¿ç”¨è¶…è¿‡80%ï¼Œä¸¢å¼ƒ20%
            ml.dropRatio.Store(20)
        } else {
            // å†…å­˜æ­£å¸¸ï¼Œä¸ä¸¢å¼ƒ
            ml.dropRatio.Store(0)
        }
        
        metrics.MemoryUsage.Set(float64(currentMem))
        metrics.MemoryUsagePercent.Set(usage * 100)
    }
}

// æ£€æŸ¥æ˜¯å¦åº”è¯¥ä¸¢å¼ƒ
func (ml *MemoryLimiter) ShouldDrop() bool {
    dropRatio := ml.dropRatio.Load()
    if dropRatio == 0 {
        return false
    }
    
    return rand.Uint64()%100 < dropRatio
}

// å†…å­˜æ± ç®¡ç†
type MemoryPoolManager struct {
    pools map[int]*sync.Pool // æŒ‰å¤§å°åˆ†ç±»çš„å¯¹è±¡æ± 
}

func NewMemoryPoolManager() *MemoryPoolManager {
    mpm := &MemoryPoolManager{
        pools: make(map[int]*sync.Pool),
    }
    
    // é¢„åˆ›å»ºå¸¸ç”¨å¤§å°çš„æ± 
    sizes := []int{64, 128, 256, 512, 1024, 2048, 4096}
    for _, size := range sizes {
        mpm.createPool(size)
    }
    
    return mpm
}

func (mpm *MemoryPoolManager) createPool(size int) {
    mpm.pools[size] = &sync.Pool{
        New: func() interface{} {
            return make([]byte, size)
        },
    }
}

// è·å–ç¼“å†²åŒº
func (mpm *MemoryPoolManager) Get(size int) []byte {
    // æ‰¾åˆ°æœ€æ¥è¿‘çš„æ± 
    poolSize := mpm.findNearestPoolSize(size)
    
    if pool, exists := mpm.pools[poolSize]; exists {
        buf := pool.Get().([]byte)
        return buf[:size] // æˆªå–æ‰€éœ€å¤§å°
    }
    
    // æ²¡æœ‰åˆé€‚çš„æ± ï¼Œç›´æ¥åˆ†é…
    return make([]byte, size)
}

// é‡Šæ”¾ç¼“å†²åŒº
func (mpm *MemoryPoolManager) Put(buf []byte) {
    capacity := cap(buf)
    
    if pool, exists := mpm.pools[capacity]; exists {
        pool.Put(buf[:capacity]) // æ¢å¤å®Œæ•´å®¹é‡
    }
}
```

### 3. æ‰¹é‡å¯¼å‡ºä¼˜åŒ–

```go
// é«˜æ€§èƒ½å¯¼å‡ºå™¨
type HighPerformanceExporter struct {
    endpoint   string
    client     *http.Client
    compressor Compressor
    serializer Serializer
    batchPool  *sync.Pool
}

func NewHighPerformanceExporter(endpoint string) *HighPerformanceExporter {
    return &HighPerformanceExporter{
        endpoint: endpoint,
        client: &http.Client{
            Transport: &http.Transport{
                MaxIdleConns:        100,
                MaxIdleConnsPerHost: 100,
                IdleConnTimeout:     90 * time.Second,
                // å¯ç”¨HTTP/2
                ForceAttemptHTTP2: true,
                // ç¦ç”¨å‹ç¼©ï¼ˆæˆ‘ä»¬è‡ªå·±å‹ç¼©ï¼‰
                DisableCompression: true,
            },
            Timeout: 30 * time.Second,
        },
        compressor: NewZstdCompressor(),
        serializer: NewProtobufSerializer(),
        batchPool: &sync.Pool{
            New: func() interface{} {
                return &bytes.Buffer{}
            },
        },
    }
}

// å¯¼å‡ºspans
func (hpe *HighPerformanceExporter) ExportSpans(
    ctx context.Context,
    spans []sdktrace.ReadOnlySpan,
) error {
    // 1. åºåˆ—åŒ–
    data, err := hpe.serializer.Serialize(spans)
    if err != nil {
        return err
    }
    
    // 2. å‹ç¼©
    compressed, err := hpe.compressor.Compress(data)
    if err != nil {
        return err
    }
    
    // 3. å‘é€
    req, err := http.NewRequestWithContext(
        ctx,
        "POST",
        hpe.endpoint,
        bytes.NewReader(compressed),
    )
    if err != nil {
        return err
    }
    
    req.Header.Set("Content-Type", "application/x-protobuf")
    req.Header.Set("Content-Encoding", "zstd")
    
    resp, err := hpe.client.Do(req)
    if err != nil {
        return err
    }
    defer resp.Body.Close()
    
    if resp.StatusCode != http.StatusOK {
        return fmt.Errorf("export failed: %d", resp.StatusCode)
    }
    
    return nil
}

// Zstdå‹ç¼©å™¨ï¼ˆé«˜æ€§èƒ½ï¼‰
type ZstdCompressor struct {
    encoder *zstd.Encoder
}

func NewZstdCompressor() *ZstdCompressor {
    encoder, _ := zstd.NewWriter(nil,
        zstd.WithEncoderLevel(zstd.SpeedFastest), // å¿«é€Ÿå‹ç¼©
        zstd.WithEncoderConcurrency(2),           // å¹¶å‘å‹ç¼©
    )
    
    return &ZstdCompressor{encoder: encoder}
}

func (zc *ZstdCompressor) Compress(data []byte) ([]byte, error) {
    return zc.encoder.EncodeAll(data, nil), nil
}

// Protobufåºåˆ—åŒ–å™¨ï¼ˆé«˜æ•ˆï¼‰
type ProtobufSerializer struct{}

func (ps *ProtobufSerializer) Serialize(spans []sdktrace.ReadOnlySpan) ([]byte, error) {
    // è½¬æ¢ä¸ºprotobufæ ¼å¼
    pbSpans := make([]*tracepb.Span, len(spans))
    for i, span := range spans {
        pbSpans[i] = convertToPbSpan(span)
    }
    
    req := &tracepb.ExportTraceServiceRequest{
        ResourceSpans: []*tracepb.ResourceSpans{
            {
                ScopeSpans: []*tracepb.ScopeSpans{
                    {
                        Spans: pbSpans,
                    },
                },
            },
        },
    }
    
    return proto.Marshal(req)
}
```

---

## å­˜å‚¨æ€§èƒ½ä¼˜åŒ–

### 1. ClickHouseä¼˜åŒ–

```sql
-- ä¼˜åŒ–çš„è¡¨ç»“æ„
CREATE TABLE traces (
    trace_id String,
    span_id String,
    parent_span_id String,
    service_name LowCardinality(String),  -- ä½åŸºæ•°ä¼˜åŒ–
    operation_name LowCardinality(String),
    start_time DateTime64(9),
    duration UInt64,
    status_code UInt8,
    
    -- å±æ€§ï¼ˆä½¿ç”¨Mapç±»å‹ï¼‰
    attributes Map(String, String),
    
    -- ç´¢å¼•å­—æ®µ
    INDEX idx_service_name service_name TYPE bloom_filter GRANULARITY 1,
    INDEX idx_operation operation_name TYPE bloom_filter GRANULARITY 1,
    INDEX idx_duration duration TYPE minmax GRANULARITY 1
)
ENGINE = MergeTree()
PARTITION BY toYYYYMMDD(start_time)  -- æŒ‰å¤©åˆ†åŒº
ORDER BY (service_name, start_time, trace_id)  -- æ’åºé”®
TTL start_time + INTERVAL 7 DAY  -- 7å¤©TTL
SETTINGS
    index_granularity = 8192,
    merge_max_block_size = 8192;

-- ç‰©åŒ–è§†å›¾ï¼ˆé¢„èšåˆï¼‰
CREATE MATERIALIZED VIEW traces_hourly_mv
ENGINE = SummingMergeTree()
PARTITION BY toYYYYMMDD(hour)
ORDER BY (service_name, operation_name, hour)
AS SELECT
    service_name,
    operation_name,
    toStartOfHour(start_time) AS hour,
    count() AS request_count,
    sum(duration) AS total_duration,
    quantile(0.50)(duration) AS p50_duration,
    quantile(0.95)(duration) AS p95_duration,
    quantile(0.99)(duration) AS p99_duration,
    countIf(status_code != 0) AS error_count
FROM traces
GROUP BY service_name, operation_name, hour;

-- æŸ¥è¯¢ä¼˜åŒ–
-- ä½¿ç”¨ç‰©åŒ–è§†å›¾æŸ¥è¯¢ï¼ˆå¿«é€Ÿï¼‰
SELECT
    service_name,
    operation_name,
    sum(request_count) AS total_requests,
    sum(error_count) / sum(request_count) AS error_rate,
    quantileMerge(0.99)(p99_duration) AS p99
FROM traces_hourly_mv
WHERE hour >= now() - INTERVAL 1 DAY
GROUP BY service_name, operation_name
ORDER BY total_requests DESC
LIMIT 100;
```

### 2. æ‰¹é‡å†™å…¥ä¼˜åŒ–

```go
// æ‰¹é‡å†™å…¥å™¨
type BatchWriter struct {
    db        *sql.DB
    batchSize int
    buffer    []*Span
    mu        sync.Mutex
    flushCh   chan struct{}
}

func NewBatchWriter(db *sql.DB, batchSize int) *BatchWriter {
    bw := &BatchWriter{
        db:        db,
        batchSize: batchSize,
        buffer:    make([]*Span, 0, batchSize),
        flushCh:   make(chan struct{}, 1),
    }
    
    // å¯åŠ¨åˆ·æ–°åç¨‹
    go bw.flusher()
    
    return bw
}

func (bw *BatchWriter) Write(span *Span) {
    bw.mu.Lock()
    bw.buffer = append(bw.buffer, span)
    shouldFlush := len(bw.buffer) >= bw.batchSize
    bw.mu.Unlock()
    
    if shouldFlush {
        select {
        case bw.flushCh <- struct{}{}:
        default:
        }
    }
}

func (bw *BatchWriter) flusher() {
    ticker := time.NewTicker(1 * time.Second)
    defer ticker.Stop()
    
    for {
        select {
        case <-bw.flushCh:
            bw.flush()
        case <-ticker.C:
            bw.flush()
        }
    }
}

func (bw *BatchWriter) flush() {
    bw.mu.Lock()
    if len(bw.buffer) == 0 {
        bw.mu.Unlock()
        return
    }
    
    batch := bw.buffer
    bw.buffer = make([]*Span, 0, bw.batchSize)
    bw.mu.Unlock()
    
    // æ‰¹é‡æ’å…¥
    if err := bw.batchInsert(batch); err != nil {
        log.Printf("Batch insert failed: %v", err)
    }
}

func (bw *BatchWriter) batchInsert(spans []*Span) error {
    // ä½¿ç”¨ClickHouseçš„æ‰¹é‡æ’å…¥
    tx, err := bw.db.Begin()
    if err != nil {
        return err
    }
    defer tx.Rollback()
    
    stmt, err := tx.Prepare(`
        INSERT INTO traces (
            trace_id, span_id, parent_span_id,
            service_name, operation_name,
            start_time, duration, status_code,
            attributes
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
    `)
    if err != nil {
        return err
    }
    defer stmt.Close()
    
    for _, span := range spans {
        _, err := stmt.Exec(
            span.TraceID,
            span.SpanID,
            span.ParentSpanID,
            span.ServiceName,
            span.OperationName,
            span.StartTime,
            span.Duration,
            span.StatusCode,
            span.Attributes,
        )
        if err != nil {
            return err
        }
    }
    
    return tx.Commit()
}
```

---

## æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–

### 1. æŸ¥è¯¢ä¼˜åŒ–

```go
// æŸ¥è¯¢ä¼˜åŒ–å™¨
type QueryOptimizer struct {
    cache *QueryCache
}

// æŸ¥è¯¢ç¼“å­˜
type QueryCache struct {
    cache *lru.Cache
    ttl   time.Duration
}

func (qc *QueryCache) Get(query string) (*QueryResult, bool) {
    if val, ok := qc.cache.Get(query); ok {
        cached := val.(*CachedResult)
        
        // æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
        if time.Since(cached.Timestamp) < qc.ttl {
            return cached.Result, true
        }
        
        // è¿‡æœŸï¼Œåˆ é™¤
        qc.cache.Remove(query)
    }
    
    return nil, false
}

// æŸ¥è¯¢é‡å†™
func (qo *QueryOptimizer) Optimize(query *Query) *Query {
    optimized := query.Clone()
    
    // 1. è°“è¯ä¸‹æ¨
    optimized = qo.pushDownPredicates(optimized)
    
    // 2. é€‰æ‹©æ€§æŠ•å½±
    optimized = qo.selectiveProjection(optimized)
    
    // 3. ç´¢å¼•æç¤º
    optimized = qo.addIndexHints(optimized)
    
    return optimized
}

// å¹¶è¡ŒæŸ¥è¯¢
func (qo *QueryOptimizer) ParallelQuery(query *Query) (*QueryResult, error) {
    // å°†æŸ¥è¯¢åˆ†ç‰‡
    shards := qo.shardQuery(query)
    
    // å¹¶è¡Œæ‰§è¡Œ
    results := make(chan *ShardResult, len(shards))
    var wg sync.WaitGroup
    
    for _, shard := range shards {
        wg.Add(1)
        go func(s *Query) {
            defer wg.Done()
            
            result, err := qo.executeQuery(s)
            results <- &ShardResult{
                Result: result,
                Error:  err,
            }
        }(shard)
    }
    
    go func() {
        wg.Wait()
        close(results)
    }()
    
    // åˆå¹¶ç»“æœ
    return qo.mergeResults(results)
}
```

### 2. ç´¢å¼•ä¼˜åŒ–

```sql
-- åˆ›å»ºåˆé€‚çš„ç´¢å¼•
CREATE INDEX idx_trace_id ON traces(trace_id) TYPE bloom_filter GRANULARITY 1;
CREATE INDEX idx_service_time ON traces(service_name, start_time) TYPE minmax GRANULARITY 1;

-- ä½¿ç”¨ç‰©åŒ–è§†å›¾åŠ é€ŸèšåˆæŸ¥è¯¢
CREATE MATERIALIZED VIEW service_metrics_mv
ENGINE = AggregatingMergeTree()
ORDER BY (service_name, minute)
AS SELECT
    service_name,
    toStartOfMinute(start_time) AS minute,
    countState() AS request_count,
    avgState(duration) AS avg_duration,
    quantileState(0.99)(duration) AS p99_duration
FROM traces
GROUP BY service_name, minute;

-- æŸ¥è¯¢æ—¶ä½¿ç”¨Mergeå‡½æ•°
SELECT
    service_name,
    countMerge(request_count) AS total_requests,
    avgMerge(avg_duration) AS avg_latency,
    quantileMerge(0.99)(p99_duration) AS p99
FROM service_metrics_mv
WHERE minute >= now() - INTERVAL 1 HOUR
GROUP BY service_name;
```

---

## ç½‘ç»œæ€§èƒ½ä¼˜åŒ–

### 1. HTTP/2ä¸gRPC

```go
// ä½¿ç”¨HTTP/2
func createHTTP2Client() *http.Client {
    return &http.Client{
        Transport: &http.Transport{
            // å¯ç”¨HTTP/2
            ForceAttemptHTTP2: true,
            
            // è¿æ¥æ± 
            MaxIdleConns:        100,
            MaxIdleConnsPerHost: 100,
            IdleConnTimeout:     90 * time.Second,
            
            // TCPä¼˜åŒ–
            DialContext: (&net.Dialer{
                Timeout:   30 * time.Second,
                KeepAlive: 30 * time.Second,
                // å¯ç”¨TCP Fast Open
                Control: func(network, address string, c syscall.RawConn) error {
                    return c.Control(func(fd uintptr) {
                        syscall.SetsockoptInt(int(fd), syscall.IPPROTO_TCP, syscall.TCP_FASTOPEN, 1)
                    })
                },
            }).DialContext,
        },
    }
}

// gRPCè¿æ¥æ± 
type GRPCConnectionPool struct {
    conns []*grpc.ClientConn
    next  atomic.Uint32
}

func NewGRPCConnectionPool(endpoint string, size int) (*GRPCConnectionPool, error) {
    pool := &GRPCConnectionPool{
        conns: make([]*grpc.ClientConn, size),
    }
    
    for i := 0; i < size; i++ {
        conn, err := grpc.Dial(endpoint,
            grpc.WithTransportCredentials(insecure.NewCredentials()),
            grpc.WithKeepaliveParams(keepalive.ClientParameters{
                Time:                10 * time.Second,
                Timeout:             3 * time.Second,
                PermitWithoutStream: true,
            }),
            // å¯ç”¨å‹ç¼©
            grpc.WithDefaultCallOptions(grpc.UseCompressor("gzip")),
        )
        if err != nil {
            return nil, err
        }
        pool.conns[i] = conn
    }
    
    return pool, nil
}

// è½®è¯¢è·å–è¿æ¥
func (pool *GRPCConnectionPool) Get() *grpc.ClientConn {
    idx := pool.next.Add(1) % uint32(len(pool.conns))
    return pool.conns[idx]
}
```

### 2. å‹ç¼©ä¼˜åŒ–

```go
// è‡ªé€‚åº”å‹ç¼©
type AdaptiveCompressor struct {
    compressors map[string]Compressor
    selector    *CompressionSelector
}

type CompressionSelector struct {
    stats map[string]*CompressionStats
}

type CompressionStats struct {
    InputSize      int64
    OutputSize     int64
    CompressionTime time.Duration
    Ratio          float64
}

// é€‰æ‹©æœ€ä½³å‹ç¼©ç®—æ³•
func (cs *CompressionSelector) SelectBest(dataSize int) string {
    // å°æ•°æ®ä¸å‹ç¼©
    if dataSize < 1024 {
        return "none"
    }
    
    // ä¸­ç­‰æ•°æ®ä½¿ç”¨LZ4ï¼ˆå¿«é€Ÿï¼‰
    if dataSize < 100*1024 {
        return "lz4"
    }
    
    // å¤§æ•°æ®ä½¿ç”¨Zstdï¼ˆé«˜å‹ç¼©æ¯”ï¼‰
    return "zstd"
}

// æµå¼å‹ç¼©ï¼ˆå¤§æ•°æ®ï¼‰
type StreamCompressor struct {
    writer *zstd.Encoder
}

func (sc *StreamCompressor) CompressStream(r io.Reader, w io.Writer) error {
    encoder, err := zstd.NewWriter(w)
    if err != nil {
        return err
    }
    defer encoder.Close()
    
    _, err = io.Copy(encoder, r)
    return err
}
```

---

## æ€§èƒ½ç›‘æ§ä¸è¯Šæ–­

### 1. æ€§èƒ½æŒ‡æ ‡

```go
// æ€§èƒ½æŒ‡æ ‡æ”¶é›†
type PerformanceMetrics struct {
    // SDKæŒ‡æ ‡
    SpanCreationLatency prometheus.Histogram
    SpanExportLatency   prometheus.Histogram
    DroppedSpans        prometheus.Counter
    
    // CollectoræŒ‡æ ‡
    ReceivedSpans       prometheus.Counter
    ProcessedSpans      prometheus.Counter
    ExportedSpans       prometheus.Counter
    QueueLength         prometheus.Gauge
    ProcessingLatency   prometheus.Histogram
    
    // èµ„æºæŒ‡æ ‡
    CPUUsage            prometheus.Gauge
    MemoryUsage         prometheus.Gauge
    GoroutineCount      prometheus.Gauge
}

// æ³¨å†ŒæŒ‡æ ‡
func RegisterMetrics() *PerformanceMetrics {
    return &PerformanceMetrics{
        SpanCreationLatency: prometheus.NewHistogram(
            prometheus.HistogramOpts{
                Name:    "otlp_span_creation_latency_seconds",
                Help:    "Span creation latency",
                Buckets: prometheus.ExponentialBuckets(0.0001, 2, 10),
            },
        ),
        // ... å…¶ä»–æŒ‡æ ‡
    }
}

// æ€§èƒ½åˆ†æ
func (pm *PerformanceMetrics) Analyze() *PerformanceReport {
    report := &PerformanceReport{}
    
    // åˆ†æå»¶è¿Ÿ
    p99 := pm.SpanCreationLatency.Quantile(0.99)
    if p99 > 0.001 { // >1ms
        report.Issues = append(report.Issues, "High span creation latency")
    }
    
    // åˆ†æä¸¢å¼ƒç‡
    dropRate := pm.DroppedSpans.Value() / pm.ReceivedSpans.Value()
    if dropRate > 0.01 { // >1%
        report.Issues = append(report.Issues, "High drop rate")
    }
    
    return report
}
```

### 2. æ€§èƒ½å‰–æ

```go
// CPU Profiling
func enableCPUProfiling() {
    f, err := os.Create("cpu.prof")
    if err != nil {
        log.Fatal(err)
    }
    pprof.StartCPUProfile(f)
    defer pprof.StopCPUProfile()
}

// Memory Profiling
func enableMemoryProfiling() {
    f, err := os.Create("mem.prof")
    if err != nil {
        log.Fatal(err)
    }
    defer f.Close()
    
    runtime.GC()
    if err := pprof.WriteHeapProfile(f); err != nil {
        log.Fatal(err)
    }
}

// Goroutine Profiling
func enableGoroutineProfiling() {
    f, err := os.Create("goroutine.prof")
    if err != nil {
        log.Fatal(err)
    }
    defer f.Close()
    
    if err := pprof.Lookup("goroutine").WriteTo(f, 0); err != nil {
        log.Fatal(err)
    }
}

// æ€§èƒ½è¿½è¸ª
func enableTracing() {
    f, err := os.Create("trace.out")
    if err != nil {
        log.Fatal(err)
    }
    defer f.Close()
    
    trace.Start(f)
    defer trace.Stop()
}
```

---

## æ€§èƒ½æµ‹è¯•

### 1. åŸºå‡†æµ‹è¯•

```go
// Spanåˆ›å»ºåŸºå‡†æµ‹è¯•
func BenchmarkSpanCreation(b *testing.B) {
    tracer := otel.Tracer("test")
    ctx := context.Background()
    
    b.ResetTimer()
    b.ReportAllocs()
    
    for i := 0; i < b.N; i++ {
        _, span := tracer.Start(ctx, "test-span")
        span.End()
    }
}

// æ‰¹é‡å¤„ç†åŸºå‡†æµ‹è¯•
func BenchmarkBatchProcessing(b *testing.B) {
    processor := NewBatchProcessor(1000)
    spans := generateSpans(1000)
    
    b.ResetTimer()
    b.ReportAllocs()
    
    for i := 0; i < b.N; i++ {
        processor.ProcessBatch(spans)
    }
}

// å‹åŠ›æµ‹è¯•
func StressTest(t *testing.T) {
    collector := NewCollector()
    
    // å¯åŠ¨å¤šä¸ªç”Ÿäº§è€…
    var wg sync.WaitGroup
    producers := 100
    spansPerProducer := 10000
    
    start := time.Now()
    
    for i := 0; i < producers; i++ {
        wg.Add(1)
        go func() {
            defer wg.Done()
            
            for j := 0; j < spansPerProducer; j++ {
                span := generateSpan()
                collector.Receive(span)
            }
        }()
    }
    
    wg.Wait()
    duration := time.Since(start)
    
    totalSpans := producers * spansPerProducer
    throughput := float64(totalSpans) / duration.Seconds()
    
    t.Logf("Throughput: %.0f spans/s", throughput)
    t.Logf("Total time: %v", duration)
}
```

### 2. æ€§èƒ½ç›®æ ‡

| ç»„ä»¶ | æŒ‡æ ‡ | ç›®æ ‡ | æµ‹è¯•æ–¹æ³• |
|------|------|------|----------|
| SDK | Spanåˆ›å»ºå»¶è¿Ÿ | <100Î¼s | Benchmark |
| SDK | å†…å­˜å¼€é”€ | <1KB/span | Memory Profile |
| Collector | ååé‡ | >100K spans/s | Stress Test |
| Collector | CPUä½¿ç”¨ç‡ | <50% | Resource Monitor |
| Collector | å†…å­˜ä½¿ç”¨ | <2GB | Memory Monitor |
| Storage | å†™å…¥å»¶è¿Ÿ | <10ms | Latency Test |
| Storage | æŸ¥è¯¢å»¶è¿Ÿ | <100ms | Query Benchmark |

---

## æ€»ç»“

æ€§èƒ½ä¼˜åŒ–æ˜¯ä¸€ä¸ªæŒç»­çš„è¿‡ç¨‹ï¼Œéœ€è¦ï¼š

1. **æµ‹é‡å…ˆè¡Œ**: ä½¿ç”¨æ€§èƒ½åˆ†æå·¥å…·è¯†åˆ«ç“¶é¢ˆ
2. **é’ˆå¯¹æ€§ä¼˜åŒ–**: ä¼˜åŒ–çœŸæ­£çš„ç“¶é¢ˆ
3. **æƒè¡¡å–èˆ**: å¹³è¡¡æ€§èƒ½ã€åŠŸèƒ½å’Œå¤æ‚åº¦
4. **æŒç»­ç›‘æ§**: å»ºç«‹æ€§èƒ½ç›‘æ§å’Œå‘Šè­¦æœºåˆ¶
5. **å®šæœŸreview**: å®šæœŸå®¡æŸ¥å’Œä¼˜åŒ–æ€§èƒ½

---

*æœ€åæ›´æ–°: 2025å¹´10æœˆ7æ—¥*-
