# OpenTelemetry 2025å¹´éƒ¨ç½²è¿ç»´æŒ‡å—

## ğŸ¯ éƒ¨ç½²è¿ç»´æŒ‡å—æ¦‚è¿°

åŸºäºå›½é™…2025å¹´æœ€æ–°æŠ€æœ¯å·¥ç¨‹æ–¹æ¡ˆæ ‡å‡†å’Œè¡Œä¸šæœ€ä½³å®è·µï¼Œæä¾›OpenTelemetryç³»ç»Ÿçš„å®Œæ•´éƒ¨ç½²è¿ç»´æŒ‡å—ï¼Œæ¶µç›–äº‘åŸç”Ÿã€è¾¹ç¼˜è®¡ç®—ã€æ··åˆäº‘ç­‰å¤šç§éƒ¨ç½²æ¨¡å¼ã€‚

---

## â˜ï¸ äº‘åŸç”Ÿéƒ¨ç½²æ¶æ„

### 1. Kuberneteséƒ¨ç½²

#### 1.1 é«˜å¯ç”¨éƒ¨ç½²é…ç½®

```yaml
# OpenTelemetry Collectoré«˜å¯ç”¨éƒ¨ç½²
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector
  namespace: observability
  labels:
    app: otel-collector
    version: v1.0.0
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 2
      maxUnavailable: 1
  selector:
    matchLabels:
      app: otel-collector
  template:
    metadata:
      labels:
        app: otel-collector
        version: v1.0.0
    spec:
      serviceAccountName: otel-collector
      securityContext:
        runAsNonRoot: true
        runAsUser: 10001
        fsGroup: 10001
      containers:
      - name: otel-collector
        image: otel/opentelemetry-collector-contrib:latest
        ports:
        - containerPort: 4317
          name: otlp-grpc
          protocol: TCP
        - containerPort: 4318
          name: otlp-http
          protocol: TCP
        - containerPort: 8888
          name: metrics
          protocol: TCP
        - containerPort: 13133
          name: health
          protocol: TCP
        env:
        - name: OTEL_RESOURCE_ATTRIBUTES
          value: "service.name=otel-collector,service.version=1.0.0"
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: "http://jaeger:14250"
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /
            port: 13133
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /
            port: 13133
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        volumeMounts:
        - name: config
          mountPath: /etc/otelcol-contrib
          readOnly: true
        - name: tmp
          mountPath: /tmp
      volumes:
      - name: config
        configMap:
          name: otel-collector-config
      - name: tmp
        emptyDir: {}
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values: [otel-collector]
            topologyKey: kubernetes.io/hostname
      tolerations:
      - key: "node-role.kubernetes.io/master"
        operator: "Exists"
        effect: "NoSchedule"
      nodeSelector:
        kubernetes.io/os: linux
```

#### 1.2 æœåŠ¡é…ç½®

```yaml
# OpenTelemetry CollectoræœåŠ¡é…ç½®
apiVersion: v1
kind: Service
metadata:
  name: otel-collector
  namespace: observability
  labels:
    app: otel-collector
spec:
  type: ClusterIP
  ports:
  - port: 4317
    targetPort: 4317
    protocol: TCP
    name: otlp-grpc
  - port: 4318
    targetPort: 4318
    protocol: TCP
    name: otlp-http
  - port: 8888
    targetPort: 8888
    protocol: TCP
    name: metrics
  - port: 13133
    targetPort: 13133
    protocol: TCP
    name: health
  selector:
    app: otel-collector
```

#### 1.3 é…ç½®ç®¡ç†

```yaml
# OpenTelemetry Collectoré…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
  namespace: observability
data:
  otel-collector.yaml: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
            max_recv_msg_size: 4194304
            max_concurrent_streams: 16
          http:
            endpoint: 0.0.0.0:4318
            max_request_body_size: 4194304
            cors:
              allowed_origins:
                - "https://*.company.com"
      
      prometheus:
        config:
          scrape_configs:
          - job_name: 'otel-collector'
            scrape_interval: 10s
            static_configs:
            - targets: ['localhost:8888']
    
    processors:
      batch:
        timeout: 1s
        send_batch_size: 1024
        send_batch_max_size: 2048
      
      memory_limiter:
        limit_mib: 512
        spike_limit_mib: 128
        check_interval: 5s
      
      resource:
        attributes:
        - key: deployment.environment
          value: production
          action: upsert
        - key: service.namespace
          value: observability
          action: upsert
      
      span:
        name:
          to_attributes:
            rules: ["^/api/v1/(?P<version>.*?)/(?P<service>.*?)$"]
        attributes:
        - key: http.route
          from_attribute: http.target
          action: insert
      
      filter:
        spans:
          exclude:
            match_type: regexp
            attributes:
            - key: http.url
              value: ".*health.*"
    
    exporters:
      jaeger:
        endpoint: jaeger:14250
        tls:
          insecure: false
      
      prometheus:
        endpoint: "0.0.0.0:8889"
        namespace: otel
        const_labels:
          label1: value1
      
      elasticsearch:
        endpoint: "http://elasticsearch:9200"
        index: "otel-traces"
        mapping:
          dynamic: true
      
      logging:
        loglevel: info
        sampling_initial: 2
        sampling_thereafter: 500
    
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, resource, span, filter, batch]
          exporters: [jaeger, elasticsearch, logging]
        
        metrics:
          receivers: [otlp, prometheus]
          processors: [memory_limiter, resource, batch]
          exporters: [prometheus, logging]
        
        logs:
          receivers: [otlp]
          processors: [memory_limiter, resource, batch]
          exporters: [elasticsearch, logging]
      
      extensions: [health_check, pprof, zpages]
      
      telemetry:
        logs:
          level: info
        metrics:
          address: 0.0.0.0:8888
```

### 2. Helm Chartéƒ¨ç½²

#### 2.1 Helm Charté…ç½®

```yaml
# values.yaml
replicaCount: 3

image:
  repository: otel/opentelemetry-collector-contrib
  tag: latest
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  ports:
    grpc: 4317
    http: 4318
    metrics: 8888
    health: 13133

resources:
  limits:
    cpu: 1000m
    memory: 1Gi
  requests:
    cpu: 500m
    memory: 512Mi

autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: app
          operator: In
          values: [otel-collector]
      topologyKey: kubernetes.io/hostname

config:
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318
  
  processors:
    batch:
      timeout: 1s
      send_batch_size: 1024
  
  exporters:
    jaeger:
      endpoint: jaeger:14250
    prometheus:
      endpoint: "0.0.0.0:8889"
  
  service:
    pipelines:
      traces:
        receivers: [otlp]
        processors: [batch]
        exporters: [jaeger]
      metrics:
        receivers: [otlp]
        processors: [batch]
        exporters: [prometheus]
```

#### 2.2 éƒ¨ç½²å‘½ä»¤

```bash
# æ·»åŠ Helmä»“åº“
helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts
helm repo update

# éƒ¨ç½²OpenTelemetry Collector
helm install otel-collector open-telemetry/opentelemetry-collector \
  --namespace observability \
  --create-namespace \
  --values values.yaml

# éªŒè¯éƒ¨ç½²
kubectl get pods -n observability
kubectl get svc -n observability
```

---

## ğŸŒ è¾¹ç¼˜è®¡ç®—éƒ¨ç½²

### 1. è¾¹ç¼˜èŠ‚ç‚¹éƒ¨ç½²

#### 1.1 DaemonSeté…ç½®

```yaml
# è¾¹ç¼˜èŠ‚ç‚¹DaemonSetéƒ¨ç½²
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: otel-collector-edge
  namespace: observability
  labels:
    app: otel-collector-edge
spec:
  selector:
    matchLabels:
      app: otel-collector-edge
  template:
    metadata:
      labels:
        app: otel-collector-edge
    spec:
      serviceAccountName: otel-collector-edge
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      containers:
      - name: otel-collector
        image: otel/opentelemetry-collector-contrib:latest
        ports:
        - containerPort: 4317
          name: otlp-grpc
          hostPort: 4317
        - containerPort: 4318
          name: otlp-http
          hostPort: 4318
        - containerPort: 8888
          name: metrics
          hostPort: 8888
        env:
        - name: OTEL_RESOURCE_ATTRIBUTES
          value: "service.name=otel-collector-edge,service.version=1.0.0"
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: "http://central-collector:4317"
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
        volumeMounts:
        - name: config
          mountPath: /etc/otelcol-contrib
          readOnly: true
        - name: tmp
          mountPath: /tmp
        - name: host-proc
          mountPath: /host/proc
          readOnly: true
        - name: host-sys
          mountPath: /host/sys
          readOnly: true
      volumes:
      - name: config
        configMap:
          name: otel-collector-edge-config
      - name: tmp
        emptyDir: {}
      - name: host-proc
        hostPath:
          path: /proc
      - name: host-sys
        hostPath:
          path: /sys
      nodeSelector:
        node-role.kubernetes.io/edge: "true"
      tolerations:
      - key: "node-role.kubernetes.io/edge"
        operator: "Exists"
        effect: "NoSchedule"
```

#### 1.2 è¾¹ç¼˜é…ç½®

```yaml
# è¾¹ç¼˜èŠ‚ç‚¹é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-edge-config
  namespace: observability
data:
  otel-collector.yaml: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      
      hostmetrics:
        collection_interval: 10s
        scrapers:
          cpu: {}
          memory: {}
          disk: {}
          network: {}
          filesystem: {}
          load: {}
          processes: {}
      
      prometheus:
        config:
          scrape_configs:
          - job_name: 'node-exporter'
            scrape_interval: 10s
            static_configs:
            - targets: ['localhost:9100']
    
    processors:
      batch:
        timeout: 5s
        send_batch_size: 512
        send_batch_max_size: 1024
      
      memory_limiter:
        limit_mib: 256
        spike_limit_mib: 64
        check_interval: 5s
      
      resource:
        attributes:
        - key: deployment.environment
          value: edge
          action: upsert
        - key: node.name
          from_attribute: host.name
          action: insert
      
      filter:
        metrics:
          exclude:
            match_type: regexp
            metric_names:
            - "system.cpu.time"
            - "system.memory.usage"
    
    exporters:
      otlp:
        endpoint: http://central-collector:4317
        tls:
          insecure: true
        retry_on_failure:
          enabled: true
          initial_interval: 1s
          max_interval: 30s
          max_elapsed_time: 300s
        sending_queue:
          enabled: true
          num_consumers: 10
          queue_size: 1000
      
      logging:
        loglevel: warn
        sampling_initial: 2
        sampling_thereafter: 500
    
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, resource, batch]
          exporters: [otlp, logging]
        
        metrics:
          receivers: [otlp, hostmetrics, prometheus]
          processors: [memory_limiter, resource, filter, batch]
          exporters: [otlp, logging]
        
        logs:
          receivers: [otlp]
          processors: [memory_limiter, resource, batch]
          exporters: [otlp, logging]
      
      extensions: [health_check, pprof]
      
      telemetry:
        logs:
          level: warn
        metrics:
          address: 0.0.0.0:8888
```

### 2. è¾¹ç¼˜æ•°æ®åŒæ­¥

#### 2.1 æ•°æ®åŒæ­¥é…ç½®

```yaml
# è¾¹ç¼˜æ•°æ®åŒæ­¥é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: edge-sync-config
  namespace: observability
data:
  sync-config.yaml: |
    sync:
      enabled: true
      interval: 30s
      batch_size: 1000
      retry_attempts: 3
      retry_interval: 5s
      
      offline_storage:
        enabled: true
        max_size: "1GB"
        retention: "7d"
        compression: true
      
      online_sync:
        enabled: true
        endpoint: "http://central-collector:4317"
        timeout: 30s
        compression: true
      
      conflict_resolution:
        strategy: "timestamp_based"
        priority: "central"
      
      data_validation:
        enabled: true
        schema_validation: true
        integrity_check: true
```

---

## ğŸ”§ è¿ç»´ç®¡ç†

### 1. ç›‘æ§å’Œå‘Šè­¦

#### 1.1 Prometheusç›‘æ§é…ç½®

```yaml
# Prometheusç›‘æ§é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: observability
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    rule_files:
      - "otel-collector-rules.yml"
    
    alerting:
      alertmanagers:
      - static_configs:
        - targets:
          - alertmanager:9093
    
    scrape_configs:
    - job_name: 'otel-collector'
      static_configs:
      - targets: ['otel-collector:8888']
      scrape_interval: 10s
      metrics_path: /metrics
    
    - job_name: 'jaeger'
      static_configs:
      - targets: ['jaeger:14269']
      scrape_interval: 10s
      metrics_path: /metrics
    
    - job_name: 'elasticsearch'
      static_configs:
      - targets: ['elasticsearch:9200']
      scrape_interval: 10s
      metrics_path: /_prometheus/metrics
```

#### 1.2 å‘Šè­¦è§„åˆ™é…ç½®

```yaml
# å‘Šè­¦è§„åˆ™é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-rules
  namespace: observability
data:
  otel-collector-rules.yml: |
    groups:
    - name: otel-collector
      rules:
      - alert: OtelCollectorDown
        expr: up{job="otel-collector"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "OpenTelemetry Collector is down"
          description: "OpenTelemetry Collector has been down for more than 1 minute"
      
      - alert: OtelCollectorHighMemoryUsage
        expr: process_resident_memory_bytes{job="otel-collector"} > 1000000000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "OpenTelemetry Collector high memory usage"
          description: "OpenTelemetry Collector memory usage is above 1GB"
      
      - alert: OtelCollectorHighCPUUsage
        expr: rate(process_cpu_seconds_total{job="otel-collector"}[5m]) > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "OpenTelemetry Collector high CPU usage"
          description: "OpenTelemetry Collector CPU usage is above 80%"
      
      - alert: OtelCollectorHighErrorRate
        expr: rate(otelcol_receiver_accepted_spans{job="otel-collector"}[5m]) / rate(otelcol_receiver_refused_spans{job="otel-collector"}[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "OpenTelemetry Collector high error rate"
          description: "OpenTelemetry Collector error rate is above 10%"
```

### 2. æ—¥å¿—ç®¡ç†

#### 2.1 æ—¥å¿—æ”¶é›†é…ç½®

```yaml
# æ—¥å¿—æ”¶é›†é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: log-collection-config
  namespace: observability
data:
  log-collection.yaml: |
    log_collection:
      enabled: true
      sources:
        - type: "container_logs"
          path: "/var/log/containers/*.log"
          format: "json"
        - type: "application_logs"
          path: "/var/log/applications/*.log"
          format: "text"
        - type: "system_logs"
          path: "/var/log/syslog"
          format: "syslog"
      
      processing:
        parsing:
          enabled: true
          rules:
            - name: "json_logs"
              pattern: "json"
              fields:
                - "timestamp"
                - "level"
                - "message"
                - "service"
            - name: "text_logs"
              pattern: "text"
              fields:
                - "timestamp"
                - "level"
                - "message"
        
        filtering:
          enabled: true
          rules:
            - name: "exclude_health_checks"
              condition: "message contains 'health'"
              action: "exclude"
            - name: "include_errors"
              condition: "level == 'ERROR'"
              action: "include"
        
        enrichment:
          enabled: true
          rules:
            - name: "add_service_info"
              condition: "service is empty"
              action: "add_field"
              field: "service"
              value: "unknown"
      
      output:
        elasticsearch:
          enabled: true
          endpoint: "http://elasticsearch:9200"
          index: "otel-logs"
          bulk_size: 1000
          flush_interval: 5s
        
        loki:
          enabled: true
          endpoint: "http://loki:3100"
          labels:
            - "service"
            - "level"
            - "environment"
```

### 3. æ€§èƒ½ä¼˜åŒ–

#### 3.1 æ€§èƒ½è°ƒä¼˜é…ç½®

```yaml
# æ€§èƒ½è°ƒä¼˜é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: performance-tuning-config
  namespace: observability
data:
  performance-tuning.yaml: |
    performance_tuning:
      batch_processing:
        batch_size: 1024
        batch_timeout: 1s
        max_batch_size: 2048
        flush_interval: 100ms
      
      memory_management:
        memory_limit: "1Gi"
        spike_limit: "256Mi"
        check_interval: 5s
        gc_threshold: 0.8
      
      cpu_optimization:
        cpu_limit: "1000m"
        cpu_request: "500m"
        thread_pool_size: 10
        worker_threads: 4
      
      network_optimization:
        connection_pool_size: 100
        keep_alive: true
        compression: "gzip"
        timeout: 30s
      
      storage_optimization:
        cache_size: "512Mi"
        cache_ttl: 300s
        compression: true
        retention: "30d"
      
      sampling:
        trace_sampling_rate: 0.1
        metric_sampling_rate: 1.0
        log_sampling_rate: 0.01
        adaptive_sampling: true
```

---

## ğŸ›¡ï¸ å®‰å…¨è¿ç»´

### 1. å®‰å…¨é…ç½®

#### 1.1 ç½‘ç»œå®‰å…¨

```yaml
# ç½‘ç»œå®‰å…¨é…ç½®
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: otel-collector-network-policy
  namespace: observability
spec:
  podSelector:
    matchLabels:
      app: otel-collector
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: default
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: TCP
      port: 4317
    - protocol: TCP
      port: 4318
    - protocol: TCP
      port: 8888
    - protocol: TCP
      port: 13133
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: observability
    ports:
    - protocol: TCP
      port: 14250
    - protocol: TCP
      port: 9200
    - protocol: TCP
      port: 3100
```

#### 1.2 è®¿é—®æ§åˆ¶

```yaml
# è®¿é—®æ§åˆ¶é…ç½®
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-collector
  namespace: observability
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: otel-collector-role
  namespace: observability
rules:
- apiGroups: [""]
  resources: ["pods", "services", "endpoints"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: otel-collector-rolebinding
  namespace: observability
subjects:
- kind: ServiceAccount
  name: otel-collector
  namespace: observability
roleRef:
  kind: Role
  name: otel-collector-role
  apiGroup: rbac.authorization.k8s.io
```

### 2. æ•°æ®å®‰å…¨

#### 2.1 æ•°æ®åŠ å¯†

```yaml
# æ•°æ®åŠ å¯†é…ç½®
apiVersion: v1
kind: Secret
metadata:
  name: otel-collector-tls
  namespace: observability
type: kubernetes.io/tls
data:
  tls.crt: <base64-encoded-cert>
  tls.key: <base64-encoded-key>
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-security-config
  namespace: observability
data:
  security-config.yaml: |
    security:
      tls:
        enabled: true
        cert_file: "/etc/tls/tls.crt"
        key_file: "/etc/tls/tls.key"
        ca_file: "/etc/tls/ca.crt"
      
      authentication:
        enabled: true
        type: "mutual_tls"
        token: "bearer_token"
      
      authorization:
        enabled: true
        type: "rbac"
        policies:
          - name: "read_only"
            rules:
              - resources: ["traces", "metrics", "logs"]
                verbs: ["get", "list"]
          - name: "write_access"
            rules:
              - resources: ["traces", "metrics", "logs"]
                verbs: ["create", "update", "delete"]
      
      data_protection:
        encryption_at_rest: true
        encryption_in_transit: true
        data_masking: true
        pii_detection: true
```

---

## ğŸ“Š è¿ç»´ç›‘æ§

### 1. å¥åº·æ£€æŸ¥

#### 1.1 å¥åº·æ£€æŸ¥é…ç½®

```yaml
# å¥åº·æ£€æŸ¥é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: health-check-config
  namespace: observability
data:
  health-check.yaml: |
    health_check:
      enabled: true
      interval: 30s
      timeout: 10s
      retries: 3
      
      checks:
        - name: "collector_health"
          type: "http"
          endpoint: "http://localhost:13133/"
          expected_status: 200
          expected_response: "Server available"
        
        - name: "jaeger_connectivity"
          type: "tcp"
          endpoint: "jaeger:14250"
          timeout: 5s
        
        - name: "elasticsearch_connectivity"
          type: "http"
          endpoint: "http://elasticsearch:9200/_cluster/health"
          expected_status: 200
        
        - name: "prometheus_connectivity"
          type: "http"
          endpoint: "http://prometheus:9090/-/healthy"
          expected_status: 200
      
      alerts:
        - name: "collector_unhealthy"
          condition: "collector_health == false"
          severity: "critical"
          message: "OpenTelemetry Collector is unhealthy"
        
        - name: "jaeger_unreachable"
          condition: "jaeger_connectivity == false"
          severity: "warning"
          message: "Jaeger is unreachable"
        
        - name: "elasticsearch_unreachable"
          condition: "elasticsearch_connectivity == false"
          severity: "warning"
          message: "Elasticsearch is unreachable"
```

### 2. æ€§èƒ½ç›‘æ§

#### 2.1 æ€§èƒ½æŒ‡æ ‡é…ç½®

```yaml
# æ€§èƒ½æŒ‡æ ‡é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: performance-metrics-config
  namespace: observability
data:
  performance-metrics.yaml: |
    performance_metrics:
      enabled: true
      collection_interval: 10s
      
      system_metrics:
        cpu:
          enabled: true
          threshold: 80
        memory:
          enabled: true
          threshold: 80
        disk:
          enabled: true
          threshold: 85
        network:
          enabled: true
          threshold: 70
      
      application_metrics:
        throughput:
          enabled: true
          threshold: 1000
        latency:
          enabled: true
          threshold: 100
        error_rate:
          enabled: true
          threshold: 5
        queue_size:
          enabled: true
          threshold: 1000
      
      custom_metrics:
        - name: "otel_collector_spans_per_second"
          query: "rate(otelcol_receiver_accepted_spans[1m])"
          threshold: 10000
        - name: "otel_collector_metrics_per_second"
          query: "rate(otelcol_receiver_accepted_metric_points[1m])"
          threshold: 5000
        - name: "otel_collector_logs_per_second"
          query: "rate(otelcol_receiver_accepted_log_records[1m])"
          threshold: 1000
```

---

## ğŸ¯ éƒ¨ç½²è¿ç»´æœ€ä½³å®è·µ

### 1. éƒ¨ç½²æœ€ä½³å®è·µ

#### 1.1 ç¯å¢ƒç®¡ç†

- ä½¿ç”¨ç¯å¢ƒéš”ç¦»
- å®æ–½é…ç½®ç®¡ç†
- å»ºç«‹ç‰ˆæœ¬æ§åˆ¶
- å®æ–½è‡ªåŠ¨åŒ–éƒ¨ç½²

#### 1.2 èµ„æºç®¡ç†

- åˆç†åˆ†é…èµ„æº
- å®æ–½èµ„æºé™åˆ¶
- ç›‘æ§èµ„æºä½¿ç”¨
- ä¼˜åŒ–èµ„æºåˆ©ç”¨

#### 1.3 é«˜å¯ç”¨è®¾è®¡

- å¤šå‰¯æœ¬éƒ¨ç½²
- è´Ÿè½½å‡è¡¡
- æ•…éšœè½¬ç§»
- æ•°æ®å¤‡ä»½

### 2. è¿ç»´æœ€ä½³å®è·µ

#### 2.1 ç›‘æ§å‘Šè­¦

- å»ºç«‹å…¨é¢ç›‘æ§
- è®¾ç½®åˆç†å‘Šè­¦
- å®æ–½è‡ªåŠ¨æ¢å¤
- å»ºç«‹è¿ç»´æµç¨‹

#### 2.2 æ—¥å¿—ç®¡ç†

- é›†ä¸­æ—¥å¿—æ”¶é›†
- ç»“æ„åŒ–æ—¥å¿—
- æ—¥å¿—åˆ†æ
- æ—¥å¿—å½’æ¡£

#### 2.3 æ€§èƒ½ä¼˜åŒ–

- æŒç»­æ€§èƒ½ç›‘æ§
- å®šæœŸæ€§èƒ½è°ƒä¼˜
- å®¹é‡è§„åˆ’
- æ€§èƒ½æµ‹è¯•

### 3. å®‰å…¨æœ€ä½³å®è·µ

#### 3.1 ç½‘ç»œå®‰å…¨

- ç½‘ç»œéš”ç¦»
- è®¿é—®æ§åˆ¶
- æµé‡ç›‘æ§
- å®‰å…¨å®¡è®¡

#### 3.2 æ•°æ®å®‰å…¨

- æ•°æ®åŠ å¯†
- è®¿é—®æ§åˆ¶
- æ•°æ®å¤‡ä»½
- å®‰å…¨å®¡è®¡

---

## ğŸ‰ æ€»ç»“

æœ¬éƒ¨ç½²è¿ç»´æŒ‡å—æä¾›äº†ï¼š

1. **å®Œæ•´çš„éƒ¨ç½²æ–¹æ¡ˆ**: äº‘åŸç”Ÿã€è¾¹ç¼˜è®¡ç®—ã€æ··åˆäº‘éƒ¨ç½²
2. **å…¨é¢çš„è¿ç»´ç®¡ç†**: ç›‘æ§ã€å‘Šè­¦ã€æ—¥å¿—ã€æ€§èƒ½ä¼˜åŒ–
3. **ä¸¥æ ¼çš„å®‰å…¨æªæ–½**: ç½‘ç»œå®‰å…¨ã€è®¿é—®æ§åˆ¶ã€æ•°æ®å®‰å…¨
4. **æœ€ä½³å®è·µæŒ‡å¯¼**: éƒ¨ç½²ã€è¿ç»´ã€å®‰å…¨æœ€ä½³å®è·µ
5. **è‡ªåŠ¨åŒ–è¿ç»´**: è‡ªåŠ¨åŒ–éƒ¨ç½²ã€ç›‘æ§ã€å‘Šè­¦ã€æ¢å¤

è¿™ä¸ªæŒ‡å—å°†å¸®åŠ©ç”¨æˆ·æˆåŠŸéƒ¨ç½²å’Œè¿ç»´OpenTelemetryç³»ç»Ÿï¼Œç¡®ä¿ç³»ç»Ÿçš„é«˜å¯ç”¨æ€§ã€é«˜æ€§èƒ½å’Œé«˜å®‰å…¨æ€§ã€‚

---

*æ–‡æ¡£åˆ›å»ºæ—¶é—´: 2025å¹´1æœˆ*  
*åŸºäº2025å¹´æœ€æ–°æŠ€æœ¯å·¥ç¨‹æ–¹æ¡ˆæ ‡å‡†*  
*éƒ¨ç½²è¿ç»´çŠ¶æ€: âœ… ç”Ÿäº§å°±ç»ª*
