#!/usr/bin/env python3
"""
OpenTelemetry 2025 æ–‡æ¡£åŒæ­¥å·¥å…·

åŠŸèƒ½ï¼š
- åŒæ­¥æ–‡æ¡£åˆ°å¤šä¸ªç›®æ ‡
- è‡ªåŠ¨å¤‡ä»½æ–‡æ¡£
- ç‰ˆæœ¬æ§åˆ¶é›†æˆ
- æ–‡æ¡£å‘å¸ƒç®¡ç†
"""

import os
import shutil
import subprocess
import json
import hashlib
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from datetime import datetime
import argparse
import logging
import tempfile
import zipfile

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class SyncTarget:
    """åŒæ­¥ç›®æ ‡"""
    name: str
    type: str  # 'local', 'remote', 'git', 's3'
    path: str
    config: Dict[str, Any]

@dataclass
class SyncResult:
    """åŒæ­¥ç»“æœ"""
    target: str
    success: bool
    files_synced: int
    files_skipped: int
    files_failed: int
    error_message: Optional[str] = None
    duration: float = 0.0

@dataclass
class DocumentInfo:
    """æ–‡æ¡£ä¿¡æ¯"""
    path: str
    size: int
    modified: datetime
    hash: str
    status: str  # 'new', 'modified', 'unchanged', 'deleted'

class DocumentSynchronizer:
    """æ–‡æ¡£åŒæ­¥å™¨"""
    
    def __init__(self, doc_root: str = "doc"):
        self.doc_root = Path(doc_root)
        self.targets = self._load_sync_targets()
        self.sync_history = self._load_sync_history()
    
    def _load_sync_targets(self) -> List[SyncTarget]:
        """åŠ è½½åŒæ­¥ç›®æ ‡é…ç½®"""
        targets = []
        
        # æœ¬åœ°å¤‡ä»½ç›®æ ‡
        targets.append(SyncTarget(
            name="local_backup",
            type="local",
            path="./backup",
            config={
                "create_timestamp": True,
                "compress": True,
                "exclude_patterns": [".git", "__pycache__", "*.pyc"]
            }
        ))
        
        # Gitä»“åº“ç›®æ ‡
        targets.append(SyncTarget(
            name="git_repo",
            type="git",
            path=".",
            config={
                "remote": "origin",
                "branch": "main",
                "commit_message": "Auto-sync documents",
                "push": True
            }
        ))
        
        return targets
    
    def _load_sync_history(self) -> Dict[str, Any]:
        """åŠ è½½åŒæ­¥å†å²"""
        history_file = self.doc_root / ".sync_history.json"
        
        if history_file.exists():
            try:
                with open(history_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"åŠ è½½åŒæ­¥å†å²å¤±è´¥: {e}")
        
        return {
            "last_sync": None,
            "sync_count": 0,
            "targets": {}
        }
    
    def _save_sync_history(self):
        """ä¿å­˜åŒæ­¥å†å²"""
        history_file = self.doc_root / ".sync_history.json"
        
        try:
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(self.sync_history, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜åŒæ­¥å†å²å¤±è´¥: {e}")
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        try:
            with open(file_path, 'rb') as f:
                content = f.read()
                return hashlib.md5(content).hexdigest()
        except Exception as e:
            logger.error(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥ {file_path}: {e}")
            return ""
    
    def _get_document_info(self, file_path: Path) -> DocumentInfo:
        """è·å–æ–‡æ¡£ä¿¡æ¯"""
        stat = file_path.stat()
        
        return DocumentInfo(
            path=str(file_path.relative_to(self.doc_root)),
            size=stat.st_size,
            modified=datetime.fromtimestamp(stat.st_mtime),
            hash=self._calculate_file_hash(file_path),
            status="unknown"
        )
    
    def _scan_documents(self) -> List[DocumentInfo]:
        """æ‰«ææ–‡æ¡£"""
        documents = []
        md_files = list(self.doc_root.rglob("*.md"))
        
        for md_file in md_files:
            # è·³è¿‡éšè—æ–‡ä»¶å’Œä¸´æ—¶æ–‡ä»¶
            if any(part.startswith('.') for part in md_file.parts):
                continue
            
            doc_info = self._get_document_info(md_file)
            documents.append(doc_info)
        
        return documents
    
    def _sync_to_local(self, target: SyncTarget, documents: List[DocumentInfo]) -> SyncResult:
        """åŒæ­¥åˆ°æœ¬åœ°ç›®å½•"""
        start_time = datetime.now()
        result = SyncResult(
            target=target.name,
            success=True,
            files_synced=0,
            files_skipped=0,
            files_failed=0
        )
        
        try:
            target_path = Path(target.path)
            
            # åˆ›å»ºæ—¶é—´æˆ³ç›®å½•
            if target.config.get("create_timestamp", False):
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                target_path = target_path / timestamp
            
            target_path.mkdir(parents=True, exist_ok=True)
            
            # åŒæ­¥æ–‡ä»¶
            for doc in documents:
                try:
                    source_file = self.doc_root / doc.path
                    dest_file = target_path / doc.path
                    
                    # åˆ›å»ºç›®æ ‡ç›®å½•
                    dest_file.parent.mkdir(parents=True, exist_ok=True)
                    
                    # å¤åˆ¶æ–‡ä»¶
                    shutil.copy2(source_file, dest_file)
                    result.files_synced += 1
                    
                except Exception as e:
                    logger.error(f"åŒæ­¥æ–‡ä»¶å¤±è´¥ {doc.path}: {e}")
                    result.files_failed += 1
            
            # å‹ç¼©å¤‡ä»½
            if target.config.get("compress", False):
                self._compress_backup(target_path)
            
        except Exception as e:
            result.success = False
            result.error_message = str(e)
            logger.error(f"æœ¬åœ°åŒæ­¥å¤±è´¥: {e}")
        
        result.duration = (datetime.now() - start_time).total_seconds()
        return result
    
    def _sync_to_git(self, target: SyncTarget, documents: List[DocumentInfo]) -> SyncResult:
        """åŒæ­¥åˆ°Gitä»“åº“"""
        start_time = datetime.now()
        result = SyncResult(
            target=target.name,
            success=True,
            files_synced=0,
            files_skipped=0,
            files_failed=0
        )
        
        try:
            # æ£€æŸ¥GitçŠ¶æ€
            git_status = subprocess.run(
                ["git", "status", "--porcelain"],
                cwd=self.doc_root,
                capture_output=True,
                text=True
            )
            
            if git_status.returncode != 0:
                raise Exception(f"GitçŠ¶æ€æ£€æŸ¥å¤±è´¥: {git_status.stderr}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å˜æ›´
            if not git_status.stdout.strip():
                result.files_skipped = len(documents)
                logger.info("æ²¡æœ‰æ–‡ä»¶å˜æ›´ï¼Œè·³è¿‡GitåŒæ­¥")
                return result
            
            # æ·»åŠ æ‰€æœ‰å˜æ›´
            add_result = subprocess.run(
                ["git", "add", "."],
                cwd=self.doc_root,
                capture_output=True,
                text=True
            )
            
            if add_result.returncode != 0:
                raise Exception(f"Gitæ·»åŠ å¤±è´¥: {add_result.stderr}")
            
            # æäº¤å˜æ›´
            commit_message = target.config.get("commit_message", "Auto-sync documents")
            commit_result = subprocess.run(
                ["git", "commit", "-m", commit_message],
                cwd=self.doc_root,
                capture_output=True,
                text=True
            )
            
            if commit_result.returncode != 0:
                if "nothing to commit" in commit_result.stdout:
                    result.files_skipped = len(documents)
                    logger.info("æ²¡æœ‰å˜æ›´éœ€è¦æäº¤")
                else:
                    raise Exception(f"Gitæäº¤å¤±è´¥: {commit_result.stderr}")
            else:
                result.files_synced = len(documents)
                logger.info(f"Gitæäº¤æˆåŠŸ: {commit_result.stdout}")
            
            # æ¨é€åˆ°è¿œç¨‹ä»“åº“
            if target.config.get("push", False):
                push_result = subprocess.run(
                    ["git", "push", target.config.get("remote", "origin"), target.config.get("branch", "main")],
                    cwd=self.doc_root,
                    capture_output=True,
                    text=True
                )
                
                if push_result.returncode != 0:
                    raise Exception(f"Gitæ¨é€å¤±è´¥: {push_result.stderr}")
                
                logger.info("Gitæ¨é€æˆåŠŸ")
        
        except Exception as e:
            result.success = False
            result.error_message = str(e)
            logger.error(f"GitåŒæ­¥å¤±è´¥: {e}")
        
        result.duration = (datetime.now() - start_time).total_seconds()
        return result
    
    def _compress_backup(self, backup_path: Path):
        """å‹ç¼©å¤‡ä»½"""
        try:
            zip_path = backup_path.with_suffix('.zip')
            
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for file_path in backup_path.rglob('*'):
                    if file_path.is_file():
                        arcname = file_path.relative_to(backup_path)
                        zipf.write(file_path, arcname)
            
            # åˆ é™¤åŸå§‹ç›®å½•
            shutil.rmtree(backup_path)
            
            logger.info(f"å¤‡ä»½å·²å‹ç¼©: {zip_path}")
        
        except Exception as e:
            logger.error(f"å‹ç¼©å¤‡ä»½å¤±è´¥: {e}")
    
    def sync_all_targets(self) -> List[SyncResult]:
        """åŒæ­¥åˆ°æ‰€æœ‰ç›®æ ‡"""
        logger.info("å¼€å§‹åŒæ­¥åˆ°æ‰€æœ‰ç›®æ ‡...")
        
        # æ‰«ææ–‡æ¡£
        documents = self._scan_documents()
        logger.info(f"æ‰«æåˆ° {len(documents)} ä¸ªæ–‡æ¡£")
        
        results = []
        
        for target in self.targets:
            logger.info(f"åŒæ­¥åˆ°ç›®æ ‡: {target.name}")
            
            if target.type == "local":
                result = self._sync_to_local(target, documents)
            elif target.type == "git":
                result = self._sync_to_git(target, documents)
            else:
                logger.warning(f"ä¸æ”¯æŒçš„ç›®æ ‡ç±»å‹: {target.type}")
                continue
            
            results.append(result)
            
            # æ›´æ–°åŒæ­¥å†å²
            self.sync_history["targets"][target.name] = {
                "last_sync": datetime.now().isoformat(),
                "success": result.success,
                "files_synced": result.files_synced
            }
        
        # æ›´æ–°æ€»ä½“åŒæ­¥å†å²
        self.sync_history["last_sync"] = datetime.now().isoformat()
        self.sync_history["sync_count"] += 1
        
        # ä¿å­˜åŒæ­¥å†å²
        self._save_sync_history()
        
        return results
    
    def sync_to_target(self, target_name: str) -> Optional[SyncResult]:
        """åŒæ­¥åˆ°æŒ‡å®šç›®æ ‡"""
        target = next((t for t in self.targets if t.name == target_name), None)
        
        if not target:
            logger.error(f"ç›®æ ‡ä¸å­˜åœ¨: {target_name}")
            return None
        
        documents = self._scan_documents()
        
        if target.type == "local":
            return self._sync_to_local(target, documents)
        elif target.type == "git":
            return self._sync_to_git(target, documents)
        else:
            logger.error(f"ä¸æ”¯æŒçš„ç›®æ ‡ç±»å‹: {target.type}")
            return None
    
    def create_backup(self, backup_name: Optional[str] = None) -> str:
        """åˆ›å»ºå¤‡ä»½"""
        if not backup_name:
            backup_name = f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        backup_path = Path("./backups") / backup_name
        backup_path.mkdir(parents=True, exist_ok=True)
        
        # å¤åˆ¶æ‰€æœ‰æ–‡æ¡£
        documents = self._scan_documents()
        for doc in documents:
            source_file = self.doc_root / doc.path
            dest_file = backup_path / doc.path
            dest_file.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(source_file, dest_file)
        
        # å‹ç¼©å¤‡ä»½
        zip_path = backup_path.with_suffix('.zip')
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for file_path in backup_path.rglob('*'):
                if file_path.is_file():
                    arcname = file_path.relative_to(backup_path)
                    zipf.write(file_path, arcname)
        
        # åˆ é™¤åŸå§‹ç›®å½•
        shutil.rmtree(backup_path)
        
        logger.info(f"å¤‡ä»½å·²åˆ›å»º: {zip_path}")
        return str(zip_path)
    
    def restore_backup(self, backup_path: str) -> bool:
        """æ¢å¤å¤‡ä»½"""
        try:
            backup_file = Path(backup_path)
            
            if not backup_file.exists():
                logger.error(f"å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_path}")
                return False
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_path = Path(temp_dir)
                
                # è§£å‹å¤‡ä»½
                with zipfile.ZipFile(backup_file, 'r') as zipf:
                    zipf.extractall(temp_path)
                
                # å¤åˆ¶æ–‡ä»¶åˆ°æ–‡æ¡£ç›®å½•
                for file_path in temp_path.rglob('*'):
                    if file_path.is_file():
                        relative_path = file_path.relative_to(temp_path)
                        dest_file = self.doc_root / relative_path
                        dest_file.parent.mkdir(parents=True, exist_ok=True)
                        shutil.copy2(file_path, dest_file)
            
            logger.info(f"å¤‡ä»½å·²æ¢å¤: {backup_path}")
            return True
        
        except Exception as e:
            logger.error(f"æ¢å¤å¤‡ä»½å¤±è´¥: {e}")
            return False
    
    def generate_sync_report(self, results: List[SyncResult]) -> str:
        """ç”ŸæˆåŒæ­¥æŠ¥å‘Š"""
        report = []
        report.append("# æ–‡æ¡£åŒæ­¥æŠ¥å‘Š")
        report.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
        report.append("")
        
        # æ€»ä½“ç»Ÿè®¡
        total_synced = sum(r.files_synced for r in results)
        total_skipped = sum(r.files_skipped for r in results)
        total_failed = sum(r.files_failed for r in results)
        success_count = sum(1 for r in results if r.success)
        
        report.append("## ğŸ“Š åŒæ­¥ç»Ÿè®¡")
        report.append(f"- **æˆåŠŸç›®æ ‡**: {success_count}/{len(results)}")
        report.append(f"- **åŒæ­¥æ–‡ä»¶**: {total_synced}")
        report.append(f"- **è·³è¿‡æ–‡ä»¶**: {total_skipped}")
        report.append(f"- **å¤±è´¥æ–‡ä»¶**: {total_failed}")
        report.append("")
        
        # è¯¦ç»†ç»“æœ
        report.append("## ğŸ“‹ è¯¦ç»†ç»“æœ")
        for result in results:
            status = "âœ… æˆåŠŸ" if result.success else "âŒ å¤±è´¥"
            report.append(f"### {result.target} - {status}")
            report.append(f"- **åŒæ­¥æ–‡ä»¶**: {result.files_synced}")
            report.append(f"- **è·³è¿‡æ–‡ä»¶**: {result.files_skipped}")
            report.append(f"- **å¤±è´¥æ–‡ä»¶**: {result.files_failed}")
            report.append(f"- **è€—æ—¶**: {result.duration:.2f}ç§’")
            
            if result.error_message:
                report.append(f"- **é”™è¯¯ä¿¡æ¯**: {result.error_message}")
            
            report.append("")
        
        return "\n".join(report)
    
    def list_backups(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å¤‡ä»½"""
        backup_dir = Path("./backups")
        
        if not backup_dir.exists():
            return []
        
        backups = []
        for backup_file in backup_dir.glob("*.zip"):
            stat = backup_file.stat()
            backup_info = {
                "name": backup_file.name,
                "path": str(backup_file),
                "size": stat.st_size,
                "created": datetime.fromtimestamp(stat.st_ctime).isoformat()
            }
            backups.append(backup_info)
        
        return sorted(backups, key=lambda x: x["created"], reverse=True)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="OpenTelemetry 2025 æ–‡æ¡£åŒæ­¥å·¥å…·")
    parser.add_argument("--doc-root", default="doc", help="æ–‡æ¡£æ ¹ç›®å½•")
    parser.add_argument("--sync-all", action="store_true", help="åŒæ­¥åˆ°æ‰€æœ‰ç›®æ ‡")
    parser.add_argument("--sync-target", help="åŒæ­¥åˆ°æŒ‡å®šç›®æ ‡")
    parser.add_argument("--create-backup", help="åˆ›å»ºå¤‡ä»½")
    parser.add_argument("--restore-backup", help="æ¢å¤å¤‡ä»½")
    parser.add_argument("--list-backups", action="store_true", help="åˆ—å‡ºæ‰€æœ‰å¤‡ä»½")
    parser.add_argument("--report", help="ç”ŸæˆåŒæ­¥æŠ¥å‘Šæ–‡ä»¶")
    parser.add_argument("--verbose", "-v", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # åˆ›å»ºåŒæ­¥å™¨
    synchronizer = DocumentSynchronizer(args.doc_root)
    
    if args.sync_all:
        # åŒæ­¥åˆ°æ‰€æœ‰ç›®æ ‡
        results = synchronizer.sync_all_targets()
        
        # ç”ŸæˆæŠ¥å‘Š
        if args.report:
            report_content = synchronizer.generate_sync_report(results)
            with open(args.report, 'w', encoding='utf-8') as f:
                f.write(report_content)
            print(f"åŒæ­¥æŠ¥å‘Šå·²ç”Ÿæˆ: {args.report}")
        
        # è¾“å‡ºæ‘˜è¦
        success_count = sum(1 for r in results if r.success)
        total_synced = sum(r.files_synced for r in results)
        print(f"åŒæ­¥å®Œæˆ: {success_count}/{len(results)} ä¸ªç›®æ ‡æˆåŠŸï¼Œå…±åŒæ­¥ {total_synced} ä¸ªæ–‡ä»¶")
    
    elif args.sync_target:
        # åŒæ­¥åˆ°æŒ‡å®šç›®æ ‡
        result = synchronizer.sync_to_target(args.sync_target)
        if result:
            status = "æˆåŠŸ" if result.success else "å¤±è´¥"
            print(f"åŒæ­¥åˆ° {args.sync_target}: {status}")
            print(f"åŒæ­¥æ–‡ä»¶: {result.files_synced}, è·³è¿‡: {result.files_skipped}, å¤±è´¥: {result.files_failed}")
        else:
            print(f"åŒæ­¥åˆ° {args.sync_target} å¤±è´¥")
    
    elif args.create_backup:
        # åˆ›å»ºå¤‡ä»½
        backup_path = synchronizer.create_backup(args.create_backup)
        print(f"å¤‡ä»½å·²åˆ›å»º: {backup_path}")
    
    elif args.restore_backup:
        # æ¢å¤å¤‡ä»½
        if synchronizer.restore_backup(args.restore_backup):
            print(f"å¤‡ä»½å·²æ¢å¤: {args.restore_backup}")
        else:
            print(f"æ¢å¤å¤‡ä»½å¤±è´¥: {args.restore_backup}")
    
    elif args.list_backups:
        # åˆ—å‡ºå¤‡ä»½
        backups = synchronizer.list_backups()
        if backups:
            print("å¯ç”¨å¤‡ä»½:")
            for backup in backups:
                size_mb = backup["size"] / (1024 * 1024)
                print(f"  - {backup['name']} ({size_mb:.2f}MB) - {backup['created']}")
        else:
            print("æ²¡æœ‰æ‰¾åˆ°å¤‡ä»½æ–‡ä»¶")
    
    else:
        print("è¯·æŒ‡å®šæ“ä½œï¼š--sync-all, --sync-target, --create-backup, --restore-backup, æˆ– --list-backups")

if __name__ == "__main__":
    main()
