#!/usr/bin/env python3
"""
OpenTelemetry 2025 æ–‡æ¡£ç®¡ç†å·¥å…·
ç”¨äºè‡ªåŠ¨åŒ–æ–‡æ¡£ç®¡ç†ã€è´¨é‡æ£€æŸ¥å’Œé“¾æ¥éªŒè¯
"""

import os
import re
import hashlib
import yaml
import json
from pathlib import Path
from datetime import datetime
from collections import defaultdict
from typing import Dict, List, Set, Tuple, Optional

class DocumentManager:
    """æ–‡æ¡£ç®¡ç†å™¨"""
    
    def __init__(self, doc_root: str = "doc"):
        self.doc_root = Path(doc_root)
        self.config = self.load_config()
        self.duplicates = defaultdict(list)
        self.content_hash = {}
        self.link_issues = []
        self.quality_issues = []
    
    def load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_file = self.doc_root / "config" / "document_config.yaml"
        if config_file.exists():
            with open(config_file, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        else:
            return self.get_default_config()
    
    def get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "document_structure": {
                "00_é¡¹ç›®æ¦‚è§ˆ": ["README.md", "é¡¹ç›®ç« ç¨‹.md", "å¿«é€Ÿå¼€å§‹.md", "æ–‡æ¡£å¯¼èˆª.md"],
                "01_ç†è®ºåŸºç¡€": ["README.md", "æ•°å­¦åŸºç¡€.md", "å½¢å¼åŒ–éªŒè¯.md", "ç†è®ºè¯æ˜.md"],
                "02_æ ‡å‡†è§„èŒƒ": ["README.md", "å›½é™…æ ‡å‡†å¯¹é½.md", "è¡Œä¸šæ ‡å‡†.md", "åˆè§„æ¡†æ¶.md"],
                "03_æŠ€æœ¯æ¶æ„": ["README.md", "ç³»ç»Ÿæ¶æ„.md", "åè®®å®ç°.md", "å·¥å…·é“¾.md"],
                "04_åº”ç”¨å®è·µ": ["README.md", "è¡Œä¸šè§£å†³æ–¹æ¡ˆ.md", "éƒ¨ç½²æŒ‡å—.md", "æœ€ä½³å®è·µ.md"],
                "05_è´¨é‡ä¿è¯": ["README.md", "æµ‹è¯•æ¡†æ¶.md", "æ€§èƒ½åŸºå‡†.md", "è´¨é‡ç›‘æ§.md"],
                "06_ç¤¾åŒºç”Ÿæ€": ["README.md", "æ²»ç†æ¡†æ¶.md", "è´¡çŒ®æŒ‡å—.md", "å­¦æœ¯åˆä½œ.md"],
                "07_å•†ä¸šåŒ–": ["README.md", "å•†ä¸šæ¨¡å¼.md", "å¸‚åœºåˆ†æ.md", "å‘å±•è·¯çº¿.md"],
                "08_é™„å½•": ["æœ¯è¯­è¡¨.md", "å‚è€ƒæ–‡çŒ®.md", "é…ç½®ç¤ºä¾‹.md", "æ•…éšœæ’é™¤.md"]
            },
            "quality_standards": {
                "min_length": 100,
                "required_sections": ["æ ‡é¢˜", "æ¦‚è¿°", "å†…å®¹", "ç»“è®º"],
                "required_metadata": ["æœ€åæ›´æ–°", "æ–‡æ¡£ç‰ˆæœ¬", "ç»´æŠ¤è€…"],
                "link_check": True,
                "duplicate_check": True
            },
            "exclude_patterns": [
                "*.tmp",
                "*.bak",
                "*.old",
                ".git/*",
                "node_modules/*"
            ]
        }
    
    def detect_duplicates(self) -> Dict[str, List[Path]]:
        """æ£€æµ‹é‡å¤æ–‡æ¡£"""
        print("ğŸ” æ£€æµ‹é‡å¤æ–‡æ¡£...")
        
        for md_file in self.doc_root.rglob("*.md"):
            if self.should_exclude(md_file):
                continue
                
            try:
                content = md_file.read_text(encoding='utf-8')
                content_hash = hashlib.md5(content.encode()).hexdigest()
                
                if content_hash in self.content_hash:
                    self.duplicates[content_hash].append(md_file)
                else:
                    self.content_hash[content_hash] = md_file
            except Exception as e:
                print(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥: {md_file} - {e}")
        
        return dict(self.duplicates)
    
    def should_exclude(self, file_path: Path) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ’é™¤æ–‡ä»¶"""
        for pattern in self.config["exclude_patterns"]:
            if file_path.match(pattern):
                return True
        return False
    
    def consolidate_duplicates(self) -> Dict[str, str]:
        """æ•´åˆé‡å¤æ–‡æ¡£"""
        print("ğŸ”„ æ•´åˆé‡å¤æ–‡æ¡£...")
        
        consolidation_results = {}
        
        for content_hash, files in self.duplicates.items():
            if len(files) > 1:
                main_file = self.select_main_document(files)
                merged_content = self.merge_content(files)
                
                # å†™å…¥ä¸»æ–‡æ¡£
                main_file.write_text(merged_content, encoding='utf-8')
                consolidation_results[str(main_file)] = "å·²æ•´åˆ"
                
                # åˆ é™¤å…¶ä»–é‡å¤æ–‡æ¡£
                for file in files:
                    if file != main_file:
                        file.unlink()
                        consolidation_results[str(file)] = "å·²åˆ é™¤"
                        print(f"ğŸ—‘ï¸ åˆ é™¤é‡å¤æ–‡æ¡£: {file}")
        
        return consolidation_results
    
    def select_main_document(self, files: List[Path]) -> Path:
        """é€‰æ‹©ä¸»æ–‡æ¡£"""
        # ä¼˜å…ˆçº§ï¼šREADME > é¡¹ç›®æ¦‚è§ˆ > å…¶ä»–
        for file in files:
            if file.name == "README.md":
                return file
            if "é¡¹ç›®æ¦‚è§ˆ" in str(file):
                return file
        
        # é€‰æ‹©æœ€å®Œæ•´çš„æ–‡æ¡£
        max_length = 0
        main_file = files[0]
        
        for file in files:
            try:
                content = file.read_text(encoding='utf-8')
                if len(content) > max_length:
                    max_length = len(content)
                    main_file = file
            except:
                continue
        
        return main_file
    
    def merge_content(self, files: List[Path]) -> str:
        """åˆå¹¶æ–‡æ¡£å†…å®¹"""
        merged_sections = defaultdict(list)
        
        for file in files:
            try:
                content = file.read_text(encoding='utf-8')
                sections = self.parse_sections(content)
                
                for section, content in sections.items():
                    merged_sections[section].append(content)
            except:
                continue
        
        # åˆå¹¶å†…å®¹
        merged_content = ""
        for section, contents in merged_sections.items():
            merged_content += f"## {section}\n\n"
            for content in contents:
                merged_content += content + "\n\n"
        
        return merged_content
    
    def parse_sections(self, content: str) -> Dict[str, str]:
        """è§£ææ–‡æ¡£ç« èŠ‚"""
        sections = {}
        current_section = "æ¦‚è¿°"
        current_content = []
        
        for line in content.split('\n'):
            if line.startswith('#'):
                if current_content:
                    sections[current_section] = '\n'.join(current_content)
                current_section = line.strip('#').strip()
                current_content = []
            else:
                current_content.append(line)
        
        if current_content:
            sections[current_section] = '\n'.join(current_content)
        
        return sections
    
    def check_document_quality(self) -> List[Dict]:
        """æ£€æŸ¥æ–‡æ¡£è´¨é‡"""
        print("ğŸ“Š æ£€æŸ¥æ–‡æ¡£è´¨é‡...")
        
        quality_issues = []
        
        for md_file in self.doc_root.rglob("*.md"):
            if self.should_exclude(md_file):
                continue
            
            try:
                content = md_file.read_text(encoding='utf-8')
                issues = self.analyze_document_quality(md_file, content)
                quality_issues.extend(issues)
            except Exception as e:
                quality_issues.append({
                    "file": str(md_file),
                    "type": "è¯»å–é”™è¯¯",
                    "message": str(e),
                    "severity": "é«˜"
                })
        
        return quality_issues
    
    def analyze_document_quality(self, file_path: Path, content: str) -> List[Dict]:
        """åˆ†æå•ä¸ªæ–‡æ¡£è´¨é‡"""
        issues = []
        
        # æ£€æŸ¥æ–‡æ¡£é•¿åº¦
        if len(content) < self.config["quality_standards"]["min_length"]:
            issues.append({
                "file": str(file_path),
                "type": "é•¿åº¦ä¸è¶³",
                "message": f"æ–‡æ¡£é•¿åº¦ {len(content)} å­—ç¬¦ï¼Œå°‘äºæœ€å°è¦æ±‚ {self.config['quality_standards']['min_length']} å­—ç¬¦",
                "severity": "ä¸­"
            })
        
        # æ£€æŸ¥å¿…éœ€ç« èŠ‚
        required_sections = self.config["quality_standards"]["required_sections"]
        for section in required_sections:
            if section not in content:
                issues.append({
                    "file": str(file_path),
                    "type": "ç¼ºå°‘ç« èŠ‚",
                    "message": f"ç¼ºå°‘å¿…éœ€ç« èŠ‚: {section}",
                    "severity": "ä¸­"
                })
        
        # æ£€æŸ¥å…ƒæ•°æ®
        required_metadata = self.config["quality_standards"]["required_metadata"]
        for metadata in required_metadata:
            if metadata not in content:
                issues.append({
                    "file": str(file_path),
                    "type": "ç¼ºå°‘å…ƒæ•°æ®",
                    "message": f"ç¼ºå°‘å¿…éœ€å…ƒæ•°æ®: {metadata}",
                    "severity": "ä½"
                })
        
        # æ£€æŸ¥é“¾æ¥
        if self.config["quality_standards"]["link_check"]:
            link_issues = self.check_links(file_path, content)
            issues.extend(link_issues)
        
        return issues
    
    def check_links(self, file_path: Path, content: str) -> List[Dict]:
        """æ£€æŸ¥æ–‡æ¡£é“¾æ¥"""
        link_issues = []
        
        # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
        link_pattern = r'\[([^\]]+)\]\(([^)]+)\)'
        links = re.findall(link_pattern, content)
        
        for link_text, link_url in links:
            if link_url.startswith('http'):
                # å¤–éƒ¨é“¾æ¥ï¼Œæš‚æ—¶è·³è¿‡
                continue
            
            # å†…éƒ¨é“¾æ¥
            if link_url.startswith('../'):
                target_path = file_path.parent / link_url
            elif link_url.startswith('./'):
                target_path = file_path.parent / link_url
            else:
                target_path = file_path.parent / link_url
            
            if not target_path.exists():
                link_issues.append({
                    "file": str(file_path),
                    "type": "é“¾æ¥é”™è¯¯",
                    "message": f"é“¾æ¥ '{link_text}' æŒ‡å‘ä¸å­˜åœ¨çš„æ–‡ä»¶: {link_url}",
                    "severity": "é«˜"
                })
        
        return link_issues
    
    def restructure_documents(self) -> Dict[str, str]:
        """é‡ç»„æ–‡æ¡£ç»“æ„"""
        print("ğŸ—ï¸ é‡ç»„æ–‡æ¡£ç»“æ„...")
        
        restructure_results = {}
        
        # åˆ›å»ºæ–°ç›®å½•ç»“æ„
        for new_dir in self.config["document_structure"].keys():
            target_dir = self.doc_root / new_dir
            target_dir.mkdir(exist_ok=True)
            restructure_results[new_dir] = "ç›®å½•å·²åˆ›å»º"
        
        # ç§»åŠ¨æ–‡æ¡£åˆ°å¯¹åº”ç›®å½•
        for dir_name, expected_files in self.config["document_structure"].items():
            target_dir = self.doc_root / dir_name
            
            for expected_file in expected_files:
                target_path = target_dir / expected_file
                
                # æŸ¥æ‰¾ç°æœ‰æ–‡ä»¶
                existing_file = self.find_existing_file(expected_file)
                
                if existing_file:
                    # ç§»åŠ¨ç°æœ‰æ–‡ä»¶
                    shutil.move(str(existing_file), str(target_path))
                    restructure_results[str(target_path)] = "æ–‡ä»¶å·²ç§»åŠ¨"
                else:
                    # åˆ›å»ºæ–°æ–‡ä»¶
                    target_path.touch()
                    restructure_results[str(target_path)] = "æ–‡ä»¶å·²åˆ›å»º"
        
        return restructure_results
    
    def find_existing_file(self, filename: str) -> Optional[Path]:
        """æŸ¥æ‰¾ç°æœ‰æ–‡ä»¶"""
        for md_file in self.doc_root.rglob(filename):
            if not self.should_exclude(md_file):
                return md_file
        return None
    
    def generate_navigation(self) -> str:
        """ç”Ÿæˆæ–‡æ¡£å¯¼èˆª"""
        print("ğŸ§­ ç”Ÿæˆæ–‡æ¡£å¯¼èˆª...")
        
        navigation = "# OpenTelemetry 2025 æ–‡æ¡£å¯¼èˆª\n\n"
        navigation += f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}\n\n"
        
        for dir_name, files in self.config["document_structure"].items():
            navigation += f"## {dir_name}\n\n"
            
            for file in files:
                file_path = self.doc_root / dir_name / file
                if file_path.exists():
                    navigation += f"- [{file}]({dir_name}/{file})\n"
                else:
                    navigation += f"- [{file}]({dir_name}/{file}) âš ï¸ å¾…åˆ›å»º\n"
            
            navigation += "\n"
        
        return navigation
    
    def generate_quality_report(self) -> str:
        """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
        print("ğŸ“‹ ç”Ÿæˆè´¨é‡æŠ¥å‘Š...")
        
        report = "# OpenTelemetry 2025 æ–‡æ¡£è´¨é‡æŠ¥å‘Š\n\n"
        report += f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}\n\n"
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_files = len(list(self.doc_root.rglob("*.md")))
        duplicate_count = sum(len(files) for files in self.duplicates.values() if len(files) > 1)
        
        report += "## ğŸ“Š ç»Ÿè®¡ä¿¡æ¯\n\n"
        report += f"- **æ€»æ–‡æ¡£æ•°é‡**: {total_files}\n"
        report += f"- **é‡å¤æ–‡æ¡£æ•°é‡**: {duplicate_count}\n"
        report += f"- **è´¨é‡é—®é¢˜æ•°é‡**: {len(self.quality_issues)}\n\n"
        
        # é‡å¤æ–‡æ¡£
        if self.duplicates:
            report += "## ğŸ”„ é‡å¤æ–‡æ¡£\n\n"
            for content_hash, files in self.duplicates.items():
                if len(files) > 1:
                    report += f"### é‡å¤ç»„ {content_hash[:8]}...\n\n"
                    for file in files:
                        report += f"- {file}\n"
                    report += "\n"
        
        # è´¨é‡é—®é¢˜
        if self.quality_issues:
            report += "## âš ï¸ è´¨é‡é—®é¢˜\n\n"
            
            # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„
            severity_groups = defaultdict(list)
            for issue in self.quality_issues:
                severity_groups[issue["severity"]].append(issue)
            
            for severity in ["é«˜", "ä¸­", "ä½"]:
                if severity in severity_groups:
                    report += f"### {severity}ä¼˜å…ˆçº§é—®é¢˜\n\n"
                    for issue in severity_groups[severity]:
                        report += f"- **{issue['file']}**: {issue['message']}\n"
                    report += "\n"
        
        return report
    
    def run_full_analysis(self) -> Dict:
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸš€ å¼€å§‹å®Œæ•´æ–‡æ¡£åˆ†æ...")
        
        results = {
            "timestamp": datetime.now().isoformat(),
            "duplicates": self.detect_duplicates(),
            "quality_issues": self.check_document_quality(),
            "consolidation_results": {},
            "restructure_results": {},
            "navigation": "",
            "quality_report": ""
        }
        
        # æ•´åˆé‡å¤æ–‡æ¡£
        if results["duplicates"]:
            results["consolidation_results"] = self.consolidate_duplicates()
        
        # é‡ç»„æ–‡æ¡£ç»“æ„
        results["restructure_results"] = self.restructure_documents()
        
        # ç”Ÿæˆå¯¼èˆª
        results["navigation"] = self.generate_navigation()
        
        # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
        results["quality_report"] = self.generate_quality_report()
        
        return results
    
    def save_results(self, results: Dict):
        """ä¿å­˜åˆ†æç»“æœ"""
        # ä¿å­˜å¯¼èˆªæ–‡æ¡£
        nav_file = self.doc_root / "00_é¡¹ç›®æ¦‚è§ˆ" / "æ–‡æ¡£å¯¼èˆª.md"
        nav_file.write_text(results["navigation"], encoding='utf-8')
        
        # ä¿å­˜è´¨é‡æŠ¥å‘Š
        report_file = self.doc_root / "è´¨é‡æŠ¥å‘Š.md"
        report_file.write_text(results["quality_report"], encoding='utf-8')
        
        # ä¿å­˜JSONç»“æœ
        json_file = self.doc_root / "åˆ†æç»“æœ.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°:")
        print(f"  - å¯¼èˆªæ–‡æ¡£: {nav_file}")
        print(f"  - è´¨é‡æŠ¥å‘Š: {report_file}")
        print(f"  - JSONç»“æœ: {json_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ OpenTelemetry 2025 æ–‡æ¡£ç®¡ç†å·¥å…·")
    print("=" * 50)
    
    # åˆ›å»ºæ–‡æ¡£ç®¡ç†å™¨
    manager = DocumentManager()
    
    # è¿è¡Œå®Œæ•´åˆ†æ
    results = manager.run_full_analysis()
    
    # ä¿å­˜ç»“æœ
    manager.save_results(results)
    
    # è¾“å‡ºæ‘˜è¦
    print("\nğŸ“Š åˆ†ææ‘˜è¦:")
    print(f"  - é‡å¤æ–‡æ¡£ç»„: {len(results['duplicates'])}")
    print(f"  - è´¨é‡é—®é¢˜: {len(results['quality_issues'])}")
    print(f"  - æ•´åˆæ–‡æ¡£: {len(results['consolidation_results'])}")
    print(f"  - é‡ç»„ç›®å½•: {len(results['restructure_results'])}")
    
    print("\nâœ… æ–‡æ¡£ç®¡ç†å®Œæˆ!")

if __name__ == "__main__":
    main()
