# æ¦‚ç‡è®ºä¸ç»Ÿè®¡åˆ†æ

## ğŸ“Š æ¦‚è¿°

æ¦‚ç‡è®ºä¸ç»Ÿè®¡åˆ†æä¸ºOpenTelemetryå¯è§‚æµ‹æ€§ç³»ç»Ÿæä¾›äº†æ•°æ®å»ºæ¨¡ã€éšæœºè¿‡ç¨‹åˆ†æã€ç»Ÿè®¡æ¨æ–­å’Œé¢„æµ‹åˆ†æçš„ç†è®ºåŸºç¡€ï¼Œç‰¹åˆ«æ˜¯åœ¨æ€§èƒ½åˆ†æã€å¼‚å¸¸æ£€æµ‹ã€å®¹é‡è§„åˆ’å’Œç³»ç»Ÿä¼˜åŒ–æ–¹é¢å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚

## ğŸ”¢ æ ¸å¿ƒæ¦‚å¿µ

### 1. æ¦‚ç‡åˆ†å¸ƒ

#### å¸¸ç”¨æ¦‚ç‡åˆ†å¸ƒ

```mathematical
// æ­£æ€åˆ†å¸ƒ
f(x) = (1/âˆš(2Ï€ÏƒÂ²)) e^(-(x-Î¼)Â²/(2ÏƒÂ²))

// æŒ‡æ•°åˆ†å¸ƒ
f(x) = Î»e^(-Î»x), x â‰¥ 0

// æ³Šæ¾åˆ†å¸ƒ
P(X = k) = (Î»^k e^(-Î»)) / k!

// å¨å¸ƒå°”åˆ†å¸ƒ
f(x) = (k/Î»)(x/Î»)^(k-1) e^(-(x/Î»)^k)
```

#### å¯è§‚æµ‹æ€§æ•°æ®çš„æ¦‚ç‡åˆ†å¸ƒ

```mathematical
// å“åº”æ—¶é—´åˆ†å¸ƒï¼ˆé€šå¸¸æœä»å¯¹æ•°æ­£æ€åˆ†å¸ƒï¼‰
f(t) = (1/(tÏƒâˆš(2Ï€))) e^(-(ln(t)-Î¼)Â²/(2ÏƒÂ²))

// é”™è¯¯ç‡åˆ†å¸ƒï¼ˆé€šå¸¸æœä»äºŒé¡¹åˆ†å¸ƒï¼‰
P(X = k) = C(n,k) p^k (1-p)^(n-k)

// ååé‡åˆ†å¸ƒï¼ˆé€šå¸¸æœä»æ³Šæ¾åˆ†å¸ƒï¼‰
P(X = k) = (Î»^k e^(-Î»)) / k!

// ç³»ç»Ÿè´Ÿè½½åˆ†å¸ƒï¼ˆé€šå¸¸æœä»ä¼½é©¬åˆ†å¸ƒï¼‰
f(x) = (1/(Î“(Î±)Î²^Î±)) x^(Î±-1) e^(-x/Î²)
```

### 2. éšæœºè¿‡ç¨‹

#### é©¬å°”å¯å¤«è¿‡ç¨‹

```mathematical
// é©¬å°”å¯å¤«æ€§è´¨
P(X_{n+1} = x | X_n = x_n, ..., X_1 = x_1) = P(X_{n+1} = x | X_n = x_n)

// è½¬ç§»æ¦‚ç‡çŸ©é˜µ
P = [p_{ij}] where p_{ij} = P(X_{n+1} = j | X_n = i)

// ç¨³æ€æ¦‚ç‡
Ï€ = Ï€P, Î£Ï€_i = 1
```

#### æ³Šæ¾è¿‡ç¨‹

```mathematical
// æ³Šæ¾è¿‡ç¨‹å®šä¹‰
P(N(t) = k) = (Î»t)^k e^(-Î»t) / k!

// åˆ°è¾¾é—´éš”æ—¶é—´
P(T > t) = e^(-Î»t)

// å¤åˆæ³Šæ¾è¿‡ç¨‹
X(t) = Î£_{i=1}^{N(t)} Y_i
```

### 3. ç»Ÿè®¡æ¨æ–­

#### å‚æ•°ä¼°è®¡

```mathematical
// æœ€å¤§ä¼¼ç„¶ä¼°è®¡
Î¸Ì‚ = argmax L(Î¸) = argmax âˆ f(x_i | Î¸)

// è´å¶æ–¯ä¼°è®¡
Ï€(Î¸ | x) = f(x | Î¸) Ï€(Î¸) / f(x)

// ç½®ä¿¡åŒºé—´
P(Î¸ âˆˆ [Î¸Ì‚ - z_{Î±/2} SE(Î¸Ì‚), Î¸Ì‚ + z_{Î±/2} SE(Î¸Ì‚)]) = 1 - Î±
```

#### å‡è®¾æ£€éªŒ

```mathematical
// æ˜¾è‘—æ€§æ£€éªŒ
H_0: Î¸ = Î¸_0 vs H_1: Î¸ â‰  Î¸_0

// æ£€éªŒç»Ÿè®¡é‡
T = (Î¸Ì‚ - Î¸_0) / SE(Î¸Ì‚)

// på€¼
p = P(|T| > |t_{obs}| | H_0)
```

## ğŸ¯ åº”ç”¨åœºæ™¯

### 1. æ€§èƒ½åˆ†æ

#### å“åº”æ—¶é—´åˆ†æ

```python
class ResponseTimeAnalyzer:
    def __init__(self):
        self.distributions = {
            'normal': NormalDistribution(),
            'lognormal': LogNormalDistribution(),
            'exponential': ExponentialDistribution(),
            'weibull': WeibullDistribution()
        }
    
    def fit_response_time_distribution(self, response_times):
        """æ‹Ÿåˆå“åº”æ—¶é—´åˆ†å¸ƒ"""
        best_fit = None
        best_aic = float('inf')
        
        for dist_name, distribution in self.distributions.items():
            try:
                # æ‹Ÿåˆåˆ†å¸ƒå‚æ•°
                params = distribution.fit(response_times)
                
                # è®¡ç®—AIC
                aic = self.calculate_aic(response_times, distribution, params)
                
                if aic < best_aic:
                    best_aic = aic
                    best_fit = {
                        'distribution': dist_name,
                        'parameters': params,
                        'aic': aic
                    }
            except Exception as e:
                continue
        
        return best_fit
    
    def calculate_aic(self, data, distribution, params):
        """è®¡ç®—AIC"""
        log_likelihood = distribution.logpdf(data, *params).sum()
        k = len(params)  # å‚æ•°ä¸ªæ•°
        n = len(data)    # æ ·æœ¬å¤§å°
        
        aic = 2 * k - 2 * log_likelihood
        return aic
    
    def predict_percentiles(self, distribution, params, percentiles):
        """é¢„æµ‹ç™¾åˆ†ä½æ•°"""
        return distribution.ppf(percentiles, *params)
    
    def calculate_sla_compliance(self, response_times, sla_threshold):
        """è®¡ç®—SLAåˆè§„æ€§"""
        # æ‹Ÿåˆåˆ†å¸ƒ
        best_fit = self.fit_response_time_distribution(response_times)
        distribution = self.distributions[best_fit['distribution']]
        params = best_fit['parameters']
        
        # è®¡ç®—SLAåˆè§„æ¦‚ç‡
        compliance_prob = distribution.cdf(sla_threshold, *params)
        
        return {
            'compliance_probability': compliance_prob,
            'sla_threshold': sla_threshold,
            'distribution': best_fit['distribution'],
            'parameters': params
        }
```

#### ååé‡åˆ†æ

```python
class ThroughputAnalyzer:
    def __init__(self):
        self.poisson_model = PoissonModel()
        self.compound_poisson = CompoundPoissonModel()
    
    def analyze_throughput_pattern(self, throughput_data):
        """åˆ†æååé‡æ¨¡å¼"""
        # è®¡ç®—åŸºæœ¬ç»Ÿè®¡é‡
        mean_throughput = np.mean(throughput_data)
        variance_throughput = np.var(throughput_data)
        
        # æ£€éªŒæ³Šæ¾åˆ†å¸ƒå‡è®¾
        poisson_test = self.test_poisson_distribution(throughput_data)
        
        # åˆ†ææ—¶é—´è¶‹åŠ¿
        trend_analysis = self.analyze_trend(throughput_data)
        
        # æ£€æµ‹å‘¨æœŸæ€§
        periodicity = self.detect_periodicity(throughput_data)
        
        return {
            'mean': mean_throughput,
            'variance': variance_throughput,
            'poisson_test': poisson_test,
            'trend': trend_analysis,
            'periodicity': periodicity
        }
    
    def test_poisson_distribution(self, data):
        """æ£€éªŒæ³Šæ¾åˆ†å¸ƒ"""
        from scipy.stats import poisson, chi2_contingency
        
        # ä¼°è®¡å‚æ•°
        lambda_est = np.mean(data)
        
        # è®¡ç®—æœŸæœ›é¢‘æ•°
        max_val = int(np.max(data))
        observed = np.bincount(data.astype(int), minlength=max_val+1)
        expected = [len(data) * poisson.pmf(i, lambda_est) for i in range(max_val+1)]
        
        # å¡æ–¹æ£€éªŒ
        chi2_stat, p_value = chi2_contingency([observed, expected])[:2]
        
        return {
            'lambda_estimate': lambda_est,
            'chi2_statistic': chi2_stat,
            'p_value': p_value,
            'is_poisson': p_value > 0.05
        }
    
    def predict_throughput(self, historical_data, forecast_horizon):
        """é¢„æµ‹ååé‡"""
        # æ—¶é—´åºåˆ—åˆ†è§£
        trend, seasonal, residual = self.decompose_time_series(historical_data)
        
        # é¢„æµ‹è¶‹åŠ¿
        trend_forecast = self.forecast_trend(trend, forecast_horizon)
        
        # é¢„æµ‹å­£èŠ‚æ€§
        seasonal_forecast = self.forecast_seasonal(seasonal, forecast_horizon)
        
        # é¢„æµ‹æ®‹å·®
        residual_forecast = self.forecast_residual(residual, forecast_horizon)
        
        # ç»„åˆé¢„æµ‹
        total_forecast = trend_forecast + seasonal_forecast + residual_forecast
        
        return {
            'forecast': total_forecast,
            'trend': trend_forecast,
            'seasonal': seasonal_forecast,
            'residual': residual_forecast,
            'confidence_interval': self.calculate_confidence_interval(total_forecast)
        }
```

### 2. å¼‚å¸¸æ£€æµ‹

#### ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹

```python
class StatisticalAnomalyDetector:
    def __init__(self):
        self.methods = {
            'z_score': ZScoreDetector(),
            'isolation_forest': IsolationForestDetector(),
            'one_class_svm': OneClassSVMDetector(),
            'local_outlier_factor': LOFDetector()
        }
    
    def detect_anomalies(self, data, method='z_score', threshold=3.0):
        """æ£€æµ‹å¼‚å¸¸"""
        detector = self.methods[method]
        anomalies = detector.detect(data, threshold)
        
        return {
            'anomalies': anomalies,
            'method': method,
            'threshold': threshold,
            'anomaly_rate': len(anomalies) / len(data)
        }
    
    def multivariate_anomaly_detection(self, data_matrix):
        """å¤šå…ƒå¼‚å¸¸æ£€æµ‹"""
        from sklearn.covariance import EmpiricalCovariance
        from scipy.stats import chi2
        
        # ä¼°è®¡åæ–¹å·®çŸ©é˜µ
        cov = EmpiricalCovariance().fit(data_matrix)
        
        # è®¡ç®—é©¬æ°è·ç¦»
        mahalanobis_distances = cov.mahalanobis(data_matrix)
        
        # è®¡ç®—å¼‚å¸¸é˜ˆå€¼ï¼ˆåŸºäºå¡æ–¹åˆ†å¸ƒï¼‰
        threshold = chi2.ppf(0.95, data_matrix.shape[1])
        
        # è¯†åˆ«å¼‚å¸¸
        anomalies = mahalanobis_distances > threshold
        
        return {
            'anomalies': anomalies,
            'mahalanobis_distances': mahalanobis_distances,
            'threshold': threshold,
            'anomaly_rate': np.mean(anomalies)
        }
    
    def time_series_anomaly_detection(self, time_series):
        """æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹"""
        # æ»‘åŠ¨çª—å£ç»Ÿè®¡
        window_size = 30
        rolling_mean = pd.Series(time_series).rolling(window_size).mean()
        rolling_std = pd.Series(time_series).rolling(window_size).std()
        
        # è®¡ç®—zåˆ†æ•°
        z_scores = (time_series - rolling_mean) / rolling_std
        
        # è¯†åˆ«å¼‚å¸¸
        anomalies = np.abs(z_scores) > 3.0
        
        # æ£€æµ‹çªå˜ç‚¹
        change_points = self.detect_change_points(time_series)
        
        return {
            'anomalies': anomalies,
            'z_scores': z_scores,
            'change_points': change_points,
            'rolling_mean': rolling_mean,
            'rolling_std': rolling_std
        }
    
    def detect_change_points(self, time_series):
        """æ£€æµ‹çªå˜ç‚¹"""
        from ruptures import Pelt, Binseg
        
        # ä½¿ç”¨PELTç®—æ³•æ£€æµ‹çªå˜ç‚¹
        model = Pelt(model="rbf").fit(time_series)
        change_points = model.predict(pen=10)
        
        return change_points
```

#### è´å¶æ–¯å¼‚å¸¸æ£€æµ‹

```python
class BayesianAnomalyDetector:
    def __init__(self):
        self.prior_distributions = {}
        self.posterior_distributions = {}
    
    def update_belief(self, data, prior_params):
        """æ›´æ–°è´å¶æ–¯ä¿¡å¿µ"""
        # è®¡ç®—åéªŒåˆ†å¸ƒ
        posterior_params = self.calculate_posterior(data, prior_params)
        
        # æ›´æ–°åˆ†å¸ƒ
        self.posterior_distributions = posterior_params
        
        return posterior_params
    
    def calculate_posterior(self, data, prior_params):
        """è®¡ç®—åéªŒåˆ†å¸ƒ"""
        # å¯¹äºæ­£æ€åˆ†å¸ƒï¼Œä½¿ç”¨å…±è½­å…ˆéªŒ
        if prior_params['distribution'] == 'normal':
            n = len(data)
            x_bar = np.mean(data)
            s_squared = np.var(data, ddof=1)
            
            # å…ˆéªŒå‚æ•°
            mu_0 = prior_params['mu_0']
            lambda_0 = prior_params['lambda_0']
            alpha_0 = prior_params['alpha_0']
            beta_0 = prior_params['beta_0']
            
            # åéªŒå‚æ•°
            lambda_n = lambda_0 + n
            mu_n = (lambda_0 * mu_0 + n * x_bar) / lambda_n
            alpha_n = alpha_0 + n / 2
            beta_n = beta_0 + 0.5 * (n * s_squared + 
                                    lambda_0 * n * (x_bar - mu_0)**2 / lambda_n)
            
            return {
                'distribution': 'normal',
                'mu_n': mu_n,
                'lambda_n': lambda_n,
                'alpha_n': alpha_n,
                'beta_n': beta_n
            }
    
    def detect_anomalies_bayesian(self, new_data, threshold=0.05):
        """è´å¶æ–¯å¼‚å¸¸æ£€æµ‹"""
        anomalies = []
        
        for i, data_point in enumerate(new_data):
            # è®¡ç®—å¼‚å¸¸æ¦‚ç‡
            anomaly_prob = self.calculate_anomaly_probability(data_point)
            
            if anomaly_prob > threshold:
                anomalies.append({
                    'index': i,
                    'value': data_point,
                    'anomaly_probability': anomaly_prob
                })
        
        return anomalies
    
    def calculate_anomaly_probability(self, data_point):
        """è®¡ç®—å¼‚å¸¸æ¦‚ç‡"""
        if not self.posterior_distributions:
            return 0.5  # æ— å…ˆéªŒä¿¡æ¯æ—¶è¿”å›ä¸­æ€§æ¦‚ç‡
        
        # è®¡ç®—é¢„æµ‹åˆ†å¸ƒ
        predictive_dist = self.calculate_predictive_distribution()
        
        # è®¡ç®—å¼‚å¸¸æ¦‚ç‡
        if predictive_dist['distribution'] == 'normal':
            mean = predictive_dist['mean']
            std = predictive_dist['std']
            z_score = abs(data_point - mean) / std
            anomaly_prob = 2 * (1 - stats.norm.cdf(z_score))
        
        return anomaly_prob
```

### 3. å®¹é‡è§„åˆ’

#### æ’é˜Ÿè®ºåˆ†æ

```python
class QueueingTheoryAnalyzer:
    def __init__(self):
        self.queue_models = {
            'M/M/1': MMCQueue(),
            'M/M/c': MMCQueue(),
            'M/G/1': MG1Queue(),
            'G/G/1': GG1Queue()
        }
    
    def analyze_mm1_queue(self, arrival_rate, service_rate):
        """åˆ†æM/M/1é˜Ÿåˆ—"""
        # è®¡ç®—åˆ©ç”¨ç‡
        rho = arrival_rate / service_rate
        
        if rho >= 1:
            return {'error': 'System is unstable'}
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        avg_customers = rho / (1 - rho)
        avg_waiting_time = rho / (service_rate * (1 - rho))
        avg_response_time = 1 / (service_rate * (1 - rho))
        
        return {
            'utilization': rho,
            'avg_customers': avg_customers,
            'avg_waiting_time': avg_waiting_time,
            'avg_response_time': avg_response_time,
            'throughput': arrival_rate
        }
    
    def analyze_mmc_queue(self, arrival_rate, service_rate, servers):
        """åˆ†æM/M/cé˜Ÿåˆ—"""
        rho = arrival_rate / (servers * service_rate)
        
        if rho >= 1:
            return {'error': 'System is unstable'}
        
        # è®¡ç®—Erlang-Cå…¬å¼
        erlang_c = self.calculate_erlang_c(arrival_rate, service_rate, servers)
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        avg_waiting_time = erlang_c / (servers * service_rate * (1 - rho))
        avg_response_time = avg_waiting_time + 1 / service_rate
        avg_customers = arrival_rate * avg_response_time
        
        return {
            'utilization': rho,
            'erlang_c': erlang_c,
            'avg_customers': avg_customers,
            'avg_waiting_time': avg_waiting_time,
            'avg_response_time': avg_response_time,
            'throughput': arrival_rate
        }
    
    def calculate_erlang_c(self, arrival_rate, service_rate, servers):
        """è®¡ç®—Erlang-Cå…¬å¼"""
        rho = arrival_rate / (servers * service_rate)
        
        # è®¡ç®—P0
        p0 = 1 / (1 + sum((arrival_rate / service_rate)**k / math.factorial(k) 
                         for k in range(servers)) + 
                  (arrival_rate / service_rate)**servers / 
                  (math.factorial(servers) * (1 - rho)))
        
        # è®¡ç®—Erlang-C
        erlang_c = ((arrival_rate / service_rate)**servers / 
                   (math.factorial(servers) * (1 - rho))) * p0
        
        return erlang_c
    
    def capacity_planning(self, current_load, target_performance):
        """å®¹é‡è§„åˆ’"""
        # åˆ†æå½“å‰æ€§èƒ½
        current_performance = self.analyze_current_performance(current_load)
        
        # è®¡ç®—æ‰€éœ€å®¹é‡
        required_capacity = self.calculate_required_capacity(
            current_load, target_performance
        )
        
        # ç”Ÿæˆæ‰©å®¹å»ºè®®
        scaling_recommendations = self.generate_scaling_recommendations(
            current_performance, required_capacity
        )
        
        return {
            'current_performance': current_performance,
            'required_capacity': required_capacity,
            'scaling_recommendations': scaling_recommendations
        }
```

## ğŸ”§ æ€§èƒ½ä¼˜åŒ–

### 1. ç»Ÿè®¡è®¡ç®—ä¼˜åŒ–

#### å¿«é€Ÿç»Ÿè®¡è®¡ç®—

```python
class FastStatisticsCalculator:
    def __init__(self):
        self.running_stats = {}
    
    def update_running_statistics(self, data_stream):
        """æ›´æ–°è¿è¡Œç»Ÿè®¡é‡"""
        for data_point in data_stream:
            self.update_statistics(data_point)
    
    def update_statistics(self, value):
        """æ›´æ–°å•ä¸ªç»Ÿè®¡é‡"""
        if 'count' not in self.running_stats:
            self.running_stats = {
                'count': 0,
                'sum': 0,
                'sum_squares': 0,
                'min': float('inf'),
                'max': float('-inf')
            }
        
        stats = self.running_stats
        stats['count'] += 1
        stats['sum'] += value
        stats['sum_squares'] += value * value
        stats['min'] = min(stats['min'], value)
        stats['max'] = max(stats['max'], value)
    
    def get_statistics(self):
        """è·å–ç»Ÿè®¡é‡"""
        if self.running_stats['count'] == 0:
            return {}
        
        count = self.running_stats['count']
        mean = self.running_stats['sum'] / count
        variance = (self.running_stats['sum_squares'] / count) - (mean * mean)
        std_dev = math.sqrt(variance)
        
        return {
            'count': count,
            'mean': mean,
            'variance': variance,
            'std_dev': std_dev,
            'min': self.running_stats['min'],
            'max': self.running_stats['max']
        }
```

#### å¢é‡ç»Ÿè®¡è®¡ç®—

```python
class IncrementalStatistics:
    def __init__(self):
        self.n = 0
        self.mean = 0
        self.M2 = 0  # äºŒé˜¶ä¸­å¿ƒçŸ©
        self.min_val = float('inf')
        self.max_val = float('-inf')
    
    def add_value(self, value):
        """æ·»åŠ å€¼"""
        self.n += 1
        delta = value - self.mean
        self.mean += delta / self.n
        delta2 = value - self.mean
        self.M2 += delta * delta2
        self.min_val = min(self.min_val, value)
        self.max_val = max(self.max_val, value)
    
    def remove_value(self, value):
        """ç§»é™¤å€¼"""
        if self.n <= 0:
            return
        
        self.n -= 1
        if self.n == 0:
            self.mean = 0
            self.M2 = 0
            self.min_val = float('inf')
            self.max_val = float('-inf')
            return
        
        delta = value - self.mean
        self.mean -= delta / self.n
        delta2 = value - self.mean
        self.M2 -= delta * delta2
    
    def get_variance(self):
        """è·å–æ–¹å·®"""
        if self.n < 2:
            return 0
        return self.M2 / (self.n - 1)
    
    def get_std_dev(self):
        """è·å–æ ‡å‡†å·®"""
        return math.sqrt(self.get_variance())
```

### 2. åˆ†å¸ƒæ‹Ÿåˆä¼˜åŒ–

#### å¿«é€Ÿåˆ†å¸ƒæ‹Ÿåˆ

```python
class FastDistributionFitter:
    def __init__(self):
        self.fitting_methods = {
            'mle': self.maximum_likelihood_estimation,
            'mom': self.method_of_moments,
            'quantile': self.quantile_matching
        }
    
    def fit_distribution(self, data, distribution_type, method='mle'):
        """æ‹Ÿåˆåˆ†å¸ƒ"""
        fitter = self.fitting_methods[method]
        return fitter(data, distribution_type)
    
    def maximum_likelihood_estimation(self, data, distribution_type):
        """æœ€å¤§ä¼¼ç„¶ä¼°è®¡"""
        if distribution_type == 'normal':
            mu = np.mean(data)
            sigma = np.std(data, ddof=1)
            return {'mu': mu, 'sigma': sigma}
        
        elif distribution_type == 'exponential':
            lambda_param = 1 / np.mean(data)
            return {'lambda': lambda_param}
        
        elif distribution_type == 'lognormal':
            log_data = np.log(data)
            mu = np.mean(log_data)
            sigma = np.std(log_data, ddof=1)
            return {'mu': mu, 'sigma': sigma}
        
        elif distribution_type == 'weibull':
            # ä½¿ç”¨æ•°å€¼æ–¹æ³•ä¼°è®¡å¨å¸ƒå°”åˆ†å¸ƒå‚æ•°
            from scipy.optimize import minimize
            
            def neg_log_likelihood(params):
                k, lambda_param = params
                if k <= 0 or lambda_param <= 0:
                    return float('inf')
                return -np.sum(np.log(k/lambda_param) + (k-1)*np.log(data/lambda_param) - (data/lambda_param)**k)
            
            result = minimize(neg_log_likelihood, [1, 1], method='L-BFGS-B', bounds=[(0.1, 10), (0.1, 10)])
            return {'k': result.x[0], 'lambda': result.x[1]}
    
    def method_of_moments(self, data, distribution_type):
        """çŸ©ä¼°è®¡æ³•"""
        if distribution_type == 'normal':
            mu = np.mean(data)
            sigma = np.std(data, ddof=1)
            return {'mu': mu, 'sigma': sigma}
        
        elif distribution_type == 'exponential':
            lambda_param = 1 / np.mean(data)
            return {'lambda': lambda_param}
        
        elif distribution_type == 'gamma':
            mean = np.mean(data)
            variance = np.var(data, ddof=1)
            alpha = mean**2 / variance
            beta = variance / mean
            return {'alpha': alpha, 'beta': beta}
    
    def quantile_matching(self, data, distribution_type):
        """åˆ†ä½æ•°åŒ¹é…æ³•"""
        quantiles = [0.25, 0.5, 0.75]
        data_quantiles = np.quantile(data, quantiles)
        
        if distribution_type == 'normal':
            # ä½¿ç”¨ä¸­ä½æ•°å’Œå››åˆ†ä½è·ä¼°è®¡å‚æ•°
            mu = data_quantiles[1]  # ä¸­ä½æ•°
            sigma = (data_quantiles[2] - data_quantiles[0]) / 1.35  # å››åˆ†ä½è·
            return {'mu': mu, 'sigma': sigma}
        
        elif distribution_type == 'exponential':
            # ä½¿ç”¨ä¸­ä½æ•°ä¼°è®¡å‚æ•°
            lambda_param = np.log(2) / data_quantiles[1]
            return {'lambda': lambda_param}
```

## ğŸ§ª æµ‹è¯•ä¸éªŒè¯

### 1. å•å…ƒæµ‹è¯•

```python
import unittest

class TestProbabilityTheory(unittest.TestCase):
    def setUp(self):
        self.analyzer = ResponseTimeAnalyzer()
        self.anomaly_detector = StatisticalAnomalyDetector()
        self.queue_analyzer = QueueingTheoryAnalyzer()
    
    def test_response_time_analysis(self):
        """æµ‹è¯•å“åº”æ—¶é—´åˆ†æ"""
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        response_times = np.random.lognormal(2, 0.5, 1000)
        
        # æ‹Ÿåˆåˆ†å¸ƒ
        best_fit = self.analyzer.fit_response_time_distribution(response_times)
        
        self.assertIn('distribution', best_fit)
        self.assertIn('parameters', best_fit)
        self.assertIn('aic', best_fit)
    
    def test_anomaly_detection(self):
        """æµ‹è¯•å¼‚å¸¸æ£€æµ‹"""
        # ç”Ÿæˆæ­£å¸¸æ•°æ®
        normal_data = np.random.normal(100, 10, 1000)
        # æ·»åŠ å¼‚å¸¸æ•°æ®
        anomalous_data = np.concatenate([normal_data, [200, 300, 400]])
        
        anomalies = self.anomaly_detector.detect_anomalies(anomalous_data)
        
        self.assertGreater(len(anomalies['anomalies']), 0)
        self.assertLess(anomalies['anomaly_rate'], 0.1)
    
    def test_queue_analysis(self):
        """æµ‹è¯•é˜Ÿåˆ—åˆ†æ"""
        arrival_rate = 10
        service_rate = 15
        
        result = self.queue_analyzer.analyze_mm1_queue(arrival_rate, service_rate)
        
        self.assertIn('utilization', result)
        self.assertIn('avg_response_time', result)
        self.assertLess(result['utilization'], 1.0)
```

### 2. æ€§èƒ½æµ‹è¯•

```python
import time

def benchmark_statistical_calculations():
    """ç»Ÿè®¡è®¡ç®—æ€§èƒ½æµ‹è¯•"""
    calculator = FastStatisticsCalculator()
    data_sizes = [1000, 10000, 100000, 1000000]
    
    for size in data_sizes:
        test_data = np.random.normal(100, 10, size)
        
        start_time = time.time()
        calculator.update_running_statistics(test_data)
        stats = calculator.get_statistics()
        end_time = time.time()
        
        print(f"Size {size}: Mean {stats['mean']:.4f}, "
              f"Std {stats['std_dev']:.4f}, Time {end_time - start_time:.4f}s")

def benchmark_distribution_fitting():
    """åˆ†å¸ƒæ‹Ÿåˆæ€§èƒ½æµ‹è¯•"""
    fitter = FastDistributionFitter()
    distributions = ['normal', 'exponential', 'lognormal', 'weibull']
    data_sizes = [1000, 10000, 100000]
    
    for size in data_sizes:
        for dist in distributions:
            if dist == 'normal':
                test_data = np.random.normal(100, 10, size)
            elif dist == 'exponential':
                test_data = np.random.exponential(2, size)
            elif dist == 'lognormal':
                test_data = np.random.lognormal(2, 0.5, size)
            elif dist == 'weibull':
                test_data = np.random.weibull(2, size)
            
            start_time = time.time()
            params = fitter.fit_distribution(test_data, dist)
            end_time = time.time()
            
            print(f"Size {size}, Distribution {dist}: "
                  f"Params {params}, Time {end_time - start_time:.4f}s")
```

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **Ross, S. M.** (2019). *Introduction to Probability Models*. Academic Press.
2. **Casella, G., & Berger, R. L.** (2002). *Statistical Inference*. Duxbury Press.
3. **Wasserman, L.** (2004). *All of Statistics: A Concise Course in Statistical Inference*. Springer.
4. **Gross, D., & Harris, C. M.** (1998). *Fundamentals of Queueing Theory*. Wiley.
5. **OpenTelemetry Specification** (2023). *OpenTelemetry Protocol (OTLP)*.

## ğŸ”— ç›¸å…³èµ„æº

- [é›†åˆè®ºåœ¨å¯è§‚æµ‹æ€§ä¸­çš„åº”ç”¨](é›†åˆè®ºåº”ç”¨.md)
- [å›¾è®ºä¸åˆ†å¸ƒå¼è¿½è¸ª](å›¾è®ºåº”ç”¨.md)
- [ä¿¡æ¯è®ºåŸºç¡€](ä¿¡æ¯è®ºåŸºç¡€.md)
- [TLA+éªŒè¯OTLPåè®®](../å½¢å¼åŒ–éªŒè¯/TLA+éªŒè¯.md)

---

*æœ¬æ–‡æ¡£æ˜¯OpenTelemetry 2025å¹´çŸ¥è¯†ä½“ç³»ç†è®ºåŸºç¡€å±‚çš„ä¸€éƒ¨åˆ†*  
*æœ€åæ›´æ–°: 2025å¹´1æœˆ*  
*ç‰ˆæœ¬: 1.0.0*
