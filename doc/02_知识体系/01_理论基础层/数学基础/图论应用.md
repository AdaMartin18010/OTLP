# å›¾è®ºä¸åˆ†å¸ƒå¼è¿½è¸ª

## ğŸ“Š æ¦‚è¿°

å›¾è®ºä¸ºOpenTelemetryåˆ†å¸ƒå¼è¿½è¸ªç³»ç»Ÿæä¾›äº†å¼ºå¤§çš„æ•°å­¦å·¥å…·ï¼Œç”¨äºå»ºæ¨¡æœåŠ¡é—´è°ƒç”¨å…³ç³»ã€åˆ†æç³»ç»Ÿæ‹“æ‰‘ç»“æ„ã€ä¼˜åŒ–è¿½è¸ªè·¯å¾„å’Œæ£€æµ‹ç³»ç»Ÿå¼‚å¸¸ã€‚

## ğŸ”¢ æ ¸å¿ƒæ¦‚å¿µ

### 1. åŸºç¡€å›¾ç»“æ„

#### æœåŠ¡è°ƒç”¨å›¾

```mathematical
// æœåŠ¡è°ƒç”¨æœ‰å‘å›¾
G = (V, E)
V = {vâ‚, vâ‚‚, ..., vâ‚™}  // æœåŠ¡èŠ‚ç‚¹é›†åˆ
E = {(váµ¢, vâ±¼) | váµ¢ è°ƒç”¨ vâ±¼}  // è°ƒç”¨è¾¹é›†åˆ

// æƒé‡å‡½æ•°
w: E â†’ â„âº, w(e) = è°ƒç”¨é¢‘ç‡æˆ–å»¶è¿Ÿ
```

#### è¿½è¸ªæ ‘ç»“æ„

```mathematical
// è¿½è¸ªæ ‘
T = (V, E, root)
root âˆˆ V  // æ ¹èŠ‚ç‚¹ï¼ˆå…¥å£æœåŠ¡ï¼‰
âˆ€v âˆˆ V, âˆƒ! path from root to v  // æ ‘æ€§è´¨

// è·¨åº¦å…³ç³»
span_relation = {(parent, child) | child æ˜¯ parent çš„å­è·¨åº¦}
```

### 2. å›¾çš„æ€§è´¨

#### è¿é€šæ€§

```mathematical
// å¼ºè¿é€šåˆ†é‡
SCC(G) = {Câ‚, Câ‚‚, ..., Câ‚–}  // å¼ºè¿é€šåˆ†é‡é›†åˆ

// å¼±è¿é€šæ€§
weakly_connected(G) = âˆ€váµ¢, vâ±¼ âˆˆ V, âˆƒ path from váµ¢ to vâ±¼ in undirected(G)

// è¿é€šåº¦
connectivity(G) = min{|S| | S âŠ† V, G-S ä¸è¿é€š}
```

#### è·¯å¾„åˆ†æ

```mathematical
// æœ€çŸ­è·¯å¾„
shortest_path(váµ¢, vâ±¼) = argmin{Î£w(e) | e âˆˆ path from váµ¢ to vâ±¼}

// å…³é”®è·¯å¾„
critical_path = longest path from source to sink

// è·¯å¾„è¦†ç›–
path_cover = {Pâ‚, Pâ‚‚, ..., Pâ‚–} | âˆªPáµ¢ = V
```

### 3. å›¾ç®—æ³•

#### æ‹“æ‰‘æ’åº

```mathematical
// æ‹“æ‰‘æ’åºç®—æ³•
topological_sort(G):
    L = []  // ç»“æœåˆ—è¡¨
    S = {v | in_degree(v) = 0}  // å…¥åº¦ä¸º0çš„èŠ‚ç‚¹
    
    while S â‰  âˆ…:
        v = S.pop()
        L.append(v)
        for each edge (v, w):
            remove edge (v, w)
            if in_degree(w) = 0:
                S.add(w)
    
    return L
```

#### å¼ºè¿é€šåˆ†é‡

```mathematical
// Tarjanç®—æ³•
tarjan_scc(G):
    index = 0
    stack = []
    lowlink = {}
    index_map = {}
    sccs = []
    
    for v in V:
        if v not in index_map:
            strongconnect(v)
    
    return sccs
```

## ğŸ¯ åº”ç”¨åœºæ™¯

### 1. æœåŠ¡ä¾èµ–åˆ†æ

#### ä¾èµ–å›¾æ„å»º

```python
class ServiceDependencyGraph:
    def __init__(self):
        self.vertices = set()  # æœåŠ¡èŠ‚ç‚¹
        self.edges = {}  # è°ƒç”¨å…³ç³»
        self.weights = {}  # è°ƒç”¨æƒé‡
    
    def add_service(self, service_name):
        """æ·»åŠ æœåŠ¡èŠ‚ç‚¹"""
        self.vertices.add(service_name)
        self.edges[service_name] = set()
    
    def add_dependency(self, from_service, to_service, weight=1):
        """æ·»åŠ æœåŠ¡ä¾èµ–"""
        self.edges[from_service].add(to_service)
        self.weights[(from_service, to_service)] = weight
    
    def get_dependencies(self, service):
        """è·å–æœåŠ¡ä¾èµ–"""
        return self.edges.get(service, set())
    
    def get_dependents(self, service):
        """è·å–ä¾èµ–è¯¥æœåŠ¡çš„æœåŠ¡"""
        dependents = set()
        for from_svc, to_services in self.edges.items():
            if service in to_services:
                dependents.add(from_svc)
        return dependents
```

#### å¾ªç¯ä¾èµ–æ£€æµ‹

```python
def detect_cycles(graph):
    """æ£€æµ‹å¾ªç¯ä¾èµ–"""
    visited = set()
    rec_stack = set()
    cycles = []
    
    def dfs(node, path):
        if node in rec_stack:
            # å‘ç°å¾ªç¯
            cycle_start = path.index(node)
            cycle = path[cycle_start:] + [node]
            cycles.append(cycle)
            return
        
        if node in visited:
            return
        
        visited.add(node)
        rec_stack.add(node)
        path.append(node)
        
        for neighbor in graph.get_dependencies(node):
            dfs(neighbor, path.copy())
        
        rec_stack.remove(node)
    
    for service in graph.vertices:
        if service not in visited:
            dfs(service, [])
    
    return cycles
```

### 2. è¿½è¸ªè·¯å¾„ä¼˜åŒ–

#### è·¯å¾„å‹ç¼©

```python
class TracePathOptimizer:
    def __init__(self):
        self.path_cache = {}
        self.compression_ratio = 0.8
    
    def compress_path(self, trace_path):
        """è·¯å¾„å‹ç¼©ç®—æ³•"""
        if len(trace_path) <= 2:
            return trace_path
        
        # è¯†åˆ«é‡å¤æ¨¡å¼
        patterns = self.find_patterns(trace_path)
        
        # åº”ç”¨å‹ç¼©
        compressed = self.apply_compression(trace_path, patterns)
        
        return compressed
    
    def find_patterns(self, path):
        """æŸ¥æ‰¾é‡å¤æ¨¡å¼"""
        patterns = {}
        n = len(path)
        
        for length in range(2, n//2 + 1):
            for i in range(n - length + 1):
                pattern = tuple(path[i:i+length])
                if pattern in patterns:
                    patterns[pattern] += 1
                else:
                    patterns[pattern] = 1
        
        # è¿‡æ»¤ä½é¢‘æ¨¡å¼
        return {p: count for p, count in patterns.items() 
                if count >= 2 and len(p) * count > 3}
    
    def apply_compression(self, path, patterns):
        """åº”ç”¨å‹ç¼©"""
        compressed = list(path)
        
        for pattern, count in sorted(patterns.items(), 
                                   key=lambda x: len(x[0]) * x[1], 
                                   reverse=True):
            pattern_list = list(pattern)
            pattern_str = f"<{len(pattern_list)}*{count}>"
            
            # æ›¿æ¢é‡å¤æ¨¡å¼
            i = 0
            while i < len(compressed) - len(pattern_list) + 1:
                if compressed[i:i+len(pattern_list)] == pattern_list:
                    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„é‡å¤
                    repeat_count = 1
                    j = i + len(pattern_list)
                    while (j + len(pattern_list) <= len(compressed) and 
                           compressed[j:j+len(pattern_list)] == pattern_list):
                        repeat_count += 1
                        j += len(pattern_list)
                    
                    if repeat_count >= 2:
                        # æ‰§è¡Œæ›¿æ¢
                        compressed[i:j] = [pattern_str]
                        i += 1
                    else:
                        i += 1
                else:
                    i += 1
        
        return compressed
```

#### å…³é”®è·¯å¾„åˆ†æ

```python
class CriticalPathAnalyzer:
    def __init__(self):
        self.earliest_start = {}
        self.latest_start = {}
        self.critical_path = []
    
    def analyze_critical_path(self, trace_graph):
        """åˆ†æå…³é”®è·¯å¾„"""
        # è®¡ç®—æœ€æ—©å¼€å§‹æ—¶é—´
        self.calculate_earliest_times(trace_graph)
        
        # è®¡ç®—æœ€æ™šå¼€å§‹æ—¶é—´
        self.calculate_latest_times(trace_graph)
        
        # è¯†åˆ«å…³é”®è·¯å¾„
        self.identify_critical_path(trace_graph)
        
        return self.critical_path
    
    def calculate_earliest_times(self, graph):
        """è®¡ç®—æœ€æ—©å¼€å§‹æ—¶é—´"""
        # æ‹“æ‰‘æ’åº
        topo_order = self.topological_sort(graph)
        
        for node in topo_order:
            if not graph.get_dependencies(node):
                self.earliest_start[node] = 0
            else:
                max_earliest = 0
                for dep in graph.get_dependencies(node):
                    dep_earliest = self.earliest_start[dep]
                    edge_weight = graph.get_weight(dep, node)
                    max_earliest = max(max_earliest, dep_earliest + edge_weight)
                self.earliest_start[node] = max_earliest
    
    def calculate_latest_times(self, graph):
        """è®¡ç®—æœ€æ™šå¼€å§‹æ—¶é—´"""
        # åå‘æ‹“æ‰‘æ’åº
        reverse_topo = list(reversed(self.topological_sort(graph)))
        
        # æ‰¾åˆ°ç»“æŸèŠ‚ç‚¹
        end_nodes = [node for node in graph.vertices 
                    if not graph.get_dependents(node)]
        
        if end_nodes:
            max_earliest = max(self.earliest_start[node] for node in end_nodes)
            for node in end_nodes:
                self.latest_start[node] = max_earliest
        
        for node in reverse_topo:
            if node not in self.latest_start:
                min_latest = float('inf')
                for dependent in graph.get_dependents(node):
                    if dependent in self.latest_start:
                        edge_weight = graph.get_weight(node, dependent)
                        min_latest = min(min_latest, 
                                       self.latest_start[dependent] - edge_weight)
                self.latest_start[node] = min_latest
    
    def identify_critical_path(self, graph):
        """è¯†åˆ«å…³é”®è·¯å¾„"""
        critical_nodes = []
        
        for node in graph.vertices:
            if (self.earliest_start[node] == self.latest_start[node]):
                critical_nodes.append(node)
        
        # æ„å»ºå…³é”®è·¯å¾„
        self.critical_path = self.build_critical_path(graph, critical_nodes)
    
    def build_critical_path(self, graph, critical_nodes):
        """æ„å»ºå…³é”®è·¯å¾„"""
        if not critical_nodes:
            return []
        
        # æ‰¾åˆ°èµ·å§‹èŠ‚ç‚¹
        start_node = None
        for node in critical_nodes:
            if not any(dep in critical_nodes 
                      for dep in graph.get_dependencies(node)):
                start_node = node
                break
        
        if not start_node:
            return []
        
        # æ„å»ºè·¯å¾„
        path = [start_node]
        current = start_node
        
        while True:
            next_node = None
            for dependent in graph.get_dependents(current):
                if dependent in critical_nodes:
                    next_node = dependent
                    break
            
            if next_node:
                path.append(next_node)
                current = next_node
            else:
                break
        
        return path
```

### 3. å¼‚å¸¸æ£€æµ‹

#### å›¾å¼‚å¸¸æ£€æµ‹

```python
class GraphAnomalyDetector:
    def __init__(self):
        self.normal_patterns = {}
        self.anomaly_threshold = 0.1
    
    def detect_anomalies(self, current_graph, historical_graphs):
        """æ£€æµ‹å›¾å¼‚å¸¸"""
        anomalies = []
        
        # ç»“æ„å¼‚å¸¸æ£€æµ‹
        structure_anomalies = self.detect_structure_anomalies(
            current_graph, historical_graphs)
        anomalies.extend(structure_anomalies)
        
        # æƒé‡å¼‚å¸¸æ£€æµ‹
        weight_anomalies = self.detect_weight_anomalies(
            current_graph, historical_graphs)
        anomalies.extend(weight_anomalies)
        
        # è·¯å¾„å¼‚å¸¸æ£€æµ‹
        path_anomalies = self.detect_path_anomalies(
            current_graph, historical_graphs)
        anomalies.extend(path_anomalies)
        
        return anomalies
    
    def detect_structure_anomalies(self, current, historical):
        """æ£€æµ‹ç»“æ„å¼‚å¸¸"""
        anomalies = []
        
        # è®¡ç®—å†å²å¹³å‡åº¦åˆ†å¸ƒ
        avg_degree_dist = self.calculate_average_degree_distribution(historical)
        current_degree_dist = self.calculate_degree_distribution(current)
        
        # æ¯”è¾ƒåº¦åˆ†å¸ƒ
        for degree, count in current_degree_dist.items():
            expected_count = avg_degree_dist.get(degree, 0)
            if expected_count > 0:
                deviation = abs(count - expected_count) / expected_count
                if deviation > self.anomaly_threshold:
                    anomalies.append({
                        'type': 'structure_anomaly',
                        'degree': degree,
                        'expected': expected_count,
                        'actual': count,
                        'deviation': deviation
                    })
        
        return anomalies
    
    def detect_weight_anomalies(self, current, historical):
        """æ£€æµ‹æƒé‡å¼‚å¸¸"""
        anomalies = []
        
        # è®¡ç®—å†å²å¹³å‡æƒé‡
        avg_weights = self.calculate_average_weights(historical)
        
        for edge, weight in current.get_all_weights().items():
            if edge in avg_weights:
                expected_weight = avg_weights[edge]
                deviation = abs(weight - expected_weight) / expected_weight
                if deviation > self.anomaly_threshold:
                    anomalies.append({
                        'type': 'weight_anomaly',
                        'edge': edge,
                        'expected': expected_weight,
                        'actual': weight,
                        'deviation': deviation
                    })
        
        return anomalies
    
    def detect_path_anomalies(self, current, historical):
        """æ£€æµ‹è·¯å¾„å¼‚å¸¸"""
        anomalies = []
        
        # è®¡ç®—å†å²å¸¸è§è·¯å¾„
        common_paths = self.calculate_common_paths(historical)
        current_paths = self.extract_paths(current)
        
        for path in current_paths:
            if path not in common_paths:
                # æ–°è·¯å¾„ï¼Œå¯èƒ½æ˜¯å¼‚å¸¸
                anomalies.append({
                    'type': 'path_anomaly',
                    'path': path,
                    'reason': 'new_path'
                })
        
        return anomalies
```

## ğŸ”§ æ€§èƒ½ä¼˜åŒ–

### 1. å›¾å­˜å‚¨ä¼˜åŒ–

#### é‚»æ¥è¡¨ä¼˜åŒ–

```python
class OptimizedGraph:
    def __init__(self):
        self.adjacency_list = {}
        self.reverse_adjacency = {}
        self.edge_weights = {}
        self.vertex_cache = {}
    
    def add_edge(self, from_vertex, to_vertex, weight=1):
        """æ·»åŠ è¾¹"""
        if from_vertex not in self.adjacency_list:
            self.adjacency_list[from_vertex] = []
        if to_vertex not in self.reverse_adjacency:
            self.reverse_adjacency[to_vertex] = []
        
        self.adjacency_list[from_vertex].append(to_vertex)
        self.reverse_adjacency[to_vertex].append(from_vertex)
        self.edge_weights[(from_vertex, to_vertex)] = weight
        
        # æ¸…é™¤ç¼“å­˜
        self.vertex_cache.clear()
    
    def get_neighbors(self, vertex):
        """è·å–é‚»å±…èŠ‚ç‚¹"""
        return self.adjacency_list.get(vertex, [])
    
    def get_predecessors(self, vertex):
        """è·å–å‰é©±èŠ‚ç‚¹"""
        return self.reverse_adjacency.get(vertex, [])
```

#### å‹ç¼©å­˜å‚¨

```python
class CompressedGraph:
    def __init__(self):
        self.vertex_map = {}  # é¡¶ç‚¹åç§°åˆ°ç´¢å¼•çš„æ˜ å°„
        self.index_map = {}   # ç´¢å¼•åˆ°é¡¶ç‚¹åç§°çš„æ˜ å°„
        self.adjacency_matrix = None
        self.next_index = 0
    
    def add_vertex(self, vertex):
        """æ·»åŠ é¡¶ç‚¹"""
        if vertex not in self.vertex_map:
            self.vertex_map[vertex] = self.next_index
            self.index_map[self.next_index] = vertex
            self.next_index += 1
    
    def add_edge(self, from_vertex, to_vertex, weight=1):
        """æ·»åŠ è¾¹"""
        self.add_vertex(from_vertex)
        self.add_vertex(to_vertex)
        
        from_idx = self.vertex_map[from_vertex]
        to_idx = self.vertex_map[to_vertex]
        
        if self.adjacency_matrix is None:
            size = len(self.vertex_map)
            self.adjacency_matrix = [[0] * size for _ in range(size)]
        
        self.adjacency_matrix[from_idx][to_idx] = weight
    
    def get_weight(self, from_vertex, to_vertex):
        """è·å–è¾¹æƒé‡"""
        if from_vertex not in self.vertex_map or to_vertex not in self.vertex_map:
            return 0
        
        from_idx = self.vertex_map[from_vertex]
        to_idx = self.vertex_map[to_vertex]
        
        return self.adjacency_matrix[from_idx][to_idx]
```

### 2. ç®—æ³•ä¼˜åŒ–

#### å¹¶è¡Œå›¾ç®—æ³•

```python
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor

class ParallelGraphAlgorithms:
    def __init__(self, num_workers=None):
        self.num_workers = num_workers or mp.cpu_count()
    
    def parallel_bfs(self, graph, start_vertex):
        """å¹¶è¡ŒBFS"""
        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
            # å°†å›¾åˆ†å‰²ä¸ºå­å›¾
            subgraphs = self.partition_graph(graph)
            
            # å¹¶è¡Œæ‰§è¡ŒBFS
            futures = []
            for subgraph in subgraphs:
                future = executor.submit(self.bfs_subgraph, subgraph, start_vertex)
                futures.append(future)
            
            # åˆå¹¶ç»“æœ
            results = []
            for future in futures:
                results.extend(future.result())
            
            return results
    
    def parallel_shortest_path(self, graph, source, targets):
        """å¹¶è¡Œæœ€çŸ­è·¯å¾„"""
        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
            futures = []
            for target in targets:
                future = executor.submit(self.dijkstra, graph, source, target)
                futures.append(future)
            
            results = {}
            for i, future in enumerate(futures):
                results[targets[i]] = future.result()
            
            return results
```

## ğŸ§ª æµ‹è¯•ä¸éªŒè¯

### 1. å•å…ƒæµ‹è¯•

```python
import unittest

class TestServiceDependencyGraph(unittest.TestCase):
    def setUp(self):
        self.graph = ServiceDependencyGraph()
        self.graph.add_service('A')
        self.graph.add_service('B')
        self.graph.add_service('C')
        self.graph.add_dependency('A', 'B')
        self.graph.add_dependency('B', 'C')
    
    def test_add_dependency(self):
        self.assertIn('B', self.graph.get_dependencies('A'))
        self.assertIn('A', self.graph.get_dependents('B'))
    
    def test_cycle_detection(self):
        # æ·»åŠ å¾ªç¯ä¾èµ–
        self.graph.add_dependency('C', 'A')
        cycles = detect_cycles(self.graph)
        self.assertTrue(len(cycles) > 0)
    
    def test_topological_sort(self):
        sorted_services = topological_sort(self.graph)
        self.assertEqual(sorted_services, ['A', 'B', 'C'])
```

### 2. æ€§èƒ½æµ‹è¯•

```python
import time
import random

def benchmark_graph_operations():
    """å›¾æ“ä½œæ€§èƒ½æµ‹è¯•"""
    sizes = [100, 1000, 10000]
    
    for size in sizes:
        graph = ServiceDependencyGraph()
        
        # æ·»åŠ èŠ‚ç‚¹
        for i in range(size):
            graph.add_service(f'service_{i}')
        
        # æ·»åŠ è¾¹
        start_time = time.time()
        for i in range(size * 2):
            from_svc = f'service_{random.randint(0, size-1)}'
            to_svc = f'service_{random.randint(0, size-1)}'
            if from_svc != to_svc:
                graph.add_dependency(from_svc, to_svc)
        
        add_time = time.time() - start_time
        
        # æµ‹è¯•æŸ¥è¯¢
        start_time = time.time()
        for i in range(1000):
            service = f'service_{random.randint(0, size-1)}'
            deps = graph.get_dependencies(service)
        
        query_time = time.time() - start_time
        
        print(f"Size {size}: Add {add_time:.4f}s, Query {query_time:.4f}s")
```

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **Cormen, T. H., et al.** (2009). *Introduction to Algorithms*. MIT Press.
2. **Bondy, J. A., & Murty, U. S. R.** (2008). *Graph Theory*. Springer.
3. **West, D. B.** (2001). *Introduction to Graph Theory*. Prentice Hall.
4. **OpenTelemetry Specification** (2023). *Distributed Tracing*.
5. **Jaeger Documentation** (2023). *Distributed Tracing Architecture*.

## ğŸ”— ç›¸å…³èµ„æº

- [é›†åˆè®ºåœ¨å¯è§‚æµ‹æ€§ä¸­çš„åº”ç”¨](é›†åˆè®ºåº”ç”¨.md)
- [ä¿¡æ¯è®ºåŸºç¡€](ä¿¡æ¯è®ºåŸºç¡€.md)
- [æ¦‚ç‡è®ºä¸ç»Ÿè®¡åˆ†æ](æ¦‚ç‡è®ºåº”ç”¨.md)
- [TLA+éªŒè¯OTLPåè®®](../å½¢å¼åŒ–éªŒè¯/TLA+éªŒè¯.md)

---

*æœ¬æ–‡æ¡£æ˜¯OpenTelemetry 2025å¹´çŸ¥è¯†ä½“ç³»ç†è®ºåŸºç¡€å±‚çš„ä¸€éƒ¨åˆ†*  
*æœ€åæ›´æ–°: 2025å¹´1æœˆ*  
*ç‰ˆæœ¬: 1.0.0*
