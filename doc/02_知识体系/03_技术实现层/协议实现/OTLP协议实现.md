# OTLPåè®®å®ç°

## ğŸ“Š æ¦‚è¿°

OTLPï¼ˆOpenTelemetry Protocolï¼‰æ˜¯OpenTelemetryçš„æ ¸å¿ƒåè®®ï¼Œç”¨äºåœ¨å¯è§‚æµ‹æ€§æ•°æ®æ”¶é›†å™¨ã€SDKå’Œåç«¯ç³»ç»Ÿä¹‹é—´ä¼ è¾“é¥æµ‹æ•°æ®ã€‚æœ¬æ–‡æ¡£è¯¦ç»†æè¿°äº†OTLPåè®®çš„å®ç°ç»†èŠ‚ã€æ€§èƒ½ä¼˜åŒ–å’Œæœ€ä½³å®è·µã€‚

## ğŸ”¢ æ ¸å¿ƒæ¦‚å¿µ

### 1. åè®®è§„èŒƒ

#### åè®®ç‰ˆæœ¬å’Œç‰¹æ€§

```yaml
OTLPåè®®è§„èŒƒ:
  åè®®ç‰ˆæœ¬: "1.0.0"
  æ”¯æŒçš„æ•°æ®ç±»å‹:
    - è¿½è¸ªæ•°æ® (Traces)
    - æŒ‡æ ‡æ•°æ® (Metrics)
    - æ—¥å¿—æ•°æ® (Logs)
  
  ä¼ è¾“åè®®:
    gRPC:
      - é»˜è®¤ä¼ è¾“åè®®
      - æ”¯æŒæµå¼ä¼ è¾“
      - æ”¯æŒå‹ç¼©
      - æ”¯æŒè®¤è¯å’Œæˆæƒ
    
    HTTP:
      - HTTP/1.1æ”¯æŒ
      - HTTP/2æ”¯æŒ
      - HTTP/3æ”¯æŒ
      - æ”¯æŒJSONå’ŒProtobufæ ¼å¼
    
    WebSocket:
      - å®æ—¶åŒå‘é€šä¿¡
      - æ”¯æŒæµè§ˆå™¨ç¯å¢ƒ
      - æ”¯æŒè‡ªåŠ¨é‡è¿
  
  æ•°æ®æ ¼å¼:
    Protocol Buffers:
      - é»˜è®¤åºåˆ—åŒ–æ ¼å¼
      - é«˜æ•ˆäºŒè¿›åˆ¶æ ¼å¼
      - å‘åå…¼å®¹
      - è·¨è¯­è¨€æ”¯æŒ
    
    JSON:
      - äººç±»å¯è¯»æ ¼å¼
      - è°ƒè¯•å‹å¥½
      - æ”¯æŒHTTPä¼ è¾“
      - æ˜“äºé›†æˆ
  
  å‹ç¼©ç®—æ³•:
    gzip:
      - é€šç”¨å‹ç¼©ç®—æ³•
      - å¹³è¡¡å‹ç¼©ç‡å’Œé€Ÿåº¦
      - å¹¿æ³›æ”¯æŒ
    
    deflate:
      - å¿«é€Ÿå‹ç¼©ç®—æ³•
      - ä½CPUä½¿ç”¨ç‡
      - é€‚åˆå®æ—¶ä¼ è¾“
    
    zstd:
      - é«˜æ•ˆå‹ç¼©ç®—æ³•
      - é«˜å‹ç¼©ç‡
      - å¿«é€Ÿè§£å‹
```

#### æ•°æ®æ¨¡å‹

```yaml
æ•°æ®æ¨¡å‹:
  è¿½è¸ªæ•°æ®æ¨¡å‹:
    èµ„æº (Resource):
      - æœåŠ¡åç§°
      - æœåŠ¡ç‰ˆæœ¬
      - éƒ¨ç½²ç¯å¢ƒ
      - ä¸»æœºä¿¡æ¯
      - è‡ªå®šä¹‰å±æ€§
    
    è¿½è¸ª (Trace):
      - è¿½è¸ªID
      - è·¨åº¦åˆ—è¡¨
      - çŠ¶æ€ä¿¡æ¯
      - å±æ€§ä¿¡æ¯
    
    è·¨åº¦ (Span):
      - è·¨åº¦ID
      - çˆ¶è·¨åº¦ID
      - æ“ä½œåç§°
      - å¼€å§‹æ—¶é—´
      - ç»“æŸæ—¶é—´
      - çŠ¶æ€ä¿¡æ¯
      - å±æ€§ä¿¡æ¯
      - äº‹ä»¶åˆ—è¡¨
      - é“¾æ¥åˆ—è¡¨
  
  æŒ‡æ ‡æ•°æ®æ¨¡å‹:
    èµ„æºæŒ‡æ ‡ (Resource Metrics):
      - èµ„æºä¿¡æ¯
      - ä½œç”¨åŸŸæŒ‡æ ‡åˆ—è¡¨
    
    ä½œç”¨åŸŸæŒ‡æ ‡ (Scope Metrics):
      - ä½œç”¨åŸŸä¿¡æ¯
      - æŒ‡æ ‡åˆ—è¡¨
    
    æŒ‡æ ‡ (Metric):
      - æŒ‡æ ‡åç§°
      - æŒ‡æ ‡æè¿°
      - æŒ‡æ ‡å•ä½
      - æ•°æ®ç±»å‹
      - æ•°æ®ç‚¹åˆ—è¡¨
  
  æ—¥å¿—æ•°æ®æ¨¡å‹:
    èµ„æºæ—¥å¿— (Resource Logs):
      - èµ„æºä¿¡æ¯
      - ä½œç”¨åŸŸæ—¥å¿—åˆ—è¡¨
    
    ä½œç”¨åŸŸæ—¥å¿— (Scope Logs):
      - ä½œç”¨åŸŸä¿¡æ¯
      - æ—¥å¿—è®°å½•åˆ—è¡¨
    
    æ—¥å¿—è®°å½• (Log Record):
      - æ—¶é—´æˆ³
      - ä¸¥é‡ç¨‹åº¦
      - æ¶ˆæ¯å†…å®¹
      - å±æ€§ä¿¡æ¯
      - èµ„æºä¿¡æ¯
```

### 2. åè®®å®ç°

#### gRPCä¼ è¾“å®ç°

```python
class OTLPGRPCTransport:
    def __init__(self, endpoint, credentials=None, compression=None):
        self.endpoint = endpoint
        self.credentials = credentials
        self.compression = compression
        self.channel = None
        self.stub = None
        self._setup_grpc_connection()
    
    def _setup_grpc_connection(self):
        """è®¾ç½®gRPCè¿æ¥"""
        # é…ç½®gRPCé€‰é¡¹
        options = [
            ('grpc.max_send_message_length', 4 * 1024 * 1024),  # 4MB
            ('grpc.max_receive_message_length', 4 * 1024 * 1024),  # 4MB
            ('grpc.keepalive_time_ms', 30000),  # 30ç§’
            ('grpc.keepalive_timeout_ms', 5000),  # 5ç§’
            ('grpc.keepalive_permit_without_calls', True),
            ('grpc.http2.max_pings_without_data', 0),
            ('grpc.http2.min_time_between_pings_ms', 10000),
            ('grpc.http2.min_ping_interval_without_data_ms', 300000)
        ]
        
        # åˆ›å»ºgRPCé€šé“
        if self.credentials:
            self.channel = grpc.secure_channel(
                self.endpoint, 
                self.credentials, 
                options=options
            )
        else:
            self.channel = grpc.insecure_channel(
                self.endpoint, 
                options=options
            )
        
        # åˆ›å»ºgRPCå­˜æ ¹
        self.stub = trace_service_pb2_grpc.TraceServiceStub(self.channel)
    
    def export_traces(self, traces):
        """å¯¼å‡ºè¿½è¸ªæ•°æ®"""
        try:
            # æ„å»ºå¯¼å‡ºè¯·æ±‚
            request = trace_service_pb2.ExportTraceServiceRequest(
                resource_spans=traces
            )
            
            # å‘é€è¯·æ±‚
            response = self.stub.Export(request)
            
            return {
                'success': True,
                'response': response,
                'error': None
            }
        except grpc.RpcError as e:
            return {
                'success': False,
                'response': None,
                'error': str(e)
            }
    
    def export_metrics(self, metrics):
        """å¯¼å‡ºæŒ‡æ ‡æ•°æ®"""
        try:
            # æ„å»ºå¯¼å‡ºè¯·æ±‚
            request = metrics_service_pb2.ExportMetricsServiceRequest(
                resource_metrics=metrics
            )
            
            # å‘é€è¯·æ±‚
            response = self.stub.Export(request)
            
            return {
                'success': True,
                'response': response,
                'error': None
            }
        except grpc.RpcError as e:
            return {
                'success': False,
                'response': None,
                'error': str(e)
            }
    
    def export_logs(self, logs):
        """å¯¼å‡ºæ—¥å¿—æ•°æ®"""
        try:
            # æ„å»ºå¯¼å‡ºè¯·æ±‚
            request = logs_service_pb2.ExportLogsServiceRequest(
                resource_logs=logs
            )
            
            # å‘é€è¯·æ±‚
            response = self.stub.Export(request)
            
            return {
                'success': True,
                'response': response,
                'error': None
            }
        except grpc.RpcError as e:
            return {
                'success': False,
                'response': None,
                'error': str(e)
            }
    
    def close(self):
        """å…³é—­è¿æ¥"""
        if self.channel:
            self.channel.close()
```

#### HTTPä¼ è¾“å®ç°

```python
class OTLPHTTPTransport:
    def __init__(self, endpoint, headers=None, timeout=30, compression=None):
        self.endpoint = endpoint
        self.headers = headers or {}
        self.timeout = timeout
        self.compression = compression
        self.session = requests.Session()
        self._setup_http_session()
    
    def _setup_http_session(self):
        """è®¾ç½®HTTPä¼šè¯"""
        # è®¾ç½®é»˜è®¤å¤´éƒ¨
        default_headers = {
            'Content-Type': 'application/x-protobuf',
            'User-Agent': 'OpenTelemetry-Python/1.0.0'
        }
        
        if self.compression:
            default_headers['Content-Encoding'] = self.compression
        
        self.headers.update(default_headers)
        self.session.headers.update(self.headers)
        
        # é…ç½®è¿æ¥æ± 
        adapter = HTTPAdapter(
            pool_connections=10,
            pool_maxsize=20,
            max_retries=3,
            pool_block=False
        )
        
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)
    
    def export_traces(self, traces):
        """å¯¼å‡ºè¿½è¸ªæ•°æ®"""
        try:
            # åºåˆ—åŒ–æ•°æ®
            serialized_data = self._serialize_traces(traces)
            
            # å‹ç¼©æ•°æ®
            if self.compression:
                serialized_data = self._compress_data(serialized_data)
            
            # å‘é€è¯·æ±‚
            response = self.session.post(
                f"{self.endpoint}/v1/traces",
                data=serialized_data,
                timeout=self.timeout
            )
            
            response.raise_for_status()
            
            return {
                'success': True,
                'response': response,
                'error': None
            }
        except requests.RequestException as e:
            return {
                'success': False,
                'response': None,
                'error': str(e)
            }
    
    def export_metrics(self, metrics):
        """å¯¼å‡ºæŒ‡æ ‡æ•°æ®"""
        try:
            # åºåˆ—åŒ–æ•°æ®
            serialized_data = self._serialize_metrics(metrics)
            
            # å‹ç¼©æ•°æ®
            if self.compression:
                serialized_data = self._compress_data(serialized_data)
            
            # å‘é€è¯·æ±‚
            response = self.session.post(
                f"{self.endpoint}/v1/metrics",
                data=serialized_data,
                timeout=self.timeout
            )
            
            response.raise_for_status()
            
            return {
                'success': True,
                'response': response,
                'error': None
            }
        except requests.RequestException as e:
            return {
                'success': False,
                'response': None,
                'error': str(e)
            }
    
    def export_logs(self, logs):
        """å¯¼å‡ºæ—¥å¿—æ•°æ®"""
        try:
            # åºåˆ—åŒ–æ•°æ®
            serialized_data = self._serialize_logs(logs)
            
            # å‹ç¼©æ•°æ®
            if self.compression:
                serialized_data = self._compress_data(serialized_data)
            
            # å‘é€è¯·æ±‚
            response = self.session.post(
                f"{self.endpoint}/v1/logs",
                data=serialized_data,
                timeout=self.timeout
            )
            
            response.raise_for_status()
            
            return {
                'success': True,
                'response': response,
                'error': None
            }
        except requests.RequestException as e:
            return {
                'success': False,
                'response': None,
                'error': str(e)
            }
    
    def _serialize_traces(self, traces):
        """åºåˆ—åŒ–è¿½è¸ªæ•°æ®"""
        # ä½¿ç”¨Protocol Buffersåºåˆ—åŒ–
        request = trace_service_pb2.ExportTraceServiceRequest(
            resource_spans=traces
        )
        return request.SerializeToString()
    
    def _serialize_metrics(self, metrics):
        """åºåˆ—åŒ–æŒ‡æ ‡æ•°æ®"""
        # ä½¿ç”¨Protocol Buffersåºåˆ—åŒ–
        request = metrics_service_pb2.ExportMetricsServiceRequest(
            resource_metrics=metrics
        )
        return request.SerializeToString()
    
    def _serialize_logs(self, logs):
        """åºåˆ—åŒ–æ—¥å¿—æ•°æ®"""
        # ä½¿ç”¨Protocol Buffersåºåˆ—åŒ–
        request = logs_service_pb2.ExportLogsServiceRequest(
            resource_logs=logs
        )
        return request.SerializeToString()
    
    def _compress_data(self, data):
        """å‹ç¼©æ•°æ®"""
        if self.compression == 'gzip':
            return gzip.compress(data)
        elif self.compression == 'deflate':
            return zlib.compress(data)
        elif self.compression == 'zstd':
            return zstd.compress(data)
        else:
            return data
```

### 3. æ•°æ®åºåˆ—åŒ–

#### Protocol Buffersåºåˆ—åŒ–

```python
class OTLPProtobufSerializer:
    def __init__(self):
        self.serializers = {
            'traces': self._serialize_traces,
            'metrics': self._serialize_metrics,
            'logs': self._serialize_logs
        }
    
    def serialize(self, data_type, data):
        """åºåˆ—åŒ–æ•°æ®"""
        serializer = self.serializers.get(data_type)
        if not serializer:
            raise ValueError(f"Unsupported data type: {data_type}")
        
        return serializer(data)
    
    def _serialize_traces(self, traces):
        """åºåˆ—åŒ–è¿½è¸ªæ•°æ®"""
        resource_spans = []
        
        for trace in traces:
            # æ„å»ºèµ„æºè·¨åº¦
            resource_span = resource_pb2.ResourceSpans(
                resource=self._build_resource(trace.get('resource', {})),
                scope_spans=self._build_scope_spans(trace.get('spans', []))
            )
            resource_spans.append(resource_span)
        
        # æ„å»ºå¯¼å‡ºè¯·æ±‚
        request = trace_service_pb2.ExportTraceServiceRequest(
            resource_spans=resource_spans
        )
        
        return request.SerializeToString()
    
    def _serialize_metrics(self, metrics):
        """åºåˆ—åŒ–æŒ‡æ ‡æ•°æ®"""
        resource_metrics = []
        
        for metric in metrics:
            # æ„å»ºèµ„æºæŒ‡æ ‡
            resource_metric = metrics_pb2.ResourceMetrics(
                resource=self._build_resource(metric.get('resource', {})),
                scope_metrics=self._build_scope_metrics(metric.get('metrics', []))
            )
            resource_metrics.append(resource_metric)
        
        # æ„å»ºå¯¼å‡ºè¯·æ±‚
        request = metrics_service_pb2.ExportMetricsServiceRequest(
            resource_metrics=resource_metrics
        )
        
        return request.SerializeToString()
    
    def _serialize_logs(self, logs):
        """åºåˆ—åŒ–æ—¥å¿—æ•°æ®"""
        resource_logs = []
        
        for log in logs:
            # æ„å»ºèµ„æºæ—¥å¿—
            resource_log = logs_pb2.ResourceLogs(
                resource=self._build_resource(log.get('resource', {})),
                scope_logs=self._build_scope_logs(log.get('logs', []))
            )
            resource_logs.append(resource_log)
        
        # æ„å»ºå¯¼å‡ºè¯·æ±‚
        request = logs_service_pb2.ExportLogsServiceRequest(
            resource_logs=resource_logs
        )
        
        return request.SerializeToString()
    
    def _build_resource(self, resource_data):
        """æ„å»ºèµ„æºå¯¹è±¡"""
        attributes = []
        
        for key, value in resource_data.get('attributes', {}).items():
            attribute = common_pb2.KeyValue(
                key=key,
                value=self._build_any_value(value)
            )
            attributes.append(attribute)
        
        return resource_pb2.Resource(attributes=attributes)
    
    def _build_any_value(self, value):
        """æ„å»ºAnyValueå¯¹è±¡"""
        if isinstance(value, str):
            return common_pb2.AnyValue(string_value=value)
        elif isinstance(value, bool):
            return common_pb2.AnyValue(bool_value=value)
        elif isinstance(value, int):
            return common_pb2.AnyValue(int_value=value)
        elif isinstance(value, float):
            return common_pb2.AnyValue(double_value=value)
        elif isinstance(value, list):
            array_value = common_pb2.ArrayValue(
                values=[self._build_any_value(item) for item in value]
            )
            return common_pb2.AnyValue(array_value=array_value)
        elif isinstance(value, dict):
            kvlist_value = common_pb2.KeyValueList(
                values=[
                    common_pb2.KeyValue(key=k, value=self._build_any_value(v))
                    for k, v in value.items()
                ]
            )
            return common_pb2.AnyValue(kvlist_value=kvlist_value)
        else:
            return common_pb2.AnyValue(string_value=str(value))
```

#### JSONåºåˆ—åŒ–

```python
class OTLPJSONSerializer:
    def __init__(self):
        self.serializers = {
            'traces': self._serialize_traces,
            'metrics': self._serialize_metrics,
            'logs': self._serialize_logs
        }
    
    def serialize(self, data_type, data):
        """åºåˆ—åŒ–æ•°æ®"""
        serializer = self.serializers.get(data_type)
        if not serializer:
            raise ValueError(f"Unsupported data type: {data_type}")
        
        return json.dumps(serializer(data), default=self._json_serializer)
    
    def _serialize_traces(self, traces):
        """åºåˆ—åŒ–è¿½è¸ªæ•°æ®"""
        resource_spans = []
        
        for trace in traces:
            resource_span = {
                'resource': self._build_resource(trace.get('resource', {})),
                'scopeSpans': self._build_scope_spans(trace.get('spans', []))
            }
            resource_spans.append(resource_span)
        
        return {
            'resourceSpans': resource_spans
        }
    
    def _serialize_metrics(self, metrics):
        """åºåˆ—åŒ–æŒ‡æ ‡æ•°æ®"""
        resource_metrics = []
        
        for metric in metrics:
            resource_metric = {
                'resource': self._build_resource(metric.get('resource', {})),
                'scopeMetrics': self._build_scope_metrics(metric.get('metrics', []))
            }
            resource_metrics.append(resource_metric)
        
        return {
            'resourceMetrics': resource_metrics
        }
    
    def _serialize_logs(self, logs):
        """åºåˆ—åŒ–æ—¥å¿—æ•°æ®"""
        resource_logs = []
        
        for log in logs:
            resource_log = {
                'resource': self._build_resource(log.get('resource', {})),
                'scopeLogs': self._build_scope_logs(log.get('logs', []))
            }
            resource_logs.append(resource_log)
        
        return {
            'resourceLogs': resource_logs
        }
    
    def _build_resource(self, resource_data):
        """æ„å»ºèµ„æºå¯¹è±¡"""
        attributes = []
        
        for key, value in resource_data.get('attributes', {}).items():
            attribute = {
                'key': key,
                'value': self._build_any_value(value)
            }
            attributes.append(attribute)
        
        return {
            'attributes': attributes
        }
    
    def _build_any_value(self, value):
        """æ„å»ºAnyValueå¯¹è±¡"""
        if isinstance(value, str):
            return {'stringValue': value}
        elif isinstance(value, bool):
            return {'boolValue': value}
        elif isinstance(value, int):
            return {'intValue': str(value)}
        elif isinstance(value, float):
            return {'doubleValue': value}
        elif isinstance(value, list):
            return {
                'arrayValue': {
                    'values': [self._build_any_value(item) for item in value]
                }
            }
        elif isinstance(value, dict):
            return {
                'kvlistValue': {
                    'values': [
                        {'key': k, 'value': self._build_any_value(v)}
                        for k, v in value.items()
                    ]
                }
            }
        else:
            return {'stringValue': str(value)}
    
    def _json_serializer(self, obj):
        """JSONåºåˆ—åŒ–å™¨"""
        if hasattr(obj, 'isoformat'):
            return obj.isoformat()
        elif hasattr(obj, '__dict__'):
            return obj.__dict__
        else:
            return str(obj)
```

## ğŸ¯ åº”ç”¨åœºæ™¯

### 1. é«˜æ€§èƒ½æ•°æ®ä¼ è¾“

#### æ‰¹é‡ä¼ è¾“ä¼˜åŒ–

```python
class OTLPBatchExporter:
    def __init__(self, transport, batch_size=1000, export_timeout=30):
        self.transport = transport
        self.batch_size = batch_size
        self.export_timeout = export_timeout
        self.batch_buffer = {
            'traces': [],
            'metrics': [],
            'logs': []
        }
        self.lock = threading.Lock()
        self.export_thread = None
        self.stop_event = threading.Event()
    
    def add_trace(self, trace):
        """æ·»åŠ è¿½è¸ªæ•°æ®"""
        with self.lock:
            self.batch_buffer['traces'].append(trace)
            
            if len(self.batch_buffer['traces']) >= self.batch_size:
                self._export_batch('traces')
    
    def add_metric(self, metric):
        """æ·»åŠ æŒ‡æ ‡æ•°æ®"""
        with self.lock:
            self.batch_buffer['metrics'].append(metric)
            
            if len(self.batch_buffer['metrics']) >= self.batch_size:
                self._export_batch('metrics')
    
    def add_log(self, log):
        """æ·»åŠ æ—¥å¿—æ•°æ®"""
        with self.lock:
            self.batch_buffer['logs'].append(log)
            
            if len(self.batch_buffer['logs']) >= self.batch_size:
                self._export_batch('logs')
    
    def _export_batch(self, data_type):
        """å¯¼å‡ºæ‰¹æ¬¡æ•°æ®"""
        with self.lock:
            batch_data = self.batch_buffer[data_type].copy()
            self.batch_buffer[data_type].clear()
        
        if not batch_data:
            return
        
        # å¼‚æ­¥å¯¼å‡º
        if data_type == 'traces':
            self.transport.export_traces(batch_data)
        elif data_type == 'metrics':
            self.transport.export_metrics(batch_data)
        elif data_type == 'logs':
            self.transport.export_logs(batch_data)
    
    def start_periodic_export(self, interval=5):
        """å¯åŠ¨å®šæœŸå¯¼å‡º"""
        self.export_thread = threading.Thread(
            target=self._periodic_export,
            args=(interval,)
        )
        self.export_thread.daemon = True
        self.export_thread.start()
    
    def _periodic_export(self, interval):
        """å®šæœŸå¯¼å‡º"""
        while not self.stop_event.is_set():
            time.sleep(interval)
            
            with self.lock:
                for data_type in ['traces', 'metrics', 'logs']:
                    if self.batch_buffer[data_type]:
                        self._export_batch(data_type)
    
    def stop(self):
        """åœæ­¢å¯¼å‡º"""
        self.stop_event.set()
        
        # å¯¼å‡ºå‰©ä½™æ•°æ®
        with self.lock:
            for data_type in ['traces', 'metrics', 'logs']:
                if self.batch_buffer[data_type]:
                    self._export_batch(data_type)
        
        if self.export_thread:
            self.export_thread.join()
```

#### æµå¼ä¼ è¾“ä¼˜åŒ–

```python
class OTLPStreamingExporter:
    def __init__(self, transport, stream_size=100):
        self.transport = transport
        self.stream_size = stream_size
        self.streams = {
            'traces': asyncio.Queue(maxsize=stream_size),
            'metrics': asyncio.Queue(maxsize=stream_size),
            'logs': asyncio.Queue(maxsize=stream_size)
        }
        self.stream_tasks = {}
        self.running = False
    
    async def start(self):
        """å¯åŠ¨æµå¼å¯¼å‡º"""
        self.running = True
        
        # å¯åŠ¨æµå¼ä»»åŠ¡
        self.stream_tasks['traces'] = asyncio.create_task(
            self._stream_export('traces')
        )
        self.stream_tasks['metrics'] = asyncio.create_task(
            self._stream_export('metrics')
        )
        self.stream_tasks['logs'] = asyncio.create_task(
            self._stream_export('logs')
        )
    
    async def stop(self):
        """åœæ­¢æµå¼å¯¼å‡º"""
        self.running = False
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        for task in self.stream_tasks.values():
            task.cancel()
        
        await asyncio.gather(*self.stream_tasks.values(), return_exceptions=True)
    
    async def add_trace(self, trace):
        """æ·»åŠ è¿½è¸ªæ•°æ®"""
        try:
            await self.streams['traces'].put(trace)
        except asyncio.QueueFull:
            # é˜Ÿåˆ—æ»¡æ—¶ä¸¢å¼ƒæœ€æ—§çš„æ•°æ®
            try:
                self.streams['traces'].get_nowait()
                await self.streams['traces'].put(trace)
            except asyncio.QueueEmpty:
                pass
    
    async def add_metric(self, metric):
        """æ·»åŠ æŒ‡æ ‡æ•°æ®"""
        try:
            await self.streams['metrics'].put(metric)
        except asyncio.QueueFull:
            # é˜Ÿåˆ—æ»¡æ—¶ä¸¢å¼ƒæœ€æ—§çš„æ•°æ®
            try:
                self.streams['metrics'].get_nowait()
                await self.streams['metrics'].put(metric)
            except asyncio.QueueEmpty:
                pass
    
    async def add_log(self, log):
        """æ·»åŠ æ—¥å¿—æ•°æ®"""
        try:
            await self.streams['logs'].put(log)
        except asyncio.QueueFull:
            # é˜Ÿåˆ—æ»¡æ—¶ä¸¢å¼ƒæœ€æ—§çš„æ•°æ®
            try:
                self.streams['logs'].get_nowait()
                await self.streams['logs'].put(log)
            except asyncio.QueueEmpty:
                pass
    
    async def _stream_export(self, data_type):
        """æµå¼å¯¼å‡º"""
        batch = []
        
        while self.running:
            try:
                # ç­‰å¾…æ•°æ®
                data = await asyncio.wait_for(
                    self.streams[data_type].get(),
                    timeout=1.0
                )
                
                batch.append(data)
                
                # è¾¾åˆ°æ‰¹æ¬¡å¤§å°æ—¶å¯¼å‡º
                if len(batch) >= self.stream_size:
                    await self._export_batch(data_type, batch)
                    batch = []
                
            except asyncio.TimeoutError:
                # è¶…æ—¶æ—¶å¯¼å‡ºå‰©ä½™æ•°æ®
                if batch:
                    await self._export_batch(data_type, batch)
                    batch = []
            except Exception as e:
                print(f"Error in stream export: {e}")
    
    async def _export_batch(self, data_type, batch):
        """å¯¼å‡ºæ‰¹æ¬¡æ•°æ®"""
        try:
            if data_type == 'traces':
                await self.transport.export_traces(batch)
            elif data_type == 'metrics':
                await self.transport.export_metrics(batch)
            elif data_type == 'logs':
                await self.transport.export_logs(batch)
        except Exception as e:
            print(f"Error exporting {data_type}: {e}")
```

### 2. é”™è¯¯å¤„ç†å’Œé‡è¯•

#### æ™ºèƒ½é‡è¯•æœºåˆ¶

```python
class OTLPRetryHandler:
    def __init__(self, max_retries=3, base_delay=1.0, max_delay=60.0, backoff_factor=2.0):
        self.max_retries = max_retries
        self.base_delay = base_delay
        self.max_delay = max_delay
        self.backoff_factor = backoff_factor
        self.retryable_errors = {
            'ConnectionError',
            'TimeoutError',
            'ServiceUnavailable',
            'InternalServerError',
            'BadGateway',
            'GatewayTimeout'
        }
    
    def should_retry(self, error):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•"""
        error_type = type(error).__name__
        return error_type in self.retryable_errors
    
    def calculate_delay(self, attempt):
        """è®¡ç®—é‡è¯•å»¶è¿Ÿ"""
        delay = self.base_delay * (self.backoff_factor ** attempt)
        return min(delay, self.max_delay)
    
    async def execute_with_retry(self, func, *args, **kwargs):
        """æ‰§è¡Œå¸¦é‡è¯•çš„å‡½æ•°"""
        last_error = None
        
        for attempt in range(self.max_retries + 1):
            try:
                return await func(*args, **kwargs)
            except Exception as e:
                last_error = e
                
                if attempt == self.max_retries or not self.should_retry(e):
                    raise e
                
                delay = self.calculate_delay(attempt)
                await asyncio.sleep(delay)
        
        raise last_error
    
    def execute_with_retry_sync(self, func, *args, **kwargs):
        """æ‰§è¡Œå¸¦é‡è¯•çš„åŒæ­¥å‡½æ•°"""
        last_error = None
        
        for attempt in range(self.max_retries + 1):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                last_error = e
                
                if attempt == self.max_retries or not self.should_retry(e):
                    raise e
                
                delay = self.calculate_delay(attempt)
                time.sleep(delay)
        
        raise last_error
```

#### é”™è¯¯æ¢å¤ç­–ç•¥

```python
class OTLPErrorRecovery:
    def __init__(self):
        self.recovery_strategies = {
            'ConnectionError': self._recover_connection_error,
            'TimeoutError': self._recover_timeout_error,
            'ServiceUnavailable': self._recover_service_unavailable,
            'InternalServerError': self._recover_internal_server_error,
            'BadGateway': self._recover_bad_gateway,
            'GatewayTimeout': self._recover_gateway_timeout
        }
        self.circuit_breaker = CircuitBreaker()
    
    async def recover_from_error(self, error, context):
        """ä»é”™è¯¯ä¸­æ¢å¤"""
        error_type = type(error).__name__
        recovery_strategy = self.recovery_strategies.get(error_type)
        
        if recovery_strategy:
            return await recovery_strategy(error, context)
        else:
            return await self._default_recovery(error, context)
    
    async def _recover_connection_error(self, error, context):
        """æ¢å¤è¿æ¥é”™è¯¯"""
        # æ£€æŸ¥ç”µè·¯æ–­è·¯å™¨çŠ¶æ€
        if self.circuit_breaker.is_open():
            return {'success': False, 'error': 'Circuit breaker is open'}
        
        # å°è¯•é‡æ–°è¿æ¥
        try:
            await context['transport'].reconnect()
            return {'success': True, 'error': None}
        except Exception as e:
            self.circuit_breaker.record_failure()
            return {'success': False, 'error': str(e)}
    
    async def _recover_timeout_error(self, error, context):
        """æ¢å¤è¶…æ—¶é”™è¯¯"""
        # å¢åŠ è¶…æ—¶æ—¶é—´
        context['transport'].timeout *= 2
        
        # å°è¯•é‡æ–°å‘é€
        try:
            return await context['transport'].export(context['data'])
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    async def _recover_service_unavailable(self, error, context):
        """æ¢å¤æœåŠ¡ä¸å¯ç”¨é”™è¯¯"""
        # ç­‰å¾…æœåŠ¡æ¢å¤
        await asyncio.sleep(5)
        
        # å°è¯•é‡æ–°å‘é€
        try:
            return await context['transport'].export(context['data'])
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    async def _recover_internal_server_error(self, error, context):
        """æ¢å¤å†…éƒ¨æœåŠ¡å™¨é”™è¯¯"""
        # è®°å½•é”™è¯¯å¹¶å°è¯•é‡æ–°å‘é€
        self._log_error(error, context)
        
        try:
            return await context['transport'].export(context['data'])
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    async def _recover_bad_gateway(self, error, context):
        """æ¢å¤åç½‘å…³é”™è¯¯"""
        # å°è¯•ä½¿ç”¨å¤‡ç”¨ç«¯ç‚¹
        if 'backup_endpoints' in context:
            for endpoint in context['backup_endpoints']:
                try:
                    context['transport'].endpoint = endpoint
                    return await context['transport'].export(context['data'])
                except Exception:
                    continue
        
        return {'success': False, 'error': 'All endpoints failed'}
    
    async def _recover_gateway_timeout(self, error, context):
        """æ¢å¤ç½‘å…³è¶…æ—¶é”™è¯¯"""
        # å‡å°‘æ•°æ®å¤§å°å¹¶é‡è¯•
        if 'data' in context:
            context['data'] = self._reduce_data_size(context['data'])
        
        try:
            return await context['transport'].export(context['data'])
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    async def _default_recovery(self, error, context):
        """é»˜è®¤æ¢å¤ç­–ç•¥"""
        # è®°å½•é”™è¯¯
        self._log_error(error, context)
        
        # è¿”å›å¤±è´¥
        return {'success': False, 'error': str(error)}
    
    def _log_error(self, error, context):
        """è®°å½•é”™è¯¯"""
        error_info = {
            'error_type': type(error).__name__,
            'error_message': str(error),
            'context': context,
            'timestamp': datetime.now().isoformat()
        }
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ—¥å¿—è®°å½•é€»è¾‘
        print(f"Error logged: {error_info}")
    
    def _reduce_data_size(self, data):
        """å‡å°‘æ•°æ®å¤§å°"""
        # è¿™é‡Œå¯ä»¥å®ç°æ•°æ®å¤§å°å‡å°‘é€»è¾‘
        # ä¾‹å¦‚ï¼šé‡‡æ ·ã€å‹ç¼©ã€åˆ é™¤éå…³é”®å­—æ®µç­‰
        return data
```

## ğŸ”§ æ€§èƒ½ä¼˜åŒ–

### 1. è¿æ¥æ± ä¼˜åŒ–

#### è¿æ¥æ± ç®¡ç†

```python
class OTLPConnectionPool:
    def __init__(self, max_connections=10, max_keepalive=30):
        self.max_connections = max_connections
        self.max_keepalive = max_keepalive
        self.connections = []
        self.available_connections = []
        self.used_connections = set()
        self.lock = threading.Lock()
        self.cleanup_thread = None
        self.running = False
    
    def start(self):
        """å¯åŠ¨è¿æ¥æ± """
        self.running = True
        self.cleanup_thread = threading.Thread(target=self._cleanup_connections)
        self.cleanup_thread.daemon = True
        self.cleanup_thread.start()
    
    def stop(self):
        """åœæ­¢è¿æ¥æ± """
        self.running = False
        
        with self.lock:
            for connection in self.connections:
                connection.close()
            
            self.connections.clear()
            self.available_connections.clear()
            self.used_connections.clear()
        
        if self.cleanup_thread:
            self.cleanup_thread.join()
    
    def get_connection(self, endpoint, credentials=None):
        """è·å–è¿æ¥"""
        with self.lock:
            # å°è¯•ä»å¯ç”¨è¿æ¥ä¸­è·å–
            for connection in self.available_connections:
                if connection.endpoint == endpoint and connection.credentials == credentials:
                    self.available_connections.remove(connection)
                    self.used_connections.add(connection)
                    return connection
            
            # åˆ›å»ºæ–°è¿æ¥
            if len(self.connections) < self.max_connections:
                connection = self._create_connection(endpoint, credentials)
                self.connections.append(connection)
                self.used_connections.add(connection)
                return connection
            
            # ç­‰å¾…å¯ç”¨è¿æ¥
            return None
    
    def return_connection(self, connection):
        """å½’è¿˜è¿æ¥"""
        with self.lock:
            if connection in self.used_connections:
                self.used_connections.remove(connection)
                self.available_connections.append(connection)
    
    def _create_connection(self, endpoint, credentials):
        """åˆ›å»ºè¿æ¥"""
        if credentials:
            return OTLPGRPCTransport(endpoint, credentials)
        else:
            return OTLPGRPCTransport(endpoint)
    
    def _cleanup_connections(self):
        """æ¸…ç†è¿æ¥"""
        while self.running:
            time.sleep(10)  # æ¯10ç§’æ¸…ç†ä¸€æ¬¡
            
            with self.lock:
                current_time = time.time()
                connections_to_remove = []
                
                for connection in self.available_connections:
                    if current_time - connection.last_used > self.max_keepalive:
                        connections_to_remove.append(connection)
                
                for connection in connections_to_remove:
                    self.available_connections.remove(connection)
                    self.connections.remove(connection)
                    connection.close()
```

### 2. æ•°æ®å‹ç¼©ä¼˜åŒ–

#### æ™ºèƒ½å‹ç¼©é€‰æ‹©

```python
class OTLPCompressionOptimizer:
    def __init__(self):
        self.compression_algorithms = {
            'gzip': GzipCompressor(),
            'deflate': DeflateCompressor(),
            'zstd': ZstdCompressor()
        }
        self.performance_metrics = {
            'gzip': {'compression_ratio': 0.3, 'speed': 1.0},
            'deflate': {'compression_ratio': 0.25, 'speed': 1.2},
            'zstd': {'compression_ratio': 0.2, 'speed': 0.8}
        }
    
    def select_optimal_compression(self, data_size, latency_requirement, bandwidth_constraint):
        """é€‰æ‹©æœ€ä¼˜å‹ç¼©ç®—æ³•"""
        candidates = []
        
        for algorithm, metrics in self.performance_metrics.items():
            compressed_size = data_size * metrics['compression_ratio']
            compression_time = data_size / metrics['speed']
            
            if (compression_time <= latency_requirement and 
                compressed_size <= bandwidth_constraint):
                candidates.append((algorithm, compressed_size, compression_time))
        
        if not candidates:
            return 'none'
        
        # é€‰æ‹©å‹ç¼©åå¤§å°æœ€å°çš„ç®—æ³•
        return min(candidates, key=lambda x: x[1])[0]
    
    def compress_data(self, data, algorithm):
        """å‹ç¼©æ•°æ®"""
        if algorithm == 'none':
            return data
        
        compressor = self.compression_algorithms.get(algorithm)
        if not compressor:
            raise ValueError(f"Unsupported compression algorithm: {algorithm}")
        
        return compressor.compress(data)
    
    def decompress_data(self, data, algorithm):
        """è§£å‹ç¼©æ•°æ®"""
        if algorithm == 'none':
            return data
        
        compressor = self.compression_algorithms.get(algorithm)
        if not compressor:
            raise ValueError(f"Unsupported compression algorithm: {algorithm}")
        
        return compressor.decompress(data)
```

## ğŸ§ª æµ‹è¯•ä¸éªŒè¯

### 1. åè®®æµ‹è¯•

```python
import unittest

class TestOTLPProtocol(unittest.TestCase):
    def setUp(self):
        self.grpc_transport = OTLPGRPCTransport('localhost:4317')
        self.http_transport = OTLPHTTPTransport('http://localhost:4318')
        self.protobuf_serializer = OTLPProtobufSerializer()
        self.json_serializer = OTLPJSONSerializer()
    
    def test_grpc_transport_connection(self):
        """æµ‹è¯•gRPCä¼ è¾“è¿æ¥"""
        self.assertIsNotNone(self.grpc_transport.channel)
        self.assertIsNotNone(self.grpc_transport.stub)
    
    def test_http_transport_connection(self):
        """æµ‹è¯•HTTPä¼ è¾“è¿æ¥"""
        self.assertIsNotNone(self.http_transport.session)
        self.assertIn('Content-Type', self.http_transport.headers)
    
    def test_protobuf_serialization(self):
        """æµ‹è¯•Protocol Buffersåºåˆ—åŒ–"""
        test_data = {
            'resource': {'service.name': 'test-service'},
            'spans': [{'name': 'test-span', 'start_time': 1234567890}]
        }
        
        serialized = self.protobuf_serializer.serialize('traces', test_data)
        self.assertIsInstance(serialized, bytes)
        self.assertGreater(len(serialized), 0)
    
    def test_json_serialization(self):
        """æµ‹è¯•JSONåºåˆ—åŒ–"""
        test_data = {
            'resource': {'service.name': 'test-service'},
            'spans': [{'name': 'test-span', 'start_time': 1234567890}]
        }
        
        serialized = self.json_serializer.serialize('traces', test_data)
        self.assertIsInstance(serialized, str)
        self.assertGreater(len(serialized), 0)
    
    def test_batch_exporter(self):
        """æµ‹è¯•æ‰¹é‡å¯¼å‡ºå™¨"""
        batch_exporter = OTLPBatchExporter(self.grpc_transport, batch_size=10)
        
        # æ·»åŠ æµ‹è¯•æ•°æ®
        for i in range(15):
            batch_exporter.add_trace({'name': f'test-span-{i}'})
        
        # éªŒè¯æ‰¹æ¬¡å¯¼å‡º
        self.assertEqual(len(batch_exporter.batch_buffer['traces']), 5)
    
    def test_compression_optimization(self):
        """æµ‹è¯•å‹ç¼©ä¼˜åŒ–"""
        optimizer = OTLPCompressionOptimizer()
        
        # æµ‹è¯•å‹ç¼©ç®—æ³•é€‰æ‹©
        optimal_algorithm = optimizer.select_optimal_compression(
            data_size=1000,
            latency_requirement=100,
            bandwidth_constraint=500
        )
        
        self.assertIn(optimal_algorithm, ['gzip', 'deflate', 'zstd', 'none'])
```

### 2. æ€§èƒ½æµ‹è¯•

```python
def benchmark_otlp_protocol():
    """OTLPåè®®æ€§èƒ½æµ‹è¯•"""
    # æµ‹è¯•åºåˆ—åŒ–æ€§èƒ½
    test_data = generate_test_trace_data(1000)
    
    protobuf_serializer = OTLPProtobufSerializer()
    json_serializer = OTLPJSONSerializer()
    
    # Protocol Buffersåºåˆ—åŒ–æ€§èƒ½
    start_time = time.time()
    protobuf_data = protobuf_serializer.serialize('traces', test_data)
    protobuf_time = time.time() - start_time
    
    # JSONåºåˆ—åŒ–æ€§èƒ½
    start_time = time.time()
    json_data = json_serializer.serialize('traces', test_data)
    json_time = time.time() - start_time
    
    print(f"Protocol Buffers serialization: {protobuf_time:.4f}s, size: {len(protobuf_data)} bytes")
    print(f"JSON serialization: {json_time:.4f}s, size: {len(json_data)} bytes")
    
    # æµ‹è¯•å‹ç¼©æ€§èƒ½
    compression_optimizer = OTLPCompressionOptimizer()
    
    for algorithm in ['gzip', 'deflate', 'zstd']:
        start_time = time.time()
        compressed_data = compression_optimizer.compress_data(protobuf_data, algorithm)
        compression_time = time.time() - start_time
        
        compression_ratio = len(compressed_data) / len(protobuf_data)
        print(f"{algorithm} compression: {compression_time:.4f}s, ratio: {compression_ratio:.2f}")

def benchmark_transport_performance():
    """ä¼ è¾“æ€§èƒ½æµ‹è¯•"""
    grpc_transport = OTLPGRPCTransport('localhost:4317')
    http_transport = OTLPHTTPTransport('http://localhost:4318')
    
    test_data = generate_test_trace_data(100)
    
    # gRPCä¼ è¾“æ€§èƒ½
    start_time = time.time()
    grpc_result = grpc_transport.export_traces(test_data)
    grpc_time = time.time() - start_time
    
    # HTTPä¼ è¾“æ€§èƒ½
    start_time = time.time()
    http_result = http_transport.export_traces(test_data)
    http_time = time.time() - start_time
    
    print(f"gRPC transport: {grpc_time:.4f}s, success: {grpc_result['success']}")
    print(f"HTTP transport: {http_time:.4f}s, success: {http_result['success']}")
```

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **OpenTelemetry Specification** (2023). *OpenTelemetry Protocol (OTLP)*.
2. **Protocol Buffers** (2023). *Protocol Buffers Language Guide*.
3. **gRPC Documentation** (2023). *gRPC Core Concepts*.
4. **HTTP/3 Specification** (2022). *RFC 9114: HTTP/3*.
5. **QUIC Protocol** (2021). *RFC 9000: QUIC: A UDP-Based Multiplexed and Secure Transport*.

## ğŸ”— ç›¸å…³èµ„æº

- [è¯­ä¹‰çº¦å®šå®ç°](è¯­ä¹‰çº¦å®šå®ç°.md)
- [æ•°æ®æ¨¡å‹å®ç°](æ•°æ®æ¨¡å‹å®ç°.md)
- [ä¼ è¾“åè®®å®ç°](ä¼ è¾“åè®®å®ç°.md)
- [å¤šè¯­è¨€SDKå¼€å‘](../SDKå¼€å‘/å¤šè¯­è¨€SDKå¼€å‘.md)

---

*æœ¬æ–‡æ¡£æ˜¯OpenTelemetry 2025å¹´çŸ¥è¯†ä½“ç³»æŠ€æœ¯å®ç°å±‚çš„ä¸€éƒ¨åˆ†*  
*æœ€åæ›´æ–°: 2025å¹´1æœˆ*  
*ç‰ˆæœ¬: 1.0.0*
