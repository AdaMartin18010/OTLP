# 多维关联分析

**文档版本**: 1.0.0  
**创建日期**: 2025年10月7日  
**所属**: 第六部分 - 多维度数据分析  

---

## 目录

- [多维关联分析](#多维关联分析)
  - [目录](#目录)
  - [概述](#概述)
  - [6.2.1 关联规则挖掘](#621-关联规则挖掘)
    - [Apriori算法](#apriori算法)
    - [FP-Growth算法](#fp-growth算法)
  - [6.2.2 因果关系分析](#622-因果关系分析)
    - [Granger因果检验](#granger因果检验)
    - [因果图构建](#因果图构建)
  - [6.2.3 相关性分析](#623-相关性分析)
    - [Pearson相关](#pearson相关)
    - [互信息](#互信息)
  - [6.2.4 多维聚类](#624-多维聚类)
    - [K-Means聚类](#k-means聚类)
    - [DBSCAN聚类](#dbscan聚类)
  - [总结](#总结)

---

## 概述

本文档介绍OTLP多维数据的关联规则挖掘、因果分析、相关性分析和聚类技术。

---

## 6.2.1 关联规则挖掘

### Apriori算法

**频繁项集挖掘**：

```go
// Apriori算法实现
type AprioriMiner struct {
    minSupport    float64
    minConfidence float64
}

type ItemSet map[string]bool

type Rule struct {
    Antecedent ItemSet
    Consequent ItemSet
    Support    float64
    Confidence float64
    Lift       float64
}

func (am *AprioriMiner) Mine(transactions [][]string) []Rule {
    // 1. 找到所有频繁项集
    frequentItemSets := am.findFrequentItemSets(transactions)
    
    // 2. 生成关联规则
    rules := am.generateRules(frequentItemSets, transactions)
    
    return rules
}

func (am *AprioriMiner) findFrequentItemSets(
    transactions [][]string,
) []ItemSet {
    n := len(transactions)
    frequentSets := []ItemSet{}
    
    // L1: 频繁1-项集
    candidates := am.generateL1(transactions)
    frequent := am.filterBySupport(candidates, transactions, n)
    frequentSets = append(frequentSets, frequent...)
    
    k := 2
    for len(frequent) > 0 {
        // Lk: 频繁k-项集
        candidates = am.aprioriGen(frequent, k)
        frequent = am.filterBySupport(candidates, transactions, n)
        frequentSets = append(frequentSets, frequent...)
        k++
    }
    
    return frequentSets
}

func (am *AprioriMiner) generateL1(transactions [][]string) []ItemSet {
    itemSets := []ItemSet{}
    items := make(map[string]bool)
    
    // 收集所有唯一项
    for _, transaction := range transactions {
        for _, item := range transaction {
            items[item] = true
        }
    }
    
    // 生成1-项集
    for item := range items {
        itemSet := make(ItemSet)
        itemSet[item] = true
        itemSets = append(itemSets, itemSet)
    }
    
    return itemSets
}

func (am *AprioriMiner) aprioriGen(
    frequentSets []ItemSet,
    k int,
) []ItemSet {
    candidates := []ItemSet{}
    
    // 连接步骤
    for i := 0; i < len(frequentSets); i++ {
        for j := i + 1; j < len(frequentSets); j++ {
            // 合并两个(k-1)-项集
            union := am.union(frequentSets[i], frequentSets[j])
            
            if len(union) == k {
                // 剪枝：检查所有(k-1)子集是否频繁
                if am.hasInfrequentSubset(union, frequentSets) {
                    continue
                }
                candidates = append(candidates, union)
            }
        }
    }
    
    return candidates
}

func (am *AprioriMiner) filterBySupport(
    itemSets []ItemSet,
    transactions [][]string,
    n int,
) []ItemSet {
    frequent := []ItemSet{}
    
    for _, itemSet := range itemSets {
        support := am.calculateSupport(itemSet, transactions)
        if support >= am.minSupport {
            frequent = append(frequent, itemSet)
        }
    }
    
    return frequent
}

func (am *AprioriMiner) calculateSupport(
    itemSet ItemSet,
    transactions [][]string,
) float64 {
    count := 0
    
    for _, transaction := range transactions {
        if am.contains(transaction, itemSet) {
            count++
        }
    }
    
    return float64(count) / float64(len(transactions))
}

func (am *AprioriMiner) generateRules(
    frequentSets []ItemSet,
    transactions [][]string,
) []Rule {
    rules := []Rule{}
    
    for _, itemSet := range frequentSets {
        if len(itemSet) < 2 {
            continue
        }
        
        // 生成所有非空真子集
        subsets := am.generateSubsets(itemSet)
        
        for _, antecedent := range subsets {
            consequent := am.difference(itemSet, antecedent)
            
            if len(consequent) == 0 {
                continue
            }
            
            // 计算置信度
            supportAB := am.calculateSupport(itemSet, transactions)
            supportA := am.calculateSupport(antecedent, transactions)
            confidence := supportAB / supportA
            
            if confidence >= am.minConfidence {
                // 计算提升度
                supportB := am.calculateSupport(consequent, transactions)
                lift := confidence / supportB
                
                rules = append(rules, Rule{
                    Antecedent: antecedent,
                    Consequent: consequent,
                    Support:    supportAB,
                    Confidence: confidence,
                    Lift:       lift,
                })
            }
        }
    }
    
    return rules
}
```

### FP-Growth算法

**FP树构建**：

```go
// FP-Growth算法
type FPGrowth struct {
    minSupport int
}

type FPNode struct {
    Item     string
    Count    int
    Parent   *FPNode
    Children map[string]*FPNode
    Next     *FPNode // 链表指针
}

type FPTree struct {
    Root       *FPNode
    HeaderTable map[string]*FPNode
}

func (fpg *FPGrowth) Mine(transactions [][]string) []ItemSet {
    // 1. 扫描数据库，计算项频率
    itemFreq := fpg.countItemFrequency(transactions)
    
    // 2. 过滤不频繁项
    frequentItems := fpg.filterFrequentItems(itemFreq)
    
    // 3. 按频率排序
    sortedItems := fpg.sortByFrequency(frequentItems)
    
    // 4. 构建FP树
    tree := fpg.buildFPTree(transactions, sortedItems)
    
    // 5. 挖掘频繁项集
    patterns := []ItemSet{}
    fpg.mineTree(tree, []string{}, &patterns)
    
    return patterns
}

func (fpg *FPGrowth) buildFPTree(
    transactions [][]string,
    sortedItems map[string]int,
) *FPTree {
    tree := &FPTree{
        Root: &FPNode{
            Item:     "root",
            Children: make(map[string]*FPNode),
        },
        HeaderTable: make(map[string]*FPNode),
    }
    
    for _, transaction := range transactions {
        // 过滤并排序
        filtered := fpg.filterAndSort(transaction, sortedItems)
        
        // 插入到FP树
        fpg.insertTree(filtered, tree.Root, tree.HeaderTable)
    }
    
    return tree
}

func (fpg *FPGrowth) insertTree(
    items []string,
    node *FPNode,
    headerTable map[string]*FPNode,
) {
    if len(items) == 0 {
        return
    }
    
    item := items[0]
    
    // 检查是否已有该子节点
    if child, exists := node.Children[item]; exists {
        child.Count++
        fpg.insertTree(items[1:], child, headerTable)
    } else {
        // 创建新节点
        newNode := &FPNode{
            Item:     item,
            Count:    1,
            Parent:   node,
            Children: make(map[string]*FPNode),
        }
        node.Children[item] = newNode
        
        // 更新头表链
        if header, exists := headerTable[item]; exists {
            // 找到链表末尾
            for header.Next != nil {
                header = header.Next
            }
            header.Next = newNode
        } else {
            headerTable[item] = newNode
        }
        
        fpg.insertTree(items[1:], newNode, headerTable)
    }
}

func (fpg *FPGrowth) mineTree(
    tree *FPTree,
    prefix []string,
    patterns *[]ItemSet,
) {
    // 从头表中获取单项
    for item, node := range tree.HeaderTable {
        // 新模式 = prefix + item
        newPattern := append(prefix, item)
        
        // 计算支持度
        support := fpg.calculateNodeSupport(node)
        
        if support >= fpg.minSupport {
            // 添加到频繁项集
            itemSet := make(ItemSet)
            for _, it := range newPattern {
                itemSet[it] = true
            }
            *patterns = append(*patterns, itemSet)
            
            // 构建条件FP树
            conditionalTree := fpg.buildConditionalTree(node)
            
            // 递归挖掘
            fpg.mineTree(conditionalTree, newPattern, patterns)
        }
    }
}
```

---

## 6.2.2 因果关系分析

### Granger因果检验

**时间序列因果检验**：

```go
// Granger因果检验
type GrangerCausality struct {
    maxLag int
}

func (gc *GrangerCausality) Test(x, y []float64) GrangerResult {
    // H0: x不是y的Granger原因
    // H1: x是y的Granger原因
    
    results := []LagResult{}
    
    for lag := 1; lag <= gc.maxLag; lag++ {
        // 1. 受限模型：y(t) = a0 + Σai*y(t-i)
        rssRestricted := gc.fitAR(y, lag)
        
        // 2. 非受限模型：y(t) = a0 + Σai*y(t-i) + Σbj*x(t-j)
        rssUnrestricted := gc.fitARX(y, x, lag)
        
        // 3. F统计量
        n := len(y) - lag
        fstat := ((rssRestricted - rssUnrestricted) / float64(lag)) /
            (rssUnrestricted / float64(n-2*lag))
        
        // 4. p值（简化：使用F分布）
        pvalue := gc.ftest(fstat, lag, n-2*lag)
        
        results = append(results, LagResult{
            Lag:    lag,
            FStat:  fstat,
            PValue: pvalue,
        })
    }
    
    // 选择最优lag
    bestLag := gc.selectBestLag(results)
    
    return GrangerResult{
        IsCausal: results[bestLag-1].PValue < 0.05,
        BestLag:  bestLag,
        Results:  results,
    }
}

func (gc *GrangerCausality) fitAR(y []float64, lag int) float64 {
    n := len(y) - lag
    
    // 构建设计矩阵
    X := make([][]float64, n)
    Y := make([]float64, n)
    
    for i := 0; i < n; i++ {
        X[i] = make([]float64, lag+1)
        X[i][0] = 1.0 // 截距
        
        for j := 1; j <= lag; j++ {
            X[i][j] = y[i+lag-j]
        }
        
        Y[i] = y[i+lag]
    }
    
    // OLS回归
    beta := gc.ols(X, Y)
    
    // 计算RSS
    rss := 0.0
    for i := 0; i < n; i++ {
        predicted := 0.0
        for j := 0; j <= lag; j++ {
            predicted += beta[j] * X[i][j]
        }
        residual := Y[i] - predicted
        rss += residual * residual
    }
    
    return rss
}

func (gc *GrangerCausality) fitARX(
    y, x []float64,
    lag int,
) float64 {
    n := len(y) - lag
    
    // 构建设计矩阵（包含x的滞后项）
    X := make([][]float64, n)
    Y := make([]float64, n)
    
    for i := 0; i < n; i++ {
        X[i] = make([]float64, 2*lag+1)
        X[i][0] = 1.0 // 截距
        
        // y的滞后项
        for j := 1; j <= lag; j++ {
            X[i][j] = y[i+lag-j]
        }
        
        // x的滞后项
        for j := 1; j <= lag; j++ {
            X[i][lag+j] = x[i+lag-j]
        }
        
        Y[i] = y[i+lag]
    }
    
    // OLS回归
    beta := gc.ols(X, Y)
    
    // 计算RSS
    rss := 0.0
    for i := 0; i < n; i++ {
        predicted := 0.0
        for j := 0; j < 2*lag+1; j++ {
            predicted += beta[j] * X[i][j]
        }
        residual := Y[i] - predicted
        rss += residual * residual
    }
    
    return rss
}
```

### 因果图构建

**PC算法（Peter-Clark）**：

```go
// PC算法构建因果图
type PCAlgorithm struct {
    alpha float64 // 显著性水平
}

type CausalGraph struct {
    Nodes []string
    Edges map[string]map[string]EdgeType
}

type EdgeType int

const (
    EdgeNone EdgeType = iota
    EdgeDirected   // A -> B
    EdgeUndirected // A - B
)

func (pc *PCAlgorithm) LearnStructure(
    data map[string][]float64,
) *CausalGraph {
    variables := []string{}
    for v := range data {
        variables = append(variables, v)
    }
    
    graph := &CausalGraph{
        Nodes: variables,
        Edges: make(map[string]map[string]EdgeType),
    }
    
    // 初始化完全无向图
    for _, v1 := range variables {
        graph.Edges[v1] = make(map[string]EdgeType)
        for _, v2 := range variables {
            if v1 != v2 {
                graph.Edges[v1][v2] = EdgeUndirected
            }
        }
    }
    
    // 1. 骨架识别（删除边）
    pc.skeletonDiscovery(graph, data)
    
    // 2. 边定向
    pc.orientEdges(graph, data)
    
    return graph
}

func (pc *PCAlgorithm) skeletonDiscovery(
    graph *CausalGraph,
    data map[string][]float64,
) {
    // 逐步增加条件集大小
    for condSize := 0; condSize < len(graph.Nodes)-2; condSize++ {
        for _, x := range graph.Nodes {
            neighbors := pc.getNeighbors(graph, x)
            
            for _, y := range neighbors {
                // 测试所有大小为condSize的条件集
                condSets := pc.generateCondSets(neighbors, y, condSize)
                
                for _, condSet := range condSets {
                    // 条件独立性检验
                    if pc.isConditionallyIndependent(x, y, condSet, data) {
                        // 删除边
                        delete(graph.Edges[x], y)
                        delete(graph.Edges[y], x)
                        break
                    }
                }
            }
        }
    }
}

func (pc *PCAlgorithm) isConditionallyIndependent(
    x, y string,
    condSet []string,
    data map[string][]float64,
) bool {
    // 使用偏相关检验
    partialCorr := pc.partialCorrelation(x, y, condSet, data)
    
    // Fisher's Z变换
    n := len(data[x])
    z := 0.5 * math.Log((1+partialCorr)/(1-partialCorr))
    z *= math.Sqrt(float64(n - len(condSet) - 3))
    
    // 检验统计量
    pvalue := 2 * (1 - pc.normalCDF(math.Abs(z)))
    
    return pvalue > pc.alpha
}

func (pc *PCAlgorithm) partialCorrelation(
    x, y string,
    condSet []string,
    data map[string][]float64,
) float64 {
    if len(condSet) == 0 {
        return pc.correlation(data[x], data[y])
    }
    
    // 递归计算偏相关
    z := condSet[0]
    restCond := condSet[1:]
    
    rxy_z := pc.partialCorrelation(x, y, restCond, data)
    rxz_z := pc.partialCorrelation(x, z, restCond, data)
    ryz_z := pc.partialCorrelation(y, z, restCond, data)
    
    numerator := rxy_z - rxz_z*ryz_z
    denominator := math.Sqrt((1 - rxz_z*rxz_z) * (1 - ryz_z*ryz_z))
    
    return numerator / denominator
}
```

---

## 6.2.3 相关性分析

### Pearson相关

**线性相关分析**：

```go
// 相关性分析器
type CorrelationAnalyzer struct{}

func (ca *CorrelationAnalyzer) PearsonCorrelation(
    x, y []float64,
) float64 {
    if len(x) != len(y) || len(x) == 0 {
        return 0
    }
    
    n := float64(len(x))
    
    // 计算均值
    meanX := ca.mean(x)
    meanY := ca.mean(y)
    
    // 计算协方差和标准差
    var cov, stdX, stdY float64
    
    for i := range x {
        dx := x[i] - meanX
        dy := y[i] - meanY
        
        cov += dx * dy
        stdX += dx * dx
        stdY += dy * dy
    }
    
    // Pearson相关系数
    return cov / math.Sqrt(stdX*stdY)
}

// Spearman秩相关
func (ca *CorrelationAnalyzer) SpearmanCorrelation(
    x, y []float64,
) float64 {
    // 1. 转换为秩
    rankX := ca.rank(x)
    rankY := ca.rank(y)
    
    // 2. 计算秩的Pearson相关
    return ca.PearsonCorrelation(rankX, rankY)
}

func (ca *CorrelationAnalyzer) rank(data []float64) []float64 {
    n := len(data)
    
    // 创建索引数组
    indices := make([]int, n)
    for i := range indices {
        indices[i] = i
    }
    
    // 按值排序索引
    sort.Slice(indices, func(i, j int) bool {
        return data[indices[i]] < data[indices[j]]
    })
    
    // 分配秩
    ranks := make([]float64, n)
    for i, idx := range indices {
        ranks[idx] = float64(i + 1)
    }
    
    return ranks
}

// 相关矩阵
func (ca *CorrelationAnalyzer) CorrelationMatrix(
    data map[string][]float64,
) map[string]map[string]float64 {
    variables := []string{}
    for v := range data {
        variables = append(variables, v)
    }
    
    matrix := make(map[string]map[string]float64)
    
    for _, v1 := range variables {
        matrix[v1] = make(map[string]float64)
        
        for _, v2 := range variables {
            if v1 == v2 {
                matrix[v1][v2] = 1.0
            } else {
                corr := ca.PearsonCorrelation(data[v1], data[v2])
                matrix[v1][v2] = corr
            }
        }
    }
    
    return matrix
}
```

### 互信息

**非线性相关度量**：

```go
// 互信息计算器
type MutualInformationCalculator struct {
    bins int
}

func (mic *MutualInformationCalculator) Calculate(
    x, y []float64,
) float64 {
    // 1. 离散化
    xBins := mic.discretize(x, mic.bins)
    yBins := mic.discretize(y, mic.bins)
    
    // 2. 计算联合概率和边缘概率
    pxy := mic.jointProbability(xBins, yBins)
    px := mic.marginalProbability(xBins)
    py := mic.marginalProbability(yBins)
    
    // 3. 计算互信息
    mi := 0.0
    
    for i := 0; i < mic.bins; i++ {
        for j := 0; j < mic.bins; j++ {
            if pxy[i][j] > 0 && px[i] > 0 && py[j] > 0 {
                mi += pxy[i][j] * math.Log2(pxy[i][j]/(px[i]*py[j]))
            }
        }
    }
    
    return mi
}

func (mic *MutualInformationCalculator) discretize(
    data []float64,
    bins int,
) []int {
    n := len(data)
    result := make([]int, n)
    
    // 找到最小值和最大值
    min, max := data[0], data[0]
    for _, v := range data {
        if v < min {
            min = v
        }
        if v > max {
            max = v
        }
    }
    
    // 计算bin宽度
    width := (max - min) / float64(bins)
    
    // 分配到bins
    for i, v := range data {
        bin := int((v - min) / width)
        if bin >= bins {
            bin = bins - 1
        }
        result[i] = bin
    }
    
    return result
}

func (mic *MutualInformationCalculator) jointProbability(
    x, y []int,
) [][]float64 {
    n := len(x)
    counts := make([][]int, mic.bins)
    for i := range counts {
        counts[i] = make([]int, mic.bins)
    }
    
    // 计数
    for i := 0; i < n; i++ {
        counts[x[i]][y[i]]++
    }
    
    // 转换为概率
    prob := make([][]float64, mic.bins)
    for i := range prob {
        prob[i] = make([]float64, mic.bins)
        for j := range prob[i] {
            prob[i][j] = float64(counts[i][j]) / float64(n)
        }
    }
    
    return prob
}
```

---

## 6.2.4 多维聚类

### K-Means聚类

**经典聚类算法**：

```go
// K-Means聚类器
type KMeans struct {
    k          int
    maxIter    int
    centroids  [][]float64
}

func (km *KMeans) Fit(data [][]float64) []int {
    n := len(data)
    dim := len(data[0])
    
    // 1. 初始化质心（K-Means++）
    km.centroids = km.initCentroids(data)
    
    // 2. 迭代优化
    labels := make([]int, n)
    
    for iter := 0; iter < km.maxIter; iter++ {
        // 分配样本到最近的质心
        changed := false
        for i, point := range data {
            newLabel := km.nearest(point)
            if newLabel != labels[i] {
                changed = true
                labels[i] = newLabel
            }
        }
        
        if !changed {
            break
        }
        
        // 更新质心
        km.updateCentroids(data, labels, dim)
    }
    
    return labels
}

func (km *KMeans) initCentroids(data [][]float64) [][]float64 {
    centroids := make([][]float64, km.k)
    
    // K-Means++初始化
    // 1. 随机选择第一个质心
    centroids[0] = data[rand.Intn(len(data))]
    
    // 2. 依次选择剩余质心
    for i := 1; i < km.k; i++ {
        distances := make([]float64, len(data))
        
        // 计算每个点到最近质心的距离
        for j, point := range data {
            minDist := math.Inf(1)
            for k := 0; k < i; k++ {
                dist := km.distance(point, centroids[k])
                if dist < minDist {
                    minDist = dist
                }
            }
            distances[j] = minDist
        }
        
        // 按距离平方的概率选择
        centroids[i] = km.weightedRandomChoice(data, distances)
    }
    
    return centroids
}

func (km *KMeans) nearest(point []float64) int {
    minDist := math.Inf(1)
    nearest := 0
    
    for i, centroid := range km.centroids {
        dist := km.distance(point, centroid)
        if dist < minDist {
            minDist = dist
            nearest = i
        }
    }
    
    return nearest
}

func (km *KMeans) distance(a, b []float64) float64 {
    var sum float64
    for i := range a {
        diff := a[i] - b[i]
        sum += diff * diff
    }
    return math.Sqrt(sum)
}

func (km *KMeans) updateCentroids(
    data [][]float64,
    labels []int,
    dim int,
) {
    counts := make([]int, km.k)
    sums := make([][]float64, km.k)
    
    for i := range sums {
        sums[i] = make([]float64, dim)
    }
    
    // 累加
    for i, point := range data {
        label := labels[i]
        counts[label]++
        for j := range point {
            sums[label][j] += point[j]
        }
    }
    
    // 计算平均
    for i := range km.centroids {
        if counts[i] > 0 {
            for j := range km.centroids[i] {
                km.centroids[i][j] = sums[i][j] / float64(counts[i])
            }
        }
    }
}
```

### DBSCAN聚类

**基于密度的聚类**：

```go
// DBSCAN聚类器
type DBSCAN struct {
    eps        float64
    minPts     int
    labels     []int
    visited    []bool
}

const (
    Noise     = -1
    Unvisited = 0
)

func (db *DBSCAN) Fit(data [][]float64) []int {
    n := len(data)
    db.labels = make([]int, n)
    db.visited = make([]bool, n)
    
    clusterID := 1
    
    for i := 0; i < n; i++ {
        if db.visited[i] {
            continue
        }
        
        db.visited[i] = true
        
        // 找到邻域
        neighbors := db.regionQuery(data, i)
        
        if len(neighbors) < db.minPts {
            // 噪声点
            db.labels[i] = Noise
        } else {
            // 扩展簇
            db.expandCluster(data, i, neighbors, clusterID)
            clusterID++
        }
    }
    
    return db.labels
}

func (db *DBSCAN) expandCluster(
    data [][]float64,
    pointIdx int,
    neighbors []int,
    clusterID int,
) {
    db.labels[pointIdx] = clusterID
    
    // 使用队列进行BFS
    queue := append([]int{}, neighbors...)
    
    for len(queue) > 0 {
        current := queue[0]
        queue = queue[1:]
        
        if !db.visited[current] {
            db.visited[current] = true
            
            // 找到当前点的邻域
            currentNeighbors := db.regionQuery(data, current)
            
            if len(currentNeighbors) >= db.minPts {
                queue = append(queue, currentNeighbors...)
            }
        }
        
        // 分配到簇
        if db.labels[current] == Unvisited || db.labels[current] == Noise {
            db.labels[current] = clusterID
        }
    }
}

func (db *DBSCAN) regionQuery(data [][]float64, pointIdx int) []int {
    neighbors := []int{}
    
    for i, point := range data {
        if db.distance(data[pointIdx], point) <= db.eps {
            neighbors = append(neighbors, i)
        }
    }
    
    return neighbors
}

func (db *DBSCAN) distance(a, b []float64) float64 {
    var sum float64
    for i := range a {
        diff := a[i] - b[i]
        sum += diff * diff
    }
    return math.Sqrt(sum)
}
```

---

## 总结

多维关联分析核心技术：

**关联规则挖掘**：

- Apriori：频繁项集
- FP-Growth：FP树高效
- 支持度、置信度、提升度

**因果关系分析**：

- Granger检验：时间序列
- PC算法：因果图构建
- 条件独立性检验

**相关性分析**：

- Pearson：线性相关
- Spearman：秩相关
- 互信息：非线性

**多维聚类**：

- K-Means：质心聚类
- DBSCAN：密度聚类
- 自动发现模式

**最佳实践**：

- 数据预处理（归一化）
- 多算法对比验证
- 可视化分析结果
- 领域知识结合

---

**上一篇**: [17_时序数据分析.md](17_时序数据分析.md)  
**下一篇**: [19_异常模式识别.md](19_异常模式识别.md)

---

*最后更新: 2025年10月7日*-
