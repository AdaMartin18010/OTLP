# 诊断决策树

**文档版本**: 1.0.0  
**创建日期**: 2025年10月7日  
**所属**: 第七部分 - 系统状态推理与诊断  

---

## 目录

- [诊断决策树](#诊断决策树)
  - [目录](#目录)
  - [概述](#概述)
  - [7.3.1 决策树构建](#731-决策树构建)
    - [ID3算法](#id3算法)
    - [C4.5算法](#c45算法)
    - [CART算法](#cart算法)
  - [7.3.2 诊断规则生成](#732-诊断规则生成)
    - [规则提取](#规则提取)
    - [规则简化](#规则简化)
  - [7.3.3 动态决策树](#733-动态决策树)
    - [在线学习](#在线学习)
    - [增量更新](#增量更新)
  - [7.3.4 诊断路径优化](#734-诊断路径优化)
    - [剪枝策略](#剪枝策略)
    - [成本敏感学习](#成本敏感学习)
  - [总结](#总结)

---

## 概述

本文档介绍OTLP的诊断决策树技术，包括决策树构建、规则生成、动态更新和路径优化。

---

## 7.3.1 决策树构建

### ID3算法

**信息增益**：

```go
// ID3决策树构建器
type ID3Builder struct {
    maxDepth   int
    minSamples int
}

type DiagnosticTree struct {
    Root *DiagnosticNode
}

type DiagnosticNode struct {
    Feature    string
    Threshold  float64
    IsLeaf     bool
    Diagnosis  string
    Confidence float64
    Children   map[string]*DiagnosticNode
    Samples    int
}

func (id3 *ID3Builder) Build(
    data []DiagnosticCase,
    features []string,
    depth int,
) *DiagnosticNode {
    // 终止条件
    if depth >= id3.maxDepth || len(data) < id3.minSamples {
        return id3.createLeaf(data)
    }
    
    // 检查是否所有样本同类
    if id3.isPure(data) {
        return id3.createLeaf(data)
    }
    
    // 选择最佳特征
    bestFeature := id3.selectBestFeature(data, features)
    
    if bestFeature == "" {
        return id3.createLeaf(data)
    }
    
    // 创建节点
    node := &DiagnosticNode{
        Feature:  bestFeature,
        IsLeaf:   false,
        Children: make(map[string]*DiagnosticNode),
        Samples:  len(data),
    }
    
    // 按特征值分割数据
    subsets := id3.splitByFeature(data, bestFeature)
    
    // 递归构建子树
    remainingFeatures := id3.removeFeature(features, bestFeature)
    
    for value, subset := range subsets {
        node.Children[value] = id3.Build(subset, remainingFeatures, depth+1)
    }
    
    return node
}

type DiagnosticCase struct {
    Features  map[string]string
    Diagnosis string
}

func (id3 *ID3Builder) selectBestFeature(
    data []DiagnosticCase,
    features []string,
) string {
    baseEntropy := id3.calculateEntropy(data)
    maxGain := 0.0
    bestFeature := ""
    
    for _, feature := range features {
        // 计算信息增益
        gain := id3.informationGain(data, feature, baseEntropy)
        
        if gain > maxGain {
            maxGain = gain
            bestFeature = feature
        }
    }
    
    return bestFeature
}

func (id3 *ID3Builder) calculateEntropy(data []DiagnosticCase) float64 {
    if len(data) == 0 {
        return 0
    }
    
    // 统计各类别数量
    counts := make(map[string]int)
    for _, case_ := range data {
        counts[case_.Diagnosis]++
    }
    
    // 计算熵
    entropy := 0.0
    total := float64(len(data))
    
    for _, count := range counts {
        if count > 0 {
            p := float64(count) / total
            entropy -= p * math.Log2(p)
        }
    }
    
    return entropy
}

func (id3 *ID3Builder) informationGain(
    data []DiagnosticCase,
    feature string,
    baseEntropy float64,
) float64 {
    // 按特征值分割
    subsets := id3.splitByFeature(data, feature)
    
    // 计算条件熵
    conditionalEntropy := 0.0
    total := float64(len(data))
    
    for _, subset := range subsets {
        weight := float64(len(subset)) / total
        conditionalEntropy += weight * id3.calculateEntropy(subset)
    }
    
    // 信息增益
    return baseEntropy - conditionalEntropy
}

func (id3 *ID3Builder) splitByFeature(
    data []DiagnosticCase,
    feature string,
) map[string][]DiagnosticCase {
    subsets := make(map[string][]DiagnosticCase)
    
    for _, case_ := range data {
        value := case_.Features[feature]
        subsets[value] = append(subsets[value], case_)
    }
    
    return subsets
}

func (id3 *ID3Builder) createLeaf(data []DiagnosticCase) *DiagnosticNode {
    // 统计最常见的诊断
    counts := make(map[string]int)
    for _, case_ := range data {
        counts[case_.Diagnosis]++
    }
    
    maxCount := 0
    diagnosis := ""
    
    for diag, count := range counts {
        if count > maxCount {
            maxCount = count
            diagnosis = diag
        }
    }
    
    confidence := float64(maxCount) / float64(len(data))
    
    return &DiagnosticNode{
        IsLeaf:     true,
        Diagnosis:  diagnosis,
        Confidence: confidence,
        Samples:    len(data),
    }
}
```

### C4.5算法

**信息增益率**：

```go
// C4.5决策树构建器
type C45Builder struct {
    maxDepth   int
    minSamples int
}

func (c45 *C45Builder) Build(
    data []DiagnosticCase,
    features []string,
    depth int,
) *DiagnosticNode {
    // 类似ID3，但使用信息增益率
    if depth >= c45.maxDepth || len(data) < c45.minSamples {
        return c45.createLeaf(data)
    }
    
    if c45.isPure(data) {
        return c45.createLeaf(data)
    }
    
    // 选择最佳特征（使用增益率）
    bestFeature := c45.selectBestFeatureByGainRatio(data, features)
    
    if bestFeature == "" {
        return c45.createLeaf(data)
    }
    
    node := &DiagnosticNode{
        Feature:  bestFeature,
        IsLeaf:   false,
        Children: make(map[string]*DiagnosticNode),
        Samples:  len(data),
    }
    
    subsets := c45.splitByFeature(data, bestFeature)
    remainingFeatures := c45.removeFeature(features, bestFeature)
    
    for value, subset := range subsets {
        node.Children[value] = c45.Build(subset, remainingFeatures, depth+1)
    }
    
    return node
}

func (c45 *C45Builder) selectBestFeatureByGainRatio(
    data []DiagnosticCase,
    features []string,
) string {
    baseEntropy := c45.calculateEntropy(data)
    maxGainRatio := 0.0
    bestFeature := ""
    
    for _, feature := range features {
        // 信息增益
        gain := c45.informationGain(data, feature, baseEntropy)
        
        // 分裂信息
        splitInfo := c45.splitInformation(data, feature)
        
        // 增益率
        gainRatio := 0.0
        if splitInfo > 0 {
            gainRatio = gain / splitInfo
        }
        
        if gainRatio > maxGainRatio {
            maxGainRatio = gainRatio
            bestFeature = feature
        }
    }
    
    return bestFeature
}

func (c45 *C45Builder) splitInformation(
    data []DiagnosticCase,
    feature string,
) float64 {
    subsets := c45.splitByFeature(data, feature)
    
    splitInfo := 0.0
    total := float64(len(data))
    
    for _, subset := range subsets {
        if len(subset) > 0 {
            p := float64(len(subset)) / total
            splitInfo -= p * math.Log2(p)
        }
    }
    
    return splitInfo
}

// 处理连续值特征
func (c45 *C45Builder) findBestThreshold(
    data []DiagnosticCase,
    feature string,
) float64 {
    // 收集所有值并排序
    values := []float64{}
    for _, case_ := range data {
        if val, err := strconv.ParseFloat(case_.Features[feature], 64); err == nil {
            values = append(values, val)
        }
    }
    
    sort.Float64s(values)
    
    // 尝试所有可能的分割点
    bestThreshold := 0.0
    maxGain := 0.0
    
    for i := 0; i < len(values)-1; i++ {
        threshold := (values[i] + values[i+1]) / 2
        gain := c45.calculateGainForThreshold(data, feature, threshold)
        
        if gain > maxGain {
            maxGain = gain
            bestThreshold = threshold
        }
    }
    
    return bestThreshold
}
```

### CART算法

**基尼指数**：

```go
// CART决策树构建器
type CARTBuilder struct {
    maxDepth   int
    minSamples int
}

func (cart *CARTBuilder) Build(
    data []DiagnosticCase,
    features []string,
    depth int,
) *DiagnosticNode {
    if depth >= cart.maxDepth || len(data) < cart.minSamples {
        return cart.createLeaf(data)
    }
    
    if cart.isPure(data) {
        return cart.createLeaf(data)
    }
    
    // 选择最佳分割（使用基尼指数）
    bestFeature, bestThreshold := cart.selectBestSplit(data, features)
    
    if bestFeature == "" {
        return cart.createLeaf(data)
    }
    
    // 二叉分割
    node := &DiagnosticNode{
        Feature:   bestFeature,
        Threshold: bestThreshold,
        IsLeaf:    false,
        Children:  make(map[string]*DiagnosticNode),
        Samples:   len(data),
    }
    
    leftData, rightData := cart.binarySplit(data, bestFeature, bestThreshold)
    
    node.Children["left"] = cart.Build(leftData, features, depth+1)
    node.Children["right"] = cart.Build(rightData, features, depth+1)
    
    return node
}

func (cart *CARTBuilder) selectBestSplit(
    data []DiagnosticCase,
    features []string,
) (string, float64) {
    minGini := math.Inf(1)
    bestFeature := ""
    bestThreshold := 0.0
    
    for _, feature := range features {
        // 尝试所有可能的阈值
        thresholds := cart.getCandidateThresholds(data, feature)
        
        for _, threshold := range thresholds {
            gini := cart.calculateGiniForSplit(data, feature, threshold)
            
            if gini < minGini {
                minGini = gini
                bestFeature = feature
                bestThreshold = threshold
            }
        }
    }
    
    return bestFeature, bestThreshold
}

func (cart *CARTBuilder) calculateGiniForSplit(
    data []DiagnosticCase,
    feature string,
    threshold float64,
) float64 {
    leftData, rightData := cart.binarySplit(data, feature, threshold)
    
    total := float64(len(data))
    leftWeight := float64(len(leftData)) / total
    rightWeight := float64(len(rightData)) / total
    
    leftGini := cart.calculateGini(leftData)
    rightGini := cart.calculateGini(rightData)
    
    return leftWeight*leftGini + rightWeight*rightGini
}

func (cart *CARTBuilder) calculateGini(data []DiagnosticCase) float64 {
    if len(data) == 0 {
        return 0
    }
    
    counts := make(map[string]int)
    for _, case_ := range data {
        counts[case_.Diagnosis]++
    }
    
    gini := 1.0
    total := float64(len(data))
    
    for _, count := range counts {
        p := float64(count) / total
        gini -= p * p
    }
    
    return gini
}

func (cart *CARTBuilder) binarySplit(
    data []DiagnosticCase,
    feature string,
    threshold float64,
) ([]DiagnosticCase, []DiagnosticCase) {
    left := []DiagnosticCase{}
    right := []DiagnosticCase{}
    
    for _, case_ := range data {
        value, _ := strconv.ParseFloat(case_.Features[feature], 64)
        
        if value < threshold {
            left = append(left, case_)
        } else {
            right = append(right, case_)
        }
    }
    
    return left, right
}
```

---

## 7.3.2 诊断规则生成

### 规则提取

**从决策树提取规则**：

```go
// 规则提取器
type RuleExtractor struct{}

type DiagnosticRule struct {
    Conditions []Condition
    Diagnosis  string
    Confidence float64
    Support    int
}

type Condition struct {
    Feature   string
    Operator  string
    Value     interface{}
}

func (re *RuleExtractor) ExtractRules(
    tree *DiagnosticTree,
) []DiagnosticRule {
    rules := []DiagnosticRule{}
    
    // DFS遍历树，收集路径
    re.traverseTree(tree.Root, []Condition{}, &rules)
    
    return rules
}

func (re *RuleExtractor) traverseTree(
    node *DiagnosticNode,
    conditions []Condition,
    rules *[]DiagnosticRule,
) {
    if node.IsLeaf {
        // 到达叶节点，生成规则
        rule := DiagnosticRule{
            Conditions: conditions,
            Diagnosis:  node.Diagnosis,
            Confidence: node.Confidence,
            Support:    node.Samples,
        }
        *rules = append(*rules, rule)
        return
    }
    
    // 遍历子节点
    for value, child := range node.Children {
        // 添加条件
        condition := Condition{
            Feature:  node.Feature,
            Operator: "==",
            Value:    value,
        }
        
        newConditions := append([]Condition{}, conditions...)
        newConditions = append(newConditions, condition)
        
        re.traverseTree(child, newConditions, rules)
    }
}

// 规则转换为可读文本
func (rule *DiagnosticRule) ToText() string {
    var builder strings.Builder
    
    builder.WriteString("IF ")
    
    for i, cond := range rule.Conditions {
        if i > 0 {
            builder.WriteString(" AND ")
        }
        
        builder.WriteString(fmt.Sprintf("%s %s %v",
            cond.Feature,
            cond.Operator,
            cond.Value,
        ))
    }
    
    builder.WriteString(fmt.Sprintf(" THEN %s (confidence: %.2f, support: %d)",
        rule.Diagnosis,
        rule.Confidence,
        rule.Support,
    ))
    
    return builder.String()
}
```

### 规则简化

**规则优化**：

```go
// 规则简化器
type RuleSimplifier struct {
    minConfidence float64
    minSupport    int
}

func (rs *RuleSimplifier) Simplify(rules []DiagnosticRule) []DiagnosticRule {
    // 1. 过滤低质量规则
    filtered := rs.filterByQuality(rules)
    
    // 2. 合并相似规则
    merged := rs.mergeRules(filtered)
    
    // 3. 删除冗余条件
    simplified := rs.removeRedundantConditions(merged)
    
    return simplified
}

func (rs *RuleSimplifier) filterByQuality(
    rules []DiagnosticRule,
) []DiagnosticRule {
    filtered := []DiagnosticRule{}
    
    for _, rule := range rules {
        if rule.Confidence >= rs.minConfidence &&
            rule.Support >= rs.minSupport {
            filtered = append(filtered, rule)
        }
    }
    
    return filtered
}

func (rs *RuleSimplifier) mergeRules(
    rules []DiagnosticRule,
) []DiagnosticRule {
    merged := []DiagnosticRule{}
    used := make(map[int]bool)
    
    for i := 0; i < len(rules); i++ {
        if used[i] {
            continue
        }
        
        similar := []int{i}
        
        // 查找相似规则
        for j := i + 1; j < len(rules); j++ {
            if used[j] {
                continue
            }
            
            if rs.areSimilar(rules[i], rules[j]) {
                similar = append(similar, j)
                used[j] = true
            }
        }
        
        // 合并
        if len(similar) > 1 {
            mergedRule := rs.mergeSimilarRules(rules, similar)
            merged = append(merged, mergedRule)
        } else {
            merged = append(merged, rules[i])
        }
        
        used[i] = true
    }
    
    return merged
}

func (rs *RuleSimplifier) areSimilar(
    rule1, rule2 DiagnosticRule,
) bool {
    // 相同诊断
    if rule1.Diagnosis != rule2.Diagnosis {
        return false
    }
    
    // 条件数量相同
    if len(rule1.Conditions) != len(rule2.Conditions) {
        return false
    }
    
    // 大部分条件相同
    matchCount := 0
    for _, cond1 := range rule1.Conditions {
        for _, cond2 := range rule2.Conditions {
            if cond1.Feature == cond2.Feature &&
                cond1.Operator == cond2.Operator {
                matchCount++
                break
            }
        }
    }
    
    similarity := float64(matchCount) / float64(len(rule1.Conditions))
    return similarity >= 0.8
}

func (rs *RuleSimplifier) removeRedundantConditions(
    rules []DiagnosticRule,
) []DiagnosticRule {
    simplified := []DiagnosticRule{}
    
    for _, rule := range rules {
        // 尝试删除每个条件
        essential := []Condition{}
        
        for i, cond := range rule.Conditions {
            // 临时删除该条件
            tempConditions := append([]Condition{}, rule.Conditions[:i]...)
            tempConditions = append(tempConditions, rule.Conditions[i+1:]...)
            
            // 检查是否影响准确性
            if rs.isEssential(cond, tempConditions, rule) {
                essential = append(essential, cond)
            }
        }
        
        simplified = append(simplified, DiagnosticRule{
            Conditions: essential,
            Diagnosis:  rule.Diagnosis,
            Confidence: rule.Confidence,
            Support:    rule.Support,
        })
    }
    
    return simplified
}
```

---

## 7.3.3 动态决策树

### 在线学习

**增量式决策树**：

```go
// 在线决策树
type OnlineDecisionTree struct {
    root          *DiagnosticNode
    updateCount   int
    rebuildThreshold int
    buffer        []DiagnosticCase
}

func (odt *OnlineDecisionTree) Update(newCase DiagnosticCase) {
    // 添加到缓冲区
    odt.buffer = append(odt.buffer, newCase)
    odt.updateCount++
    
    // 检查是否需要重建
    if odt.updateCount >= odt.rebuildThreshold {
        odt.rebuild()
        odt.updateCount = 0
        odt.buffer = []DiagnosticCase{}
    } else {
        // 局部更新
        odt.incrementalUpdate(newCase)
    }
}

func (odt *OnlineDecisionTree) incrementalUpdate(newCase DiagnosticCase) {
    // 找到对应的叶节点
    leaf := odt.findLeaf(odt.root, newCase)
    
    if leaf == nil {
        return
    }
    
    // 更新叶节点统计
    leaf.Samples++
    
    // 检查是否需要分裂
    if odt.shouldSplit(leaf) {
        odt.splitLeaf(leaf)
    }
}

func (odt *OnlineDecisionTree) findLeaf(
    node *DiagnosticNode,
    case_ DiagnosticCase,
) *DiagnosticNode {
    if node.IsLeaf {
        return node
    }
    
    // 根据特征值导航
    featureValue := case_.Features[node.Feature]
    
    if child, exists := node.Children[featureValue]; exists {
        return odt.findLeaf(child, case_)
    }
    
    return nil
}

func (odt *OnlineDecisionTree) shouldSplit(leaf *DiagnosticNode) bool {
    // 样本数足够
    if leaf.Samples < 30 {
        return false
    }
    
    // 置信度不够高
    if leaf.Confidence > 0.95 {
        return false
    }
    
    return true
}

func (odt *OnlineDecisionTree) splitLeaf(leaf *DiagnosticNode) {
    // 从缓冲区获取该叶节点的样本
    samples := odt.getSamplesForLeaf(leaf)
    
    if len(samples) < 10 {
        return
    }
    
    // 选择最佳分割特征
    builder := &ID3Builder{}
    bestFeature := builder.selectBestFeature(samples, odt.getAvailableFeatures())
    
    if bestFeature == "" {
        return
    }
    
    // 转换为内部节点
    leaf.IsLeaf = false
    leaf.Feature = bestFeature
    leaf.Children = make(map[string]*DiagnosticNode)
    
    // 创建子节点
    subsets := builder.splitByFeature(samples, bestFeature)
    
    for value, subset := range subsets {
        leaf.Children[value] = builder.createLeaf(subset)
    }
}
```

### 增量更新

**Hoeffding树**：

```go
// Hoeffding树（VFDT）
type HoeffdingTree struct {
    root             *HoeffdingNode
    delta            float64 // 置信度参数
    nmin             int     // 最小样本数
    tau              float64 // 分裂阈值
}

type HoeffdingNode struct {
    *DiagnosticNode
    statistics map[string]*FeatureStatistics
}

type FeatureStatistics struct {
    ValueCounts map[string]map[string]int // [feature_value][class] = count
    TotalCount  int
}

func (ht *HoeffdingTree) Learn(case_ DiagnosticCase) {
    // 找到对应的叶节点
    leaf := ht.findLeaf(ht.root, case_)
    
    // 更新统计信息
    ht.updateStatistics(leaf, case_)
    
    // 检查是否可以分裂
    if leaf.Samples >= ht.nmin {
        ht.attemptSplit(leaf)
    }
}

func (ht *HoeffdingTree) updateStatistics(
    node *HoeffdingNode,
    case_ DiagnosticCase,
) {
    node.Samples++
    
    // 更新每个特征的统计
    for feature, value := range case_.Features {
        if _, exists := node.statistics[feature]; !exists {
            node.statistics[feature] = &FeatureStatistics{
                ValueCounts: make(map[string]map[string]int),
            }
        }
        
        stats := node.statistics[feature]
        
        if _, exists := stats.ValueCounts[value]; !exists {
            stats.ValueCounts[value] = make(map[string]int)
        }
        
        stats.ValueCounts[value][case_.Diagnosis]++
        stats.TotalCount++
    }
}

func (ht *HoeffdingTree) attemptSplit(node *HoeffdingNode) {
    // 计算每个特征的信息增益
    gains := make(map[string]float64)
    
    for feature, stats := range node.statistics {
        gains[feature] = ht.calculateInformationGain(stats, node.Samples)
    }
    
    // 找到最佳和次佳特征
    best, secondBest := ht.findTopTwo(gains)
    
    if best == "" {
        return
    }
    
    // Hoeffding界
    R := math.Log2(float64(len(ht.getClasses())))
    epsilon := math.Sqrt((R * R * math.Log(1.0/ht.delta)) / (2.0 * float64(node.Samples)))
    
    // 判断是否分裂
    if gains[best]-gains[secondBest] > epsilon || epsilon < ht.tau {
        ht.split(node, best)
    }
}

func (ht *HoeffdingTree) split(node *HoeffdingNode, feature string) {
    node.IsLeaf = false
    node.Feature = feature
    node.Children = make(map[string]*DiagnosticNode)
    
    // 为每个特征值创建子节点
    stats := node.statistics[feature]
    
    for value := range stats.ValueCounts {
        child := &HoeffdingNode{
            DiagnosticNode: &DiagnosticNode{
                IsLeaf:   true,
                Children: make(map[string]*DiagnosticNode),
            },
            statistics: make(map[string]*FeatureStatistics),
        }
        
        node.Children[value] = child.DiagnosticNode
    }
}
```

---

## 7.3.4 诊断路径优化

### 剪枝策略

**预剪枝和后剪枝**：

```go
// 决策树剪枝器
type TreePruner struct {
    validationData []DiagnosticCase
}

// 后剪枝（REP - Reduced Error Pruning）
func (tp *TreePruner) ReducedErrorPruning(
    tree *DiagnosticTree,
) *DiagnosticTree {
    // 自底向上剪枝
    tp.pruneNode(tree.Root)
    return tree
}

func (tp *TreePruner) pruneNode(node *DiagnosticNode) {
    if node.IsLeaf {
        return
    }
    
    // 递归剪枝子树
    for _, child := range node.Children {
        tp.pruneNode(child)
    }
    
    // 尝试剪枝当前节点
    originalAccuracy := tp.calculateAccuracy(node)
    
    // 临时转换为叶节点
    originalIsLeaf := node.IsLeaf
    originalChildren := node.Children
    
    node.IsLeaf = true
    node.Children = nil
    
    prunedAccuracy := tp.calculateAccuracy(node)
    
    // 如果剪枝后准确率不降低，保持剪枝
    if prunedAccuracy < originalAccuracy {
        // 恢复
        node.IsLeaf = originalIsLeaf
        node.Children = originalChildren
    }
}

func (tp *TreePruner) calculateAccuracy(node *DiagnosticNode) float64 {
    correct := 0
    
    for _, case_ := range tp.validationData {
        prediction := tp.predict(node, case_)
        if prediction == case_.Diagnosis {
            correct++
        }
    }
    
    return float64(correct) / float64(len(tp.validationData))
}

// 代价复杂度剪枝（CCP - Cost Complexity Pruning）
func (tp *TreePruner) CostComplexityPruning(
    tree *DiagnosticTree,
    alpha float64,
) *DiagnosticTree {
    // 计算每个节点的代价复杂度
    tp.calculateCostComplexity(tree.Root, alpha)
    
    // 剪枝代价复杂度最小的节点
    tp.pruneMinCostNode(tree.Root)
    
    return tree
}

func (tp *TreePruner) calculateCostComplexity(
    node *DiagnosticNode,
    alpha float64,
) float64 {
    if node.IsLeaf {
        return tp.calculateError(node)
    }
    
    // 子树误差
    subtreeError := 0.0
    leafCount := 0
    
    for _, child := range node.Children {
        subtreeError += tp.calculateCostComplexity(child, alpha)
        leafCount += tp.countLeaves(child)
    }
    
    // 代价复杂度 = 误差 + α * 叶节点数
    return subtreeError + alpha*float64(leafCount)
}

func (tp *TreePruner) calculateError(node *DiagnosticNode) float64 {
    errors := 0
    
    for _, case_ := range tp.validationData {
        if tp.belongsToNode(case_, node) {
            if node.Diagnosis != case_.Diagnosis {
                errors++
            }
        }
    }
    
    return float64(errors)
}
```

### 成本敏感学习

**考虑诊断成本**：

```go
// 成本敏感决策树
type CostSensitiveTree struct {
    tree          *DiagnosticTree
    testCosts     map[string]float64 // 特征测试成本
    misclassCosts map[string]map[string]float64 // 误分类成本
}

func (cst *CostSensitiveTree) Build(
    data []DiagnosticCase,
    features []string,
) *DiagnosticNode {
    return cst.buildWithCost(data, features, 0, 0.0)
}

func (cst *CostSensitiveTree) buildWithCost(
    data []DiagnosticCase,
    features []string,
    depth int,
    accumulatedCost float64,
) *DiagnosticNode {
    // 终止条件
    if len(data) < 10 || depth > 10 {
        return cst.createCostSensitiveLeaf(data)
    }
    
    // 选择成本效益最优的特征
    bestFeature := cst.selectBestFeatureByCost(data, features, accumulatedCost)
    
    if bestFeature == "" {
        return cst.createCostSensitiveLeaf(data)
    }
    
    // 创建节点
    node := &DiagnosticNode{
        Feature:  bestFeature,
        IsLeaf:   false,
        Children: make(map[string]*DiagnosticNode),
    }
    
    // 分割数据
    subsets := cst.splitByFeature(data, bestFeature)
    remainingFeatures := cst.removeFeature(features, bestFeature)
    newCost := accumulatedCost + cst.testCosts[bestFeature]
    
    for value, subset := range subsets {
        node.Children[value] = cst.buildWithCost(
            subset,
            remainingFeatures,
            depth+1,
            newCost,
        )
    }
    
    return node
}

func (cst *CostSensitiveTree) selectBestFeatureByCost(
    data []DiagnosticCase,
    features []string,
    currentCost float64,
) string {
    minCostBenefit := math.Inf(1)
    bestFeature := ""
    
    for _, feature := range features {
        // 信息增益
        gain := cst.calculateInformationGain(data, feature)
        
        // 测试成本
        testCost := cst.testCosts[feature]
        
        // 成本效益比
        costBenefit := (testCost + currentCost) / (gain + 1e-10)
        
        if costBenefit < minCostBenefit {
            minCostBenefit = costBenefit
            bestFeature = feature
        }
    }
    
    return bestFeature
}

func (cst *CostSensitiveTree) createCostSensitiveLeaf(
    data []DiagnosticCase,
) *DiagnosticNode {
    // 选择期望误分类成本最小的类别
    minCost := math.Inf(1)
    bestDiagnosis := ""
    
    classes := cst.getClasses(data)
    
    for _, class := range classes {
        expectedCost := cst.calculateExpectedMisclassCost(data, class)
        
        if expectedCost < minCost {
            minCost = expectedCost
            bestDiagnosis = class
        }
    }
    
    return &DiagnosticNode{
        IsLeaf:     true,
        Diagnosis:  bestDiagnosis,
        Confidence: 1.0 - minCost,
        Samples:    len(data),
    }
}

func (cst *CostSensitiveTree) calculateExpectedMisclassCost(
    data []DiagnosticCase,
    predictedClass string,
) float64 {
    totalCost := 0.0
    
    for _, case_ := range data {
        if case_.Diagnosis != predictedClass {
            cost := cst.misclassCosts[case_.Diagnosis][predictedClass]
            totalCost += cost
        }
    }
    
    return totalCost / float64(len(data))
}
```

---

## 总结

诊断决策树核心技术：

**决策树构建**：

- ID3：信息增益
- C4.5：增益率
- CART：基尼指数
- 处理连续/离散特征

**规则生成**：

- 路径提取
- 规则简化
- 冗余消除
- 可读性优化

**动态更新**：

- 在线学习
- 增量更新
- Hoeffding树
- 自适应分裂

**路径优化**：

- 预剪枝/后剪枝
- REP/CCP剪枝
- 成本敏感学习
- 测试成本优化

**最佳实践**：

- 特征工程
- 交叉验证
- 集成方法
- 可解释性
- 持续优化

---

**上一篇**: [21_根因分析算法.md](21_根因分析算法.md)  
**下一篇**: [23_形式化验证框架.md](23_形式化验证框架.md)

---

*最后更新: 2025年10月7日*-
