# 日志分析与关联

**文档版本**: 1.0.0  
**创建日期**: 2025年10月7日  
**所属**: 第三部分 - 排错与故障定位  

---

## 目录

- [日志分析与关联](#日志分析与关联)
  - [目录](#目录)
  - [概述](#概述)
  - [3.2.1 日志模式挖掘](#321-日志模式挖掘)
    - [Drain日志解析算法](#drain日志解析算法)
    - [日志序列模式挖掘](#日志序列模式挖掘)
  - [3.2.2 Trace-Log关联](#322-trace-log关联)
    - [基于时间窗口的关联](#基于时间窗口的关联)
    - [基于因果关系的关联](#基于因果关系的关联)
  - [总结](#总结)

---

## 概述

本文档介绍日志模式挖掘和Trace-Log关联技术，实现日志与追踪数据的深度融合。

---

## 3.2.1 日志模式挖掘

### Drain日志解析算法

**算法实现**：

```go
// Drain日志解析算法
type DrainParser struct {
    rootNode    *LogClusterNode
    maxDepth    int
    simThreshold float64
    logClusters map[int]*LogCluster
}

type LogClusterNode struct {
    children map[string]*LogClusterNode
    clusters []*LogCluster
}

type LogCluster struct {
    ID       int
    Template []string  // 日志模板
    Logs     []string  // 原始日志
    Count    int
}

func (dp *DrainParser) Parse(log string) *LogCluster {
    // 1. 预处理：分词
    tokens := dp.tokenize(log)
    
    // 2. 搜索树遍历
    node := dp.rootNode
    depth := 0
    
    for depth < dp.maxDepth && depth < len(tokens) {
        token := tokens[depth]
        
        // 如果是数字或特殊字符，使用通配符
        if dp.isWildcard(token) {
            token = "*"
        }
        
        if child, ok := node.children[token]; ok {
            node = child
        } else {
            // 创建新节点
            newNode := &LogClusterNode{
                children: make(map[string]*LogClusterNode),
            }
            node.children[token] = newNode
            node = newNode
        }
        depth++
    }
    
    // 3. 在叶节点中查找最相似的cluster
    bestCluster := dp.findBestCluster(node, tokens)
    
    if bestCluster != nil {
        // 更新现有cluster
        bestCluster.Logs = append(bestCluster.Logs, log)
        bestCluster.Count++
        dp.updateTemplate(bestCluster, tokens)
    } else {
        // 创建新cluster
        newCluster := &LogCluster{
            ID:       len(dp.logClusters),
            Template: tokens,
            Logs:     []string{log},
            Count:    1,
        }
        node.clusters = append(node.clusters, newCluster)
        dp.logClusters[newCluster.ID] = newCluster
        bestCluster = newCluster
    }
    
    return bestCluster
}

func (dp *DrainParser) findBestCluster(
    node *LogClusterNode, 
    tokens []string,
) *LogCluster {
    maxSim := 0.0
    var bestCluster *LogCluster
    
    for _, cluster := range node.clusters {
        sim := dp.similarity(tokens, cluster.Template)
        if sim > maxSim && sim >= dp.simThreshold {
            maxSim = sim
            bestCluster = cluster
        }
    }
    
    return bestCluster
}

func (dp *DrainParser) similarity(tokens1, tokens2 []string) float64 {
    if len(tokens1) != len(tokens2) {
        return 0.0
    }
    
    matches := 0
    for i := range tokens1 {
        if tokens1[i] == tokens2[i] || tokens2[i] == "*" {
            matches++
        }
    }
    
    return float64(matches) / float64(len(tokens1))
}
```

**示例**：

```text
原始日志：
  "User 123 logged in from 192.168.1.1"
  "User 456 logged in from 10.0.0.5"
  "User 789 logged in from 172.16.0.10"

提取模板：
  "User * logged in from *"
  
聚类结果：
  Cluster ID: 1
  Template: ["User", "*", "logged", "in", "from", "*"]
  Count: 3
```

### 日志序列模式挖掘

**PrefixSpan算法**：

```go
// PrefixSpan算法挖掘频繁日志序列
type PrefixSpan struct {
    minSupport int
    patterns   []LogPattern
}

type LogPattern struct {
    Sequence []int  // cluster ID序列
    Support  int    // 支持度
}

func (ps *PrefixSpan) Mine(sequences [][]int) []LogPattern {
    // 1. 找到所有频繁1-序列
    freq1 := ps.findFrequent1Sequences(sequences)
    
    // 2. 递归挖掘
    patterns := []LogPattern{}
    for _, item := range freq1 {
        prefix := []int{item}
        ps.mineRecursive(sequences, prefix, &patterns)
    }
    
    return patterns
}

func (ps *PrefixSpan) mineRecursive(
    sequences [][]int, 
    prefix []int, 
    patterns *[]LogPattern,
) {
    // 1. 构建投影数据库
    projectedDB := ps.projectDatabase(sequences, prefix)
    
    // 2. 找到频繁项
    freqItems := ps.findFrequentItems(projectedDB)
    
    // 3. 对每个频繁项递归
    for _, item := range freqItems {
        newPrefix := append(prefix, item)
        
        // 添加到模式集合
        *patterns = append(*patterns, LogPattern{
            Sequence: newPrefix,
            Support:  ps.countSupport(sequences, newPrefix),
        })
        
        // 递归挖掘
        ps.mineRecursive(sequences, newPrefix, patterns)
    }
}
```

**应用示例**：

```text
日志序列（Cluster ID）：
  Seq1: [1, 2, 3, 4]
  Seq2: [1, 2, 4]
  Seq3: [1, 3, 4]
  Seq4: [1, 2, 3, 4]

挖掘结果（minSupport=2）：
  Pattern: [1, 2] (Support: 3)
  Pattern: [1, 2, 4] (Support: 3)
  Pattern: [1, 2, 3, 4] (Support: 2)
  Pattern: [1, 3, 4] (Support: 3)
```

---

## 3.2.2 Trace-Log关联

### 基于时间窗口的关联

**关联算法**：

```go
// Trace和Log关联器
type TraceLogCorrelator struct {
    timeWindow time.Duration
    traceIndex map[string]*Trace
    logBuffer  *TimeSeriesBuffer
}

func (tlc *TraceLogCorrelator) Correlate(trace *Trace) []LogEntry {
    correlatedLogs := []LogEntry{}
    
    // 1. 获取Trace的时间范围
    startTime := trace.StartTime
    endTime := trace.EndTime
    
    // 2. 扩展时间窗口
    searchStart := startTime.Add(-tlc.timeWindow)
    searchEnd := endTime.Add(tlc.timeWindow)
    
    // 3. 查询时间窗口内的日志
    logs := tlc.logBuffer.Query(searchStart, searchEnd)
    
    // 4. 精确匹配
    for _, log := range logs {
        if tlc.isRelated(trace, log) {
            correlatedLogs = append(correlatedLogs, log)
        }
    }
    
    return correlatedLogs
}

func (tlc *TraceLogCorrelator) isRelated(
    trace *Trace, 
    log LogEntry,
) bool {
    // 1. TraceID匹配
    if log.TraceID == trace.TraceID {
        return true
    }
    
    // 2. 服务名匹配
    for _, span := range trace.Spans {
        if span.ServiceName == log.ServiceName {
            // 3. 时间重叠检查
            if log.Timestamp.After(span.StartTime) && 
               log.Timestamp.Before(span.EndTime) {
                return true
            }
        }
    }
    
    // 4. 关键字匹配
    for _, span := range trace.Spans {
        if strings.Contains(log.Message, span.OperationName) {
            return true
        }
    }
    
    return false
}
```

### 基于因果关系的关联

**因果关联分析**：

```go
// 因果关联分析
type CausalCorrelator struct {
    causalGraph *CausalGraph
    timeWindow  time.Duration
}

func (cc *CausalCorrelator) AnalyzeCausality(
    trace *Trace, 
    logs []LogEntry,
) *CausalRelation {
    relation := &CausalRelation{
        TraceID: trace.TraceID,
        Edges:   []CausalEdge{},
    }
    
    // 1. 构建Span-Log时间序列
    events := cc.buildEventTimeline(trace, logs)
    
    // 2. 分析因果关系
    for i := 0; i < len(events)-1; i++ {
        for j := i + 1; j < len(events); j++ {
            if cc.hasCausalRelation(events[i], events[j]) {
                edge := CausalEdge{
                    From:     events[i].ID,
                    To:       events[j].ID,
                    Strength: cc.calculateStrength(events[i], events[j]),
                }
                relation.Edges = append(relation.Edges, edge)
            }
        }
    }
    
    return relation
}

func (cc *CausalCorrelator) hasCausalRelation(
    e1, e2 Event,
) bool {
    // 1. 时间顺序检查
    if !e1.Timestamp.Before(e2.Timestamp) {
        return false
    }
    
    // 2. 时间窗口检查
    if e2.Timestamp.Sub(e1.Timestamp) > cc.timeWindow {
        return false
    }
    
    // 3. 语义关联检查
    return cc.hasSemanticRelation(e1, e2)
}

func (cc *CausalCorrelator) hasSemanticRelation(
    e1, e2 Event,
) bool {
    // 检查是否有共同的实体引用
    entities1 := cc.extractEntities(e1)
    entities2 := cc.extractEntities(e2)
    
    for entity := range entities1 {
        if entities2[entity] {
            return true
        }
    }
    
    return false
}
```

**关联结果可视化**：

```text
Trace-Log因果图：

Span1 (API Gateway) ──┐
  ├─ Log: "Received request"
  └─ Log: "Routing to service-a"
                        │
                        ↓
Span2 (Service A) ─────┤
  ├─ Log: "Processing request"
  ├─ Log: "Querying database"
  └─ Log: "ERROR: Connection timeout"
                        │
                        ↓
Span3 (Database) ──────┘
  └─ Log: "Connection pool exhausted"
  
根因: Database connection pool exhausted
```

---

## 总结

日志分析与关联核心技术：

**日志模式挖掘**：

- Drain算法：模板提取
- PrefixSpan：序列模式挖掘
- 自动聚类：减少日志量

**Trace-Log关联**：

- 时间窗口匹配
- TraceID关联
- 因果关系分析
- 语义关联

**应用价值**：

- 快速定位问题
- 理解系统行为
- 发现异常模式
- 辅助根因分析

**最佳实践**：

- 结构化日志
- 注入TraceID
- 合理采样
- 实时关联

---

**上一篇**: [08_分布式追踪与问题定位.md](08_分布式追踪与问题定位.md)  
**下一篇**: [10_性能瓶颈分析.md](10_性能瓶颈分析.md)

---

*最后更新: 2025年10月7日*:
