# 📊 OTLP数据生命周期可视化图表

> **创建日期**: 2025年10月20日  
> **目的**: 为数据模型与语义转换指南提供配套可视化  
> **包含**: 4大核心流程图 + 多个辅助图表

---

## 🎯 图表概览

本文档提供OTLP数据模型与生命周期的完整可视化图表，配套[数据模型与语义转换完整指南](📊_数据模型与语义转换完整指南_2025_10_20.md)使用。

**核心图表**:

1. 数据生命周期完整流程图
2. OTLP三层语义模型结构图
3. Collector处理流程详细图
4. 多后端存储架构图

---

## 图表1: 数据生命周期完整流程

### 1.1 整体流程图

```mermaid
graph TB
    subgraph "阶段1: 数据收集"
        A1[用户应用<br/>自定义业务模型]
        A2[自动仪器化<br/>Auto-instrumentation]
        A3[手动埋点<br/>Manual Instrumentation]
        A1 --> A2
        A1 --> A3
    end
    
    subgraph "阶段2: SDK标准化"
        B1[OpenTelemetry SDK]
        B2[标准语义约定<br/>Semantic Conventions]
        B3[OTLP格式化<br/>Protobuf/JSON]
        A2 --> B1
        A3 --> B1
        B1 --> B2
        B2 --> B3
    end
    
    subgraph "阶段3: 数据传输"
        C1[gRPC传输<br/>HTTP/2]
        C2[HTTP/1.1传输<br/>JSON]
        C3[批处理+压缩<br/>gzip 60-70%]
        B3 --> C1
        B3 --> C2
        C1 --> C3
        C2 --> C3
    end
    
    subgraph "阶段4: Collector转换"
        D1[接收器<br/>OTLP Receiver]
        D2[处理器<br/>Processors]
        D3[导出器<br/>Exporters]
        C3 --> D1
        D1 --> D2
        D2 --> D3
    end
    
    subgraph "阶段5: 数据存储"
        E1[热数据 7天<br/>Elasticsearch]
        E2[温数据 30天<br/>ClickHouse]
        E3[冷数据 >30天<br/>S3 Glacier]
        D3 --> E1
        D3 --> E2
        D3 --> E3
    end
    
    subgraph "阶段6: 数据处理"
        F1[实时流处理<br/>Apache Flink]
        F2[批处理分析<br/>Apache Spark]
        F3[增量计算<br/>t-digest]
        E1 --> F1
        E2 --> F2
        E3 --> F2
    end
    
    subgraph "阶段7: 数据查询与可视化"
        G1[实时监控<br/>Grafana]
        G2[日志查询<br/>Kibana]
        G3[Ad-hoc分析<br/>Presto/ClickHouse]
        F1 --> G1
        F2 --> G2
        F2 --> G3
    end
    
    style A1 fill:#e1f5ff
    style B2 fill:#fff4e1
    style D2 fill:#ffe1e1
    style E2 fill:#e1ffe1
    style F1 fill:#f0e1ff
    style G1 fill:#ffe1f0
```

### 1.2 详细说明

#### 阶段1: 数据收集

**输入**: 用户应用中的业务逻辑

**处理**:

- 自动仪器化：框架级别自动收集（Flask, Spring Boot, Express等）
- 手动埋点：自定义业务属性添加

**输出**: 原始遥测数据（未标准化）

---

#### 阶段2: SDK标准化

**输入**: 原始遥测数据

**处理**:

1. OpenTelemetry SDK处理
2. 应用标准语义约定（Semantic Conventions v1.29.0）
3. 格式化为OTLP（Protobuf或JSON）

**输出**: 标准化的OTLP数据

**关键转换**:

```text
用户自定义属性:
  myapp.order.id → 保留（vendor前缀）
  
标准属性自动添加:
  http.request.method: "POST"
  http.route: "/api/orders"
  service.name: "order-service"
  deployment.environment: "production"
```

---

#### 阶段3: 数据传输

**输入**: 标准化OTLP数据

**传输协议**:

- gRPC (HTTP/2): 推荐，二进制高效
- HTTP/1.1 (JSON): 兼容性好

**优化**:

- 批处理: batch_size=512, timeout=1s
- gzip压缩: 节省60-70%带宽
- 异步发送: 不阻塞业务逻辑

**输出**: 传输到Collector的OTLP数据

---

#### 阶段4: Collector转换

**输入**: 接收的OTLP数据

**处理流程** (详见图表3):

1. **接收器**: 接收OTLP数据
2. **处理器**:
   - 资源检测（云平台属性）
   - 属性转换和清洗
   - PII脱敏和哈希
   - 尾部采样（保留异常）
   - Span Metrics生成
3. **导出器**: 路由到多个后端

**输出**: 转换后的数据，发送到存储后端

---

#### 阶段5: 数据存储

**分层存储策略**:

| 层级 | 时间范围 | 存储后端 | 特点 | 成本 |
|------|---------|---------|------|------|
| **热数据** | 最近7天 | Elasticsearch | 高性能查询，全文搜索 | 高 |
| **温数据** | 7-30天 | ClickHouse | 列式存储，OLAP分析 | 中 |
| **冷数据** | >30天 | S3 Glacier | 归档存储，合规保留 | 低 |

**容量规划**:

```text
示例（10M spans/天）:
├─ 热数据: 70M spans × 2KB = 140GB
├─ 温数据: 230M spans × 0.5KB (压缩) = 115GB
└─ 冷数据: 1B+ spans × 0.1KB (深度压缩) = 100GB
总计: 355GB (vs 原始2TB，节省82%)
```

---

#### 阶段6: 数据处理

**实时流处理 (Apache Flink)**:

- 延迟: <1秒
- 吞吐: 100K+ events/s
- 用途: 实时告警、异常检测

**批处理分析 (Apache Spark)**:

- 延迟: 分钟级-小时级
- 吞吐: TB级数据
- 用途: 趋势分析、报表生成

**增量计算**:

- t-digest: P99延迟计算
- HyperLogLog: 基数估计
- Count-Min Sketch: 频率统计

---

#### 阶段7: 数据查询与可视化

**实时监控 (Grafana)**:

- 数据源: Prometheus + Tempo
- 更新频率: 1-15秒
- 用途: 运维监控

**日志查询 (Kibana)**:

- 数据源: Elasticsearch
- 功能: 全文搜索、日志到Trace跳转

**Ad-hoc分析 (Presto/ClickHouse)**:

- 交互式SQL查询
- 多维度聚合分析

---

## 图表2: OTLP三层语义模型结构图

```mermaid
graph TB
    subgraph "第一层: Resource Model 资源模型"
        R1[service.name<br/>service.version<br/>service.namespace]
        R2[deployment.environment<br/>service.instance.id]
        R3[telemetry.sdk.name<br/>telemetry.sdk.version]
        R4[平台属性<br/>host.* / container.* / k8s.* / cloud.*]
        
        R1 --> RM[Resource 对象]
        R2 --> RM
        R3 --> RM
        R4 --> RM
    end
    
    subgraph "第二层: Instrumentation Scope 仪器化作用域"
        IS1[name: string<br/>library/instrumentation名称]
        IS2[version: string<br/>库版本号]
        IS3[schema_url: string<br/>语义约定版本URL]
        IS4[attributes: Map<br/>作用域级别属性]
        
        IS1 --> ISM[InstrumentationScope 对象]
        IS2 --> ISM
        IS3 --> ISM
        IS4 --> ISM
    end
    
    subgraph "第三层: Telemetry Data Model 遥测数据模型"
        subgraph "Trace Model"
            T1[Span对象]
            T2[trace_id<br/>span_id<br/>parent_span_id]
            T3[name<br/>kind<br/>start_time<br/>end_time]
            T4[attributes<br/>events<br/>links<br/>status]
            T1 --> T2
            T1 --> T3
            T1 --> T4
        end
        
        subgraph "Metric Model"
            M1[Metric对象]
            M2[name<br/>description<br/>unit]
            M3[type: Sum/Gauge/<br/>Histogram/<br/>ExponentialHistogram]
            M4[data_points:<br/>value<br/>attributes<br/>timestamp]
            M1 --> M2
            M1 --> M3
            M1 --> M4
        end
        
        subgraph "Log Model"
            L1[LogRecord对象]
            L2[timestamp<br/>observed_timestamp]
            L3[severity_number<br/>severity_text]
            L4[body<br/>attributes<br/>trace_id<br/>span_id]
            L1 --> L2
            L1 --> L3
            L1 --> L4
        end
    end
    
    RM -.一个Resource包含多个.-> ISM
    ISM -.一个Scope产生多个.-> T1
    ISM -.一个Scope产生多个.-> M1
    ISM -.一个Scope产生多个.-> L1
    
    style RM fill:#e3f2fd
    style ISM fill:#fff3e0
    style T1 fill:#e8f5e9
    style M1 fill:#fce4ec
    style L1 fill:#f3e5f5
```

### 2.1 层次关系说明

**关系描述**:

```text
1个应用进程
  └─ 1个 Resource (进程级别，启动时确定)
       ├─ N个 InstrumentationScope (不同库/框架)
       │    ├─ Scope 1: "io.opentelemetry.api" v1.30.0
       │    │    ├─ M个 Span
       │    │    ├─ K个 Metric
       │    │    └─ P个 LogRecord
       │    │
       │    ├─ Scope 2: "com.mycompany.custom" v1.0.0
       │    │    ├─ ...
       │    │
       │    └─ Scope N: ...
```

**生命周期**:

- **Resource**: 进程启动时创建，运行期不变
- **InstrumentationScope**: 库加载时创建，通常不变
- **Telemetry Data**: 运行时动态生成

---

## 图表3: Collector处理流程详细图

```mermaid
graph LR
    subgraph "接收器 Receivers"
        RCV1[OTLP/gRPC<br/>:4317]
        RCV2[OTLP/HTTP<br/>:4318]
        RCV3[Jaeger<br/>:14250]
        RCV4[Prometheus<br/>:8889]
    end
    
    subgraph "处理器 Processors Pipeline"
        direction TB
        P1[1. batch<br/>批处理聚合]
        P2[2. resourcedetection<br/>云平台属性检测]
        P3[3. attributes<br/>属性转换/删除/哈希]
        P4[4. resource<br/>资源属性合并]
        P5[5. filter<br/>数据过滤/采样]
        P6[6. spanmetrics<br/>从Span生成Metric]
        P7[7. tail_sampling<br/>尾部采样决策]
        
        P1 --> P2
        P2 --> P3
        P3 --> P4
        P4 --> P5
        P5 --> P6
        P6 --> P7
    end
    
    subgraph "导出器 Exporters"
        EXP1[OTLP/Jaeger<br/>Trace后端]
        EXP2[Prometheus<br/>Metric后端]
        EXP3[Elasticsearch<br/>Log后端]
        EXP4[Kafka<br/>消息队列]
        EXP5[S3<br/>归档存储]
    end
    
    RCV1 --> P1
    RCV2 --> P1
    RCV3 --> P1
    RCV4 --> P1
    
    P7 --> EXP1
    P7 --> EXP2
    P7 --> EXP3
    P7 --> EXP4
    P7 --> EXP5
    
    style P1 fill:#e3f2fd
    style P3 fill:#ffebee
    style P7 fill:#f3e5f5
```

### 3.1 处理器详细说明

#### 处理器1: batch (批处理)

**配置**:

```yaml
batch:
  timeout: 1s
  send_batch_size: 512
  send_batch_max_size: 1024
```

**功能**:

- 聚合多个span/metric/log为batch
- 减少网络调用次数
- 提升吞吐量50-100%

---

#### 处理器2: resourcedetection (资源检测)

**配置**:

```yaml
resourcedetection:
  detectors: [env, system, docker, ec2, ecs, gcp, azure]
  timeout: 5s
  override: false
```

**自动添加属性**:

```text
AWS EC2:
  cloud.provider: "aws"
  cloud.platform: "aws_ec2"
  cloud.region: "us-east-1"
  cloud.account.id: "123456789012"
  host.id: "i-0123456789abcdef0"

Kubernetes:
  k8s.cluster.name: "prod-cluster"
  k8s.namespace.name: "default"
  k8s.pod.name: "order-service-7d4f8b9c-xk2lp"
  k8s.deployment.name: "order-service"
  container.id: "abc123..."
```

---

#### 处理器3: attributes (属性处理)

**配置示例**:

```yaml
attributes:
  actions:
    # 重命名（向后兼容）
    - key: http.method
      action: update
      from_attribute: http.request.method
    
    # 删除敏感数据
    - key: http.request.header.authorization
      action: delete
    
    - key: http.request.header.cookie
      action: delete
    
    # PII哈希化
    - key: user.email
      action: hash
    
    - key: user.phone
      action: hash
    
    # 添加固定标签
    - key: deployment.environment
      value: "production"
      action: insert
    
    # 正则提取
    - key: http.target
      pattern: ^/api/v(\d+)/.*
      action: extract
      to_attribute: api.version
```

---

#### 处理器7: tail_sampling (尾部采样)

**配置示例**:

```yaml
tail_sampling:
  decision_wait: 10s
  num_traces: 100000
  expected_new_traces_per_sec: 1000
  
  policies:
    # 策略1: 所有错误trace
    - name: error-policy
      type: status_code
      status_code:
        status_codes: [ERROR]
    
    # 策略2: 慢trace (>2秒)
    - name: slow-trace-policy
      type: latency
      latency:
        threshold_ms: 2000
    
    # 策略3: VIP用户 (100%采样)
    - name: vip-user-policy
      type: attribute
      attribute:
        key: myshop.user.tier
        values: [platinum, gold]
    
    # 策略4: 包含特定span名称
    - name: important-operation-policy
      type: span_name
      span_name:
        name_patterns: ["payment", "checkout"]
    
    # 策略5: 普通流量 (1%概率采样)
    - name: general-traffic-policy
      type: probabilistic
      probabilistic:
        sampling_percentage: 1
```

**采样效果**:

```text
输入: 10M traces/天
├─ 错误trace (1%): 100K → 100% 保留 = 100K
├─ 慢trace (0.5%): 50K → 100% 保留 = 50K
├─ VIP用户 (5%): 500K → 100% 保留 = 500K
├─ 重要操作 (2%): 200K → 100% 保留 = 200K
└─ 普通流量 (91.5%): 9.15M → 1% 采样 = 91.5K

输出: 941.5K traces/天 (仅9.4%，但包含99.9%关键信息)
存储成本: $1M/月 → $94K/月 (节省90.6%)
```

---

## 图表4: 多后端存储架构图

```mermaid
graph TB
    subgraph "OTLP Collector"
        COL[Collector<br/>统一入口]
    end
    
    subgraph "Trace存储"
        T1[Jaeger<br/>分布式追踪UI]
        T2[Tempo<br/>大规模Trace存储]
        T3[Zipkin<br/>兼容后端]
    end
    
    subgraph "Metric存储"
        M1[Prometheus<br/>时序数据库]
        M2[VictoriaMetrics<br/>高性能TSDB]
        M3[Thanos<br/>长期存储]
    end
    
    subgraph "Log存储"
        L1[Elasticsearch<br/>全文搜索]
        L2[Loki<br/>轻量级日志]
        L3[Splunk<br/>企业级SIEM]
    end
    
    subgraph "统一存储"
        U1[ClickHouse<br/>OLAP分析]
        U2[PostgreSQL<br/>关系型查询]
        U3[S3/GCS<br/>对象存储归档]
    end
    
    subgraph "消息队列"
        Q1[Kafka<br/>数据缓冲]
        Q2[Redis<br/>快速缓存]
    end
    
    COL -->|traces| T1
    COL -->|traces| T2
    COL -->|traces| T3
    
    COL -->|metrics| M1
    COL -->|metrics| M2
    COL -->|metrics| M3
    
    COL -->|logs| L1
    COL -->|logs| L2
    COL -->|logs| L3
    
    COL -->|all signals| U1
    COL -->|all signals| U2
    COL -->|all signals| U3
    
    COL -->|buffer| Q1
    COL -->|cache| Q2
    
    T2 -.长期存储.-> U3
    M3 -.长期存储.-> U3
    L2 -.长期存储.-> U3
    
    Q1 -.延迟处理.-> U1
    
    style COL fill:#e3f2fd
    style T2 fill:#e8f5e9
    style M1 fill:#fff3e0
    style L1 fill:#fce4ec
    style U1 fill:#f3e5f5
```

### 4.1 后端选择建议

#### Trace后端对比

| 后端 | 优势 | 劣势 | 适用场景 |
|------|------|------|---------|
| **Jaeger** | UI优秀，社区活跃 | 存储扩展性有限 | 中小规模（<1M traces/天） |
| **Tempo** | 成本低，S3存储 | 查询功能较弱 | 大规模（>10M traces/天） |
| **Zipkin** | 轻量级，易部署 | 功能相对简单 | 开发测试环境 |

#### Metric后端对比

| 后端 | 优势 | 劣势 | 适用场景 |
|------|------|------|---------|
| **Prometheus** | 生态最成熟 | 单机扩展性有限 | 标准监控（<10M series） |
| **VictoriaMetrics** | 高性能，低成本 | 社区较小 | 大规模（>100M series） |
| **Thanos** | 长期存储 | 复杂度较高 | 多集群、长期趋势分析 |

#### Log后端对比

| 后端 | 优势 | 劣势 | 适用场景 |
|------|------|------|---------|
| **Elasticsearch** | 全文搜索强大 | 成本较高 | 需要复杂查询（<100GB/天） |
| **Loki** | 成本低，与Grafana集成 | 查询功能有限 | 日志量大，查询简单 |
| **Splunk** | 企业级功能全面 | 价格昂贵 | 大型企业，合规要求高 |

---

## 图表5: 数据模型对应关系图

```mermaid
graph TB
    subgraph "用户自定义数据模型"
        U1[电商订单模型]
        U2[金融交易模型]
        U3[IoT设备模型]
        
        U1 --> U1A[order_id<br/>status<br/>total_amount<br/>user_tier]
        U2 --> U2A[transaction_id<br/>amount<br/>risk_score<br/>fraud_detected]
        U3 --> U3A[device_id<br/>sensor_data<br/>location<br/>battery_level]
    end
    
    subgraph "语义约定映射"
        S1[标准属性<br/>Semantic Conventions]
        S2[vendor前缀<br/>自定义属性]
        
        S1 --> S1A[http.request.method<br/>http.response.status_code<br/>service.name<br/>db.system]
        S2 --> S2A[mycompany.order.id<br/>fintech.transaction.id<br/>iot.device.id]
    end
    
    subgraph "OTLP三层模型"
        O1[Resource]
        O2[InstrumentationScope]
        O3[Span Attributes]
        
        O1 --> O1A[service.name<br/>deployment.environment<br/>host.name]
        O2 --> O2A[name<br/>version<br/>schema_url]
        O3 --> O3A[标准属性 +<br/>自定义属性]
    end
    
    U1A -.映射.-> S2A
    U2A -.映射.-> S2A
    U3A -.映射.-> S2A
    
    S1A -.应用.-> O3A
    S2A -.应用.-> O3A
    
    style U1 fill:#e1f5ff
    style S1 fill:#fff4e1
    style O3 fill:#e1ffe1
```

---

## 图表6: Collector部署架构图

### 6.1 基础架构（单集群）

```mermaid
graph TB
    subgraph "应用层"
        APP1[Service A]
        APP2[Service B]
        APP3[Service C]
    end
    
    subgraph "Agent Collector DaemonSet"
        AG1[Agent on Node 1]
        AG2[Agent on Node 2]
        AG3[Agent on Node 3]
    end
    
    subgraph "Gateway Collector Deployment"
        GW1[Gateway 1]
        GW2[Gateway 2]
        GW3[Gateway 3]
        LB[Load Balancer]
    end
    
    subgraph "后端存储"
        BE1[Jaeger]
        BE2[Prometheus]
        BE3[Elasticsearch]
    end
    
    APP1 --> AG1
    APP2 --> AG2
    APP3 --> AG3
    
    AG1 --> LB
    AG2 --> LB
    AG3 --> LB
    
    LB --> GW1
    LB --> GW2
    LB --> GW3
    
    GW1 --> BE1
    GW1 --> BE2
    GW1 --> BE3
    GW2 --> BE1
    GW2 --> BE2
    GW2 --> BE3
    GW3 --> BE1
    GW3 --> BE2
    GW3 --> BE3
    
    style AG1 fill:#e3f2fd
    style GW2 fill:#fff3e0
    style BE2 fill:#e8f5e9
```

### 6.2 混合云架构

```mermaid
graph TB
    subgraph "AWS集群"
        AWS_APP[AWS Services]
        AWS_AG[Agent Collectors]
        AWS_GW[Gateway Collectors]
    end
    
    subgraph "Azure集群"
        AZ_APP[Azure Services]
        AZ_AG[Agent Collectors]
        AZ_GW[Gateway Collectors]
    end
    
    subgraph "GCP集群"
        GCP_APP[GCP Services]
        GCP_AG[Agent Collectors]
        GCP_GW[Gateway Collectors]
    end
    
    subgraph "中心化存储"
        CENTRAL[Central Collector<br/>Kafka缓冲]
        TEMPO[Tempo<br/>S3存储]
        THANOS[Thanos<br/>长期Metric]
        ES[Elasticsearch<br/>中心化Log]
    end
    
    AWS_APP --> AWS_AG
    AWS_AG --> AWS_GW
    
    AZ_APP --> AZ_AG
    AZ_AG --> AZ_GW
    
    GCP_APP --> GCP_AG
    GCP_AG --> GCP_GW
    
    AWS_GW --> CENTRAL
    AZ_GW --> CENTRAL
    GCP_GW --> CENTRAL
    
    CENTRAL --> TEMPO
    CENTRAL --> THANOS
    CENTRAL --> ES
    
    style CENTRAL fill:#f3e5f5
```

---

## 图表7: 成熟案例架构图

### 7.1 Uber架构

```mermaid
graph TB
    subgraph "移动端"
        M1[Rider App<br/>100M+ DAU]
        M2[Driver App<br/>10M+ DAU]
    end
    
    subgraph "后端服务"
        SVC[2000+ 微服务<br/>500K+ QPS]
    end
    
    subgraph "Regional Collector"
        R1[美洲区域]
        R2[欧洲区域]
        R3[亚太区域]
    end
    
    subgraph "Central Pipeline"
        CP1[Head-based采样<br/>1%保留]
        CP2[Tail-based采样<br/>异常100%保留]
        CP3[Span→Metric转换]
    end
    
    subgraph "多层存储"
        ST1[热数据 7天<br/>Cassandra]
        ST2[温数据 30天<br/>HDFS+Parquet]
        ST3[冷数据 >30天<br/>S3 Glacier]
    end
    
    subgraph "查询层"
        Q1[实时监控<br/>Grafana]
        Q2[Ad-hoc查询<br/>Presto]
        Q3[ML训练<br/>Spark MLlib]
    end
    
    M1 --> R1
    M2 --> R1
    SVC --> R1
    SVC --> R2
    SVC --> R3
    
    R1 --> CP1
    R2 --> CP1
    R3 --> CP1
    
    CP1 --> CP2
    CP2 --> CP3
    
    CP3 --> ST1
    CP3 --> ST2
    CP3 --> ST3
    
    ST1 --> Q1
    ST2 --> Q2
    ST3 --> Q3
    
    style CP2 fill:#ffebee
    style ST1 fill:#e8f5e9
```

**关键数据**:

- 每天采集: 50亿+ Spans
- 采样后保留: 5千万 Spans (1%)
- 但保留了: 99.9%的错误和异常
- 存储成本: 从$500K/月降至$5K/月

---

## 使用指南

### 如何使用这些图表

1. **学习理解**:
   - 图表1: 理解整个数据生命周期
   - 图表2: 理解OTLP数据模型结构
   - 图表3: 理解Collector处理逻辑

2. **架构设计**:
   - 图表4: 选择合适的后端存储
   - 图表6: 设计Collector部署架构

3. **实际应用**:
   - 图表5: 设计自定义数据模型
   - 图表7: 参考成熟案例

### 配套文档

| 文档 | 链接 | 说明 |
|------|------|------|
| **数据模型指南** | [📊_数据模型与语义转换完整指南_2025_10_20.md](📊_数据模型与语义转换完整指南_2025_10_20.md) | 详细文字说明 |
| **对标分析报告** | [📊_OTLP项目2025年10月20日全面对标分析报告.md](📊_OTLP项目2025年10月20日全面对标分析报告.md) | 标准符合性分析 |
| **推进计划** | [🚀_OTLP项目持续推进计划_2025_10_20.md](🚀_OTLP项目持续推进计划_2025_10_20.md) | 后续工作计划 |

---

## 总结

本文档提供了7个核心可视化图表，全面展示了OTLP数据模型和生命周期：

✅ **图表1**: 数据生命周期完整流程（7个阶段）  
✅ **图表2**: OTLP三层语义模型结构  
✅ **图表3**: Collector处理流程详细图  
✅ **图表4**: 多后端存储架构  
✅ **图表5**: 数据模型对应关系  
✅ **图表6**: Collector部署架构（单集群+混合云）  
✅ **图表7**: 成熟案例架构（Uber）

这些图表配合[数据模型与语义转换完整指南](📊_数据模型与语义转换完整指南_2025_10_20.md)使用，可以帮助您：

- 深入理解OTLP数据模型
- 设计合理的可观测性架构
- 参考成熟案例的最佳实践

---

**创建时间**: 2025年10月20日  
**文档作者**: OTLP项目团队  
**文档版本**: v1.0.0  
**图表数量**: 7个核心图表 + 多个辅助说明

---

**🎨 可视化让复杂的概念变得清晰！** ✨
