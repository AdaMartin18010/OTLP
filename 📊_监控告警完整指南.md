# 📊 OTLP监控告警完整指南

> **目标**: 帮助用户建立完善的OTLP监控告警体系  
> **适用场景**: 生产环境运维监控  
> **更新时间**: 2025年10月20日

---

## 📋 目录

- [📊 OTLP监控告警完整指南](#-otlp监控告警完整指南)
  - [📋 目录](#-目录)
  - [📈 监控指标](#-监控指标)
    - [1. Collector关键指标](#1-collector关键指标)
      - [Receiver指标](#receiver指标)
      - [Processor指标](#processor指标)
      - [Exporter指标](#exporter指标)
      - [系统指标](#系统指标)
    - [2. 查询指标](#2-查询指标)
  - [🔍 Prometheus配置](#-prometheus配置)
    - [1. Prometheus配置文件](#1-prometheus配置文件)
    - [2. Kubernetes ServiceMonitor](#2-kubernetes-servicemonitor)
  - [📊 Grafana Dashboard](#-grafana-dashboard)
    - [1. Collector Overview Dashboard](#1-collector-overview-dashboard)
    - [2. 导入Dashboard](#2-导入dashboard)
    - [3. 推荐的Grafana Dashboards](#3-推荐的grafana-dashboards)
  - [🚨 告警规则](#-告警规则)
    - [1. Prometheus告警规则](#1-prometheus告警规则)
    - [2. AlertManager配置](#2-alertmanager配置)
  - [📧 告警通知](#-告警通知)
    - [1. Slack集成](#1-slack集成)
    - [2. 企业微信集成](#2-企业微信集成)
    - [3. 邮件通知](#3-邮件通知)
  - [📝 日志监控](#-日志监控)
    - [1. 日志采集](#1-日志采集)
    - [2. ELK Stack集成](#2-elk-stack集成)
    - [3. 日志告警](#3-日志告警)
  - [🔍 性能分析](#-性能分析)
    - [1. Profiling](#1-profiling)
    - [2. zpages](#2-zpages)
  - [📊 监控最佳实践](#-监控最佳实践)
    - [1. 监控指标选择](#1-监控指标选择)
    - [2. 告警阈值设置](#2-告警阈值设置)
    - [3. 监控数据保留](#3-监控数据保留)
  - [🔗 相关资源](#-相关资源)

---

## 📈 监控指标

### 1. Collector关键指标

#### Receiver指标

```text
# 接收速率
otelcol_receiver_accepted_spans{receiver="otlp"}
otelcol_receiver_accepted_metric_points{receiver="otlp"}

# 拒绝速率
otelcol_receiver_refused_spans{receiver="otlp"}
otelcol_receiver_refused_metric_points{receiver="otlp"}
```

#### Processor指标

```text
# 批处理
otelcol_processor_batch_batch_send_size_sum
otelcol_processor_batch_batch_send_size_count
otelcol_processor_batch_timeout_trigger_send

# 内存限制
otelcol_processor_refused_spans{processor="memory_limiter"}
```

#### Exporter指标

```text
# 导出速率
otelcol_exporter_sent_spans{exporter="jaeger"}
otelcol_exporter_send_failed_spans{exporter="jaeger"}

# 队列大小
otelcol_exporter_queue_size{exporter="jaeger"}
otelcol_exporter_queue_capacity{exporter="jaeger"}
```

#### 系统指标

```text
# 内存使用
process_runtime_go_mem_heap_alloc_bytes
process_runtime_go_mem_heap_sys_bytes

# CPU使用
process_cpu_seconds_total

# Goroutines
process_runtime_go_goroutines
```

### 2. 查询指标

```bash
# 查看所有可用指标
curl http://localhost:8888/metrics

# 使用Prometheus查询
# 接收速率（每秒）
rate(otelcol_receiver_accepted_spans[5m])

# 导出失败率
rate(otelcol_exporter_send_failed_spans[5m]) / rate(otelcol_exporter_sent_spans[5m])

# 队列使用率
otelcol_exporter_queue_size / otelcol_exporter_queue_capacity
```

---

## 🔍 Prometheus配置

### 1. Prometheus配置文件

创建 `prometheus.yml`:

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'production'
    env: 'prod'

# 告警管理器
alerting:
  alertmanagers:
  - static_configs:
    - targets: ['alertmanager:9093']

# 告警规则
rule_files:
  - '/etc/prometheus/rules/*.yml'

scrape_configs:
  # OpenTelemetry Collector
  - job_name: 'otel-collector'
    static_configs:
    - targets: ['otel-collector:8888']
      labels:
        service: 'otel-collector'
        component: 'collector'
    
    metric_relabel_configs:
    # 只保留必要的指标
    - source_labels: [__name__]
      regex: 'otelcol_(receiver|processor|exporter)_.*'
      action: keep
  
  # Jaeger
  - job_name: 'jaeger'
    static_configs:
    - targets: ['jaeger:14269']
      labels:
        service: 'jaeger'
  
  # Prometheus自身
  - job_name: 'prometheus'
    static_configs:
    - targets: ['localhost:9090']
```

### 2. Kubernetes ServiceMonitor

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: otel-collector
  namespace: observability
  labels:
    app: otel-collector
spec:
  selector:
    matchLabels:
      app: otel-collector
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
    honorLabels: true
    relabelings:
    - sourceLabels: [__meta_kubernetes_pod_name]
      targetLabel: pod
    - sourceLabels: [__meta_kubernetes_namespace]
      targetLabel: namespace
```

---

## 📊 Grafana Dashboard

### 1. Collector Overview Dashboard

```json
{
  "dashboard": {
    "title": "OpenTelemetry Collector",
    "panels": [
      {
        "title": "Span接收速率",
        "targets": [{
          "expr": "rate(otelcol_receiver_accepted_spans[5m])",
          "legendFormat": "{{receiver}}"
        }],
        "type": "graph"
      },
      {
        "title": "Span导出速率",
        "targets": [{
          "expr": "rate(otelcol_exporter_sent_spans[5m])",
          "legendFormat": "{{exporter}}"
        }],
        "type": "graph"
      },
      {
        "title": "导出失败率",
        "targets": [{
          "expr": "rate(otelcol_exporter_send_failed_spans[5m])",
          "legendFormat": "{{exporter}}"
        }],
        "type": "graph"
      },
      {
        "title": "队列使用率",
        "targets": [{
          "expr": "otelcol_exporter_queue_size / otelcol_exporter_queue_capacity * 100",
          "legendFormat": "{{exporter}}"
        }],
        "type": "gauge"
      },
      {
        "title": "内存使用",
        "targets": [{
          "expr": "process_runtime_go_mem_heap_alloc_bytes / 1024 / 1024",
          "legendFormat": "Heap Alloc (MB)"
        }],
        "type": "graph"
      },
      {
        "title": "Goroutines",
        "targets": [{
          "expr": "process_runtime_go_goroutines",
          "legendFormat": "Goroutines"
        }],
        "type": "graph"
      }
    ]
  }
}
```

### 2. 导入Dashboard

```bash
# 使用Grafana CLI
grafana-cli plugins install grafana-piechart-panel

# 或使用API导入
curl -X POST http://admin:admin@localhost:3000/api/dashboards/db \
  -H "Content-Type: application/json" \
  -d @dashboard.json
```

### 3. 推荐的Grafana Dashboards

从 Grafana.com 导入:

```text
# OpenTelemetry Collector
Dashboard ID: 15983

# Jaeger
Dashboard ID: 10001

# Kubernetes Monitoring
Dashboard ID: 13125
```

---

## 🚨 告警规则

### 1. Prometheus告警规则

创建 `otel-alerts.yml`:

```yaml
groups:
  - name: otel-collector-alerts
    interval: 30s
    rules:
      # 高错误率
      - alert: HighExportFailureRate
        expr: |
          rate(otelcol_exporter_send_failed_spans[5m])
          /
          rate(otelcol_exporter_sent_spans[5m])
          > 0.05
        for: 5m
        labels:
          severity: critical
          component: otel-collector
        annotations:
          summary: "高导出失败率 (>5%)"
          description: "Exporter {{ $labels.exporter }} 失败率达到 {{ $value | humanizePercentage }}"
      
      # 队列接近满
      - alert: CollectorQueueAlmostFull
        expr: |
          otelcol_exporter_queue_size
          /
          otelcol_exporter_queue_capacity
          > 0.8
        for: 5m
        labels:
          severity: warning
          component: otel-collector
        annotations:
          summary: "Collector队列接近满 (>80%)"
          description: "Exporter {{ $labels.exporter }} 队列使用率: {{ $value | humanizePercentage }}"
      
      # 高内存使用
      - alert: HighMemoryUsage
        expr: |
          process_runtime_go_mem_heap_alloc_bytes
          /
          1024 / 1024 / 1024
          > 1.5
        for: 10m
        labels:
          severity: warning
          component: otel-collector
        annotations:
          summary: "高内存使用 (>1.5GB)"
          description: "Collector内存使用: {{ $value | humanize }}GB"
      
      # Collector宕机
      - alert: CollectorDown
        expr: up{job="otel-collector"} == 0
        for: 1m
        labels:
          severity: critical
          component: otel-collector
        annotations:
          summary: "Collector实例宕机"
          description: "Collector实例 {{ $labels.instance }} 已宕机"
      
      # 数据接收停止
      - alert: NoDataReceived
        expr: |
          rate(otelcol_receiver_accepted_spans[5m]) == 0
        for: 10m
        labels:
          severity: warning
          component: otel-collector
        annotations:
          summary: "没有接收到数据"
          description: "Receiver {{ $labels.receiver }} 10分钟内未接收到任何数据"
      
      # 批处理延迟高
      - alert: HighBatchProcessingDelay
        expr: |
          otelcol_processor_batch_timeout_trigger_send
          /
          otelcol_processor_batch_batch_send_size_count
          > 0.5
        for: 10m
        labels:
          severity: warning
          component: otel-collector
        annotations:
          summary: "批处理延迟高"
          description: "超过50%的批处理因超时触发"
      
      # Goroutine泄漏
      - alert: GoroutineLeak
        expr: |
          process_runtime_go_goroutines > 10000
        for: 15m
        labels:
          severity: warning
          component: otel-collector
        annotations:
          summary: "可能的Goroutine泄漏"
          description: "Goroutine数量: {{ $value }}"

  - name: jaeger-alerts
    interval: 30s
    rules:
      # Jaeger宕机
      - alert: JaegerDown
        expr: up{job="jaeger"} == 0
        for: 1m
        labels:
          severity: critical
          component: jaeger
        annotations:
          summary: "Jaeger实例宕机"
          description: "Jaeger实例 {{ $labels.instance }} 已宕机"
      
      # 存储空间不足
      - alert: LowStorageSpace
        expr: |
          jaeger_storage_free_space_bytes
          /
          jaeger_storage_total_space_bytes
          < 0.1
        for: 5m
        labels:
          severity: critical
          component: jaeger
        annotations:
          summary: "存储空间不足 (<10%)"
          description: "Jaeger存储空间仅剩 {{ $value | humanizePercentage }}"
```

### 2. AlertManager配置

创建 `alertmanager.yml`:

```yaml
global:
  resolve_timeout: 5m
  smtp_smarthost: 'smtp.example.com:587'
  smtp_from: 'alerts@example.com'
  smtp_auth_username: 'alerts@example.com'
  smtp_auth_password: 'password'

# 路由规则
route:
  group_by: ['alertname', 'component']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'
  
  routes:
  # Critical告警立即发送
  - match:
      severity: critical
    receiver: 'critical-team'
    repeat_interval: 1h
  
  # Warning告警
  - match:
      severity: warning
    receiver: 'warning-team'
    repeat_interval: 4h

# 接收器
receivers:
  - name: 'default'
    email_configs:
    - to: 'ops@example.com'
      headers:
        Subject: '[OTLP] {{ .Status | toUpper }} {{ .GroupLabels.alertname }}'
  
  - name: 'critical-team'
    email_configs:
    - to: 'critical@example.com'
    
    slack_configs:
    - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
      channel: '#alerts-critical'
      title: '{{ .GroupLabels.alertname }}'
      text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
    
    pagerduty_configs:
    - service_key: 'YOUR_PAGERDUTY_KEY'
  
  - name: 'warning-team'
    slack_configs:
    - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
      channel: '#alerts-warning'

# 抑制规则
inhibit_rules:
  # 如果Collector宕机，抑制其他相关告警
  - source_match:
      alertname: 'CollectorDown'
    target_match_re:
      alertname: '.*'
    equal: ['instance']
```

---

## 📧 告警通知

### 1. Slack集成

```yaml
receivers:
  - name: 'slack-notifications'
    slack_configs:
    - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
      channel: '#otlp-alerts'
      title: '{{ .GroupLabels.alertname }}'
      text: |-
        {{ range .Alerts }}
        *Alert:* {{ .Labels.alertname }}
        *Severity:* {{ .Labels.severity }}
        *Summary:* {{ .Annotations.summary }}
        *Description:* {{ .Annotations.description }}
        {{ end }}
      color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'
```

### 2. 企业微信集成

```yaml
receivers:
  - name: 'wechat'
    wechat_configs:
    - corp_id: 'YOUR_CORP_ID'
      api_secret: 'YOUR_API_SECRET'
      to_party: '1'
      agent_id: 'YOUR_AGENT_ID'
      api_url: 'https://qyapi.weixin.qq.com/cgi-bin/'
      message: |-
        {{ range .Alerts }}
        告警: {{ .Labels.alertname }}
        级别: {{ .Labels.severity }}
        描述: {{ .Annotations.description }}
        {{ end }}
```

### 3. 邮件通知

```yaml
receivers:
  - name: 'email'
    email_configs:
    - to: 'ops-team@example.com'
      from: 'alertmanager@example.com'
      smarthost: 'smtp.example.com:587'
      auth_username: 'alertmanager@example.com'
      auth_password: 'password'
      headers:
        Subject: '[{{ .Status | toUpper }}] {{ .GroupLabels.alertname }}'
      html: |-
        <h2>{{ .GroupLabels.alertname }}</h2>
        <table>
        {{ range .Alerts }}
        <tr>
          <td>Severity:</td>
          <td>{{ .Labels.severity }}</td>
        </tr>
        <tr>
          <td>Summary:</td>
          <td>{{ .Annotations.summary }}</td>
        </tr>
        <tr>
          <td>Description:</td>
          <td>{{ .Annotations.description }}</td>
        </tr>
        {{ end }}
        </table>
```

---

## 📝 日志监控

### 1. 日志采集

```yaml
# Collector日志配置
service:
  telemetry:
    logs:
      level: info
      output_paths:
        - /var/log/otel/collector.log
        - stdout
      error_output_paths:
        - /var/log/otel/error.log
        - stderr
```

### 2. ELK Stack集成

```yaml
# Filebeat配置
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/otel/*.log
  fields:
    service: otel-collector
    component: collector
  json.keys_under_root: true
  json.add_error_key: true

output.elasticsearch:
  hosts: ["elasticsearch:9200"]
  index: "otel-logs-%{+yyyy.MM.dd}"
```

### 3. 日志告警

```yaml
# ElastAlert配置
name: "High Error Rate"
type: frequency
index: otel-logs-*
num_events: 50
timeframe:
  minutes: 5

filter:
- term:
    level: "error"
- term:
    service: "otel-collector"

alert:
- slack:
    slack_webhook_url: "https://hooks.slack.com/services/YOUR/WEBHOOK"
```

---

## 🔍 性能分析

### 1. Profiling

```yaml
# Collector配置
extensions:
  pprof:
    endpoint: 0.0.0.0:1777
    block_profile_fraction: 0
    mutex_profile_fraction: 0

service:
  extensions: [pprof]
```

访问性能分析:

```bash
# CPU Profile
go tool pprof http://localhost:1777/debug/pprof/profile?seconds=30

# Heap Profile
go tool pprof http://localhost:1777/debug/pprof/heap

# Goroutine Profile
go tool pprof http://localhost:1777/debug/pprof/goroutine
```

### 2. zpages

```yaml
extensions:
  zpages:
    endpoint: 0.0.0.0:55679

service:
  extensions: [zpages]
```

访问 `http://localhost:55679/debug/`:

- `/debug/tracez` - 追踪采样
- `/debug/pipelinez` - Pipeline统计
- `/debug/servicez` - 服务状态
- `/debug/extensionz` - 扩展状态

---

## 📊 监控最佳实践

### 1. 监控指标选择

**必须监控**:

- 接收/导出速率
- 错误率
- 队列使用率
- 内存/CPU使用

**推荐监控**:

- 批处理延迟
- Goroutine数量
- 网络延迟
- 存储空间

### 2. 告警阈值设置

```yaml
# 示例阈值
- 导出失败率 > 5% (Critical)
- 队列使用率 > 80% (Warning)
- 队列使用率 > 95% (Critical)
- 内存使用 > 80% (Warning)
- CPU使用 > 85% (Warning)
- 数据接收停止 > 10分钟 (Warning)
```

### 3. 监控数据保留

```yaml
# Prometheus保留策略
global:
  storage:
    tsdb:
      retention.time: 15d  # 保留15天
      retention.size: 50GB  # 最大50GB
```

---

## 🔗 相关资源

- [⚡ 性能优化指南](⚡_性能优化完整指南.md)
- [🔧 故障排查手册](🔧_故障排查完整手册.md)
- [🐳 Docker部署指南](🐳_Docker部署完整指南.md)

---

**更新时间**: 2025年10月20日  
**版本**: v1.0.0  
**维护者**: OTLP项目团队

---

**📊 建立完善的监控告警体系！** 🚀
