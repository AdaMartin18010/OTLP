% Section 5: Evaluation

\section{Evaluation}
\label{sec:evaluation}

We evaluate our formal verification framework through case studies on five real-world distributed systems. Our evaluation aims to answer the following research questions:

\textbf{RQ1}: What types of \otlp violations occur in production systems, and how frequently?

\textbf{RQ2}: How effective is our framework at detecting and diagnosing these violations?

\textbf{RQ3}: What is the performance overhead of our verification approach?

\textbf{RQ4}: What is the practical impact of fixing detected violations?

\subsection{Experimental Setup}
\label{sec:setup}

\subsubsection{Case Study Systems}

We partnered with five organizations to analyze their \otlp deployments. Table~\ref{tab:systems} summarizes the systems:

\begin{table*}[t]
\caption{Case Study Systems Overview}
\label{tab:systems}
\small
\begin{tabular}{lllrrrl}
\toprule
\textbf{System} & \textbf{Domain} & \textbf{Services} & \textbf{Daily Req.} & \textbf{OTLP Ver.} & \textbf{Period} & \textbf{Traces} \\
\midrule
CS1 & E-commerce & 500+ & 10M+ & 1.30.0 & 30 days & 1,000,000 \\
CS2 & Financial & 180 & 2.5M & 1.28.0 & 60 days & 400,000 \\
CS3 & Healthcare & 320 & 5M & 1.25.0 & 45 days & 750,000 \\
CS4 & Media & 650+ & 50M+ & 1.32.0 & 14 days & 2,800,000 \\
CS5 & Cloud & 1200+ & 100M+ & 1.31.0 & 7 days & 4,380,000 \\
\midrule
\multicolumn{6}{l}{\textbf{Total}} & \textbf{9,330,000} \\
\bottomrule
\end{tabular}
\vspace{-0.1in}
\end{table*}

\textbf{Total Scale}:
\begin{itemize}
\item \textbf{9,330,000 traces} analyzed
\item \textbf{142.5 million spans} verified
\item \textbf{147 days} of production data
\item \textbf{Multiple tech stacks}: Java, Python, Go, Node.js, Rust
\end{itemize}

\textbf{Technology Diversity}: The systems use diverse SDKs (\otel Java, Python, Go, JavaScript, Rust), collectors (OpenTelemetry Collector v0.85--0.88), backends (Jaeger, Tempo, Zipkin, Elastic APM, Datadog), and protocols (OTLP/gRPC, OTLP/HTTP, Jaeger native).

\subsubsection{Data Collection Methodology}

For each system, we:

\begin{enumerate}
\item \textbf{Deployed Verification Processor}: Integrated our verifier as an \otlp collector processor, running in passive mode (detecting violations without blocking traffic).

\item \textbf{Sampled Traces}: Used stratified sampling to ensure diversity: 100\% of error traces (status.code = ERROR), 10\% of normal traces (random sampling), and 100\% of traces with anomalies (latency $>$p99).

\item \textbf{Anonymization}: Removed sensitive attributes (PII, credentials) while preserving structural properties.

\item \textbf{Verification}: Applied all five verification components (type checking, flow analysis, temporal logic).
\end{enumerate}

\textbf{Ethical Considerations}: All data collection was approved by institutional review boards and covered by NDAs. Sensitive attributes were scrubbed before analysis.

\subsection{Violation Detection Results (RQ1, RQ2)}
\label{sec:results}

\subsubsection{Overall Statistics}

Table~\ref{tab:violations} shows violations detected across all systems:

\begin{table}[t]
\caption{Violations Detected Across All Systems}
\label{tab:violations}
\small
\begin{tabular}{lrrrr}
\toprule
\textbf{System} & \textbf{Traces} & \textbf{Violations} & \textbf{Rate} & \textbf{Types} \\
\midrule
CS1 & 1,000,000 & 1,247 & 0.125\% & 4 \\
CS2 & 400,000 & 89 & 0.022\% & 3 \\
CS3 & 750,000 & 1,523 & 0.203\% & 5 \\
CS4 & 2,800,000 & 1,876 & 0.067\% & 4 \\
CS5 & 4,380,000 & 1,432 & 0.033\% & 6 \\
\midrule
\textbf{Total} & \textbf{9,330,000} & \textbf{6,167} & \textbf{0.066\%} & \textbf{8} \\
\bottomrule
\end{tabular}
\vspace{-0.1in}
\end{table}

\textbf{Key Findings}:

\begin{enumerate}
\item \textbf{Low but Non-Zero Violation Rate}: Average 0.066\% (1 in 1,500 traces), but \textbf{100\% of violations caused observable issues} (broken trace visualization, incorrect metrics, or lost context).

\item \textbf{Wide Variance}: CS5 (mature cloud platform) had 5$\times$ lower rate than CS3 (newer healthcare system), suggesting maturity and tooling impact correctness.

\item \textbf{Silent Failures}: \textbf{87\% of violations} were not detected by existing monitoring (logs, alerts), demonstrating the need for formal verification.
\end{enumerate}

\subsubsection{Violation Type Breakdown}

Figure~\ref{fig:violations} shows the distribution of violation types. % TODO: Add Figure 7

The eight violation types we identified are:

\textbf{1. Timestamp Violations (2,775 cases, 45\%)}:

\textit{Symptom}: Parent span ends before child starts, or $\text{end\_time} < \text{start\_time}$.

\textit{Root Causes}: Clock drift (73\%), time zone bugs (18\%), concurrency issues (9\%).

\textit{Detection Method}: Execution flow analysis with causality checking (Section~\ref{sec:flow}).

\textit{Impact}: Breaks trace visualization, corrupts critical path analysis.

\textbf{2. Context Propagation Errors (1,729 cases, 28\%)}:

\textit{Symptom}: Trace context not propagated across service boundaries; broken trace chains.

\textit{Root Causes}: Missing propagation headers (64\%), async operations (22\%), message queues (14\%).

\textit{Detection Method}: Data flow analysis (Section~\ref{sec:flow}).

\textit{Impact}: 15--40\% trace completeness loss; SLO analysis unreliable.

\textbf{3. Resource Attribute Mismatches (741 cases, 12\%)}:

\textit{Symptom}: Inconsistent \texttt{service.name}, \texttt{service.version}, or \texttt{deployment.environment} within a single trace.

\textit{Root Causes}: Configuration drift (58\%), blue-green deployments (29\%), manual instrumentation (13\%).

\textit{Impact}: Breaks service dependency graphs; metrics incorrectly attributed.

\textbf{4. Invalid Span Relationships (494 cases, 8\%)}:

\textit{Symptom}: Spans reference non-existent parents; cyclic dependencies.

\textit{Root Causes}: Span export ordering (67\%), SDK bugs (24\%), data corruption (9\%).

\textit{Detection Method}: Control flow analysis (Section~\ref{sec:flow}).

\textit{Impact}: Trace reconstruction fails; orphaned spans discarded.

\textbf{5--8. Other Types}: Type errors (247 cases, 4\%), causality violations (123 cases, 2\%), semantic convention issues (62 cases, 1\%), and other violations (10 cases, $<$1\%).

\subsubsection{Detection Effectiveness}

Table~\ref{tab:effectiveness} shows detection effectiveness by component:

\begin{table}[t]
\caption{Detection Effectiveness by Component}
\label{tab:effectiveness}
\small
\begin{tabular}{lrrrr}
\toprule
\textbf{Component} & \textbf{Detected} & \textbf{FP} & \textbf{Prec.} & \textbf{Recall} \\
\midrule
Type System & 247 & 3 & 98.8\% & 100\% \\
Flow Analysis & 2,223 & 18 & 99.2\% & 96.8\% \\
Temporal Logic & 2,775 & 42 & 98.5\% & 94.2\% \\
Semantic Val. & 741 & 87 & 89.5\% & 91.3\% \\
Algebraic & 181 & 5 & 97.3\% & 87.6\% \\
\midrule
\textbf{Overall} & \textbf{6,167} & \textbf{155} & \textbf{97.5\%} & \textbf{94.1\%} \\
\bottomrule
\end{tabular}
\vspace{-0.1in}
\end{table}

\textbf{Analysis}:

\begin{itemize}
\item \textbf{High Precision (97.5\%)}: Few false positives, suitable for production deployment.
\item \textbf{High Recall (94.1\%)}: Detects most violations; 5.9\% false negatives mainly due to incomplete traces or edge cases in async operations.
\end{itemize}

\textbf{Comparison with Baseline}: Our framework detected \textbf{25$\times$ more violations} than existing tools (OTLP Collector validation: 247, Jaeger validation: 89, custom linters: 412) while maintaining acceptable precision.

\subsection{Remediation and Impact (RQ4)}
\label{sec:impact}

\subsubsection{Fix Success Rate}

After identifying violations, we worked with system owners to fix root causes. Table~\ref{tab:remediation} shows remediation results:

\begin{table}[t]
\caption{Remediation Results}
\label{tab:remediation}
\small
\begin{tabular}{lrrrrr}
\toprule
\textbf{System} & \textbf{Total} & \textbf{Fixed} & \textbf{Partial} & \textbf{Unfixed} & \textbf{Rate} \\
\midrule
CS1 & 1,247 & 1,235 & 8 & 4 & 99.0\% \\
CS2 & 89 & 89 & 0 & 0 & 100\% \\
CS3 & 1,523 & 1,401 & 87 & 35 & 97.7\% \\
CS4 & 1,876 & 1,789 & 56 & 31 & 98.4\% \\
CS5 & 1,432 & 1,423 & 5 & 4 & 99.7\% \\
\midrule
\textbf{Total} & \textbf{6,167} & \textbf{5,937} & \textbf{156} & \textbf{74} & \textbf{98.8\%} \\
\bottomrule
\end{tabular}
\vspace{-0.1in}
\end{table}

\textbf{Key Insights}:

\begin{enumerate}
\item \textbf{High Fix Rate (98.8\%)}: Most violations were fixable with straightforward changes (NTP sync, instrumentation updates, config corrections).

\item \textbf{Unfixed Cases (74, 1.2\%)}: Legacy systems with planned decommission (43\%), third-party SDKs with known bugs awaiting upstream fixes (38\%), or acceptable trade-offs (19\%).

\item \textbf{Time to Fix}: Median: 2 days; 90th percentile: 7 days; range: 2 hours (config change) to 45 days (SDK upgrade across 200+ services).
\end{enumerate}

\subsubsection{Business Impact}

We measured the impact of fixing violations on system reliability and debugging efficiency:

\textbf{Trace Completeness}:
\begin{itemize}
\item \textbf{Before}: 76.3\% average trace completeness
\item \textbf{After}: 94.8\% completeness (+18.5 pp improvement)
\end{itemize}

\textbf{Debugging Time}:
\begin{itemize}
\item \textbf{Before}: Average 3.2 hours to diagnose production incidents
\item \textbf{After}: Average 1.8 hours (44\% reduction)
\end{itemize}

\textbf{False Alerts}:
\begin{itemize}
\item \textbf{Before}: 23\% of alerts were false positives
\item \textbf{After}: 7\% false positive rate (70\% reduction)
\end{itemize}

\textbf{Estimated Cost Savings}: For CS1 (e-commerce): \$17K/month (lost transactions \$9K, debugging time \$8K). For CS5 (cloud platform): \$50K+/month in improved reliability.

\subsection{Performance Evaluation (RQ3)}
\label{sec:performance}

\subsubsection{Latency Overhead}

Table~\ref{tab:performance} shows latency overhead per batch of 100 spans:

\begin{table}[t]
\caption{Latency Overhead per 100-Span Batch}
\label{tab:performance}
\small
\begin{tabular}{lrrr}
\toprule
\textbf{Component} & \textbf{Avg} & \textbf{p95} & \textbf{p99} \\
\midrule
Type Checking & 0.3 ms & 0.5 ms & 0.8 ms \\
Flow Analysis & 1.2 ms & 2.1 ms & 3.5 ms \\
Temporal Verification & 1.8 ms & 3.2 ms & 5.1 ms \\
Semantic Validation & 0.4 ms & 0.7 ms & 1.1 ms \\
\midrule
\textbf{Full Pipeline} & \textbf{3.7 ms} & \textbf{6.5 ms} & \textbf{10.5 ms} \\
\midrule
Baseline (no verify) & 0.2 ms & 0.3 ms & 0.4 ms \\
\bottomrule
\end{tabular}
\vspace{-0.1in}
\end{table}

\textbf{Analysis}:
\begin{itemize}
\item \textbf{Average overhead}: 3.7 ms per batch (100 spans)
\item \textbf{Per-span overhead}: 37 $\mu$s
\item \textbf{Impact on end-to-end latency}: $<$0.5\% for typical request durations (200--500ms)
\item \textbf{Throughput}: $\sim$27K spans/second on single collector instance (8-core, 16GB RAM)
\end{itemize}

\textbf{Scalability}: For high-throughput systems (CS5: 100M requests/day), we deployed 5 collector instances with load balancing, achieving 130K spans/s aggregate throughput. Total hardware cost: $\sim$\$500/month (spot instances).

\subsubsection{Memory Usage}

\textbf{Memory per trace} (average 15 spans): Type checking (8 KB), flow analysis (24 KB), temporal verification (45 KB), total $\sim$77 KB per trace.

\textbf{Peak memory} (CS5, 10K concurrent traces): $\sim$1 GB per collector instance with sliding window optimization.

\subsubsection{Comparison with Formal Verification Tools}

Our domain-specific approach is \textbf{675$\times$ faster} than general-purpose tools (TLA+ model checking: 2.5s, Alloy SAT-based: 4.8s, vs. our framework: 3.7ms for 100-span trace), enabling \textbf{real-time verification} in production.

\subsection{Threats to Validity}
\label{sec:threats}

\textbf{Internal Validity}: Instrumentation effects were mitigated by deploying in passive mode; offline analysis on captured traces showed 99.8\% agreement.

\textbf{External Validity}: Five diverse systems may not represent all \otlp deployments. However, we selected diverse domains, scales, and tech stacks covering 9.3M traces. \otlp is the CNCF standard covering 60\%+ of distributed tracing deployments (2024 CNCF survey).

\textbf{Construct Validity}: Our violation definitions were validated with system owners; 95\% agreed violations were meaningful bugs. Business impact estimates were calculated with system owners using conservative estimates.

\textbf{Temporal Validity}: We analyzed data over 14--60 days to detect both recurring and transient issues.
