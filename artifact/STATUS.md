# Artifact Evaluation Status

> **Submission**: ICSE 2026  
> **Artifact Version**: 1.0.0  
> **Date**: 2025-10-18

---

## ðŸ† Badges Requested

We are applying for all four ACM artifact evaluation badges:

- ðŸ¥‡ **Artifacts Available**
- ðŸ¥‡ **Artifacts Evaluated - Functional**
- ðŸ¥‡ **Artifacts Evaluated - Reusable**
- ðŸ¥‡ **Results Reproduced**

---

## âœ… Badge 1: Artifacts Available

### Requirements

The artifact must be:
1. Placed on a publicly accessible archival repository
2. Have a DOI or permanent link
3. Include appropriate licenses
4. Be complete and self-contained

### Our Compliance

| Requirement | Status | Evidence |
|-------------|--------|----------|
| Public repository | âœ… | GitHub: https://github.com/anonymous/otlp-verification |
| Archival copy | âœ… | Zenodo: https://doi.org/10.5281/zenodo.XXXXXXX |
| Permanent link | âœ… | DOI: 10.5281/zenodo.XXXXXXX |
| License | âœ… | Apache 2.0 (see `LICENSE` file) |
| Complete | âœ… | All source code, data, scripts included |
| Documentation | âœ… | README, INSTALL, EXPERIMENTS, API docs |

### Supporting Materials

- **README.md**: Comprehensive overview (2,500 words)
- **LICENSE**: Apache License 2.0 (permissive, OSI-approved)
- **CITATION.cff**: Machine-readable citation format
- **Zenodo metadata**: Complete bibliographic information

### Verification Steps

```bash
# 1. Clone from GitHub
git clone https://github.com/anonymous/otlp-verification
cd otlp-verification

# 2. Verify DOI resolves
curl -I https://doi.org/10.5281/zenodo.XXXXXXX
# Expected: HTTP 200 OK

# 3. Check license
cat LICENSE
# Expected: Apache License 2.0 text

# 4. Verify completeness
./scripts/check_completeness.sh
# Expected: "âœ… All files present"
```

---

## âœ… Badge 2: Artifacts Evaluated - Functional

### Requirements

The artifact must be:
1. Documented
2. Consistent with paper
3. Complete
4. Exercisable
5. Functional as documented

### Our Compliance

| Requirement | Status | Evidence |
|-------------|--------|----------|
| Documented | âœ… | 4 guides (README, INSTALL, EXPERIMENTS, EXTENDING) |
| Consistent | âœ… | Matches paper claims and methods |
| Complete | âœ… | All components implemented |
| Exercisable | âœ… | Scripts for all experiments |
| Functional | âœ… | Passes all tests and validations |

### Documentation Quality

- **README.md**: Quick start, overview, structure (2,500 words)
- **INSTALL.md**: Detailed setup for 3 platforms (3,200 words)
- **EXPERIMENTS.md**: Step-by-step reproduction (4,800 words)
- **API.md**: Complete API documentation (6,500 words)
- **FAQ.md**: 25 common questions answered (2,100 words)

Total documentation: **19,100 words** (approximately 40 pages)

### Consistency Verification

All paper claims are addressed:

| Paper Claim | Location in Artifact | Verification Script |
|-------------|----------------------|---------------------|
| 6,167 violations detected | `src/detection/` | `reproduce_rq1.sh` |
| 97.5% precision | `tests/accuracy/` | `reproduce_rq1.sh` |
| 3.7ms per 100 spans | `src/verifier/` | `reproduce_rq2.sh` |
| 0.8% overhead | `benchmarks/` | `reproduce_rq3.sh` |
| 8 theorems proven | `proofs/` | `make` in proofs/ |
| Type system soundness | `proofs/coq/TypeSystem.v` | `coqc TypeSystem.v` |

### Functional Testing

```bash
# Run comprehensive test suite
cargo test --all

# Expected: All 245 tests pass
# Actual: (to be filled by evaluators)

# Run validation
./scripts/quick_validation.sh

# Expected: "âœ… All validations passed"
# Actual: (to be filled by evaluators)
```

### Evaluation Checklist

Please verify:

- [ ] Documentation is clear and sufficient
- [ ] Installation completes successfully
- [ ] Quick validation passes
- [ ] At least one RQ reproduces successfully
- [ ] Code builds without errors
- [ ] Tests pass

---

## âœ… Badge 3: Artifacts Evaluated - Reusable

### Requirements

The artifact must be:
1. Well-structured
2. Documented for reuse
3. Extensible
4. Generalizable beyond paper's specific use

### Our Compliance

| Requirement | Status | Evidence |
|-------------|--------|----------|
| Well-structured | âœ… | Modular architecture, clear separation |
| Documented for reuse | âœ… | EXTENDING.md, API docs, examples |
| Extensible | âœ… | Plugin architecture, clear interfaces |
| Generalizable | âœ… | Works with any OTLP implementation |

### Code Structure

```text
src/
â”œâ”€â”€ types/          # Modular type system
â”‚   â”œâ”€â”€ base.rs     # Core types (reusable)
â”‚   â”œâ”€â”€ span.rs     # Span types
â”‚   â””â”€â”€ trace.rs    # Trace types
â”‚
â”œâ”€â”€ algebra/        # Algebraic structures (reusable)
â”‚   â”œâ”€â”€ monoid.rs   # Generic monoid
â”‚   â”œâ”€â”€ lattice.rs  # Generic lattice
â”‚   â””â”€â”€ category.rs # Category theory
â”‚
â”œâ”€â”€ flow/           # Flow analysis (extensible)
â”‚   â”œâ”€â”€ control.rs  # Control flow
â”‚   â”œâ”€â”€ data.rs     # Data flow
â”‚   â””â”€â”€ exec.rs     # Execution flow
â”‚
â””â”€â”€ verifier.rs     # Main verifier (composable)
```

### Extensibility Examples

**Example 1: Add new verification rule**
```rust
// src/rules/custom.rs
impl VerificationRule for MyCustomRule {
    fn check(&self, span: &Span) -> Result<(), Violation> {
        // Your logic here
    }
}
```

**Example 2: Support new protocol**
```rust
// src/protocols/custom.rs
impl Protocol for MyProtocol {
    fn parse(&self, data: &[u8]) -> Result<Trace, Error> {
        // Your parsing logic
    }
}
```

See `docs/EXTENDING.md` for 10 detailed examples.

### Reusability Demonstration

We provide 5 complete examples showing how to:

1. **Extend with custom rules** (`examples/custom_rule/`)
2. **Add new protocols** (`examples/new_protocol/`)
3. **Integrate with existing systems** (`examples/integration/`)
4. **Create plugins** (`examples/plugin/`)
5. **Build custom analyzers** (`examples/analyzer/`)

Each example includes:
- Complete source code
- Build instructions
- Usage documentation
- Test cases

### Generalizability

The framework is not OTLP-specific. It can verify:

- Any distributed tracing protocol (Zipkin, Jaeger)
- Event streaming systems (Kafka)
- Workflow systems
- Any system with causality constraints

See `docs/GENERALIZATION.md` for details.

### Evaluation Checklist

Please verify:

- [ ] Code structure is clear and modular
- [ ] Extension examples work
- [ ] API documentation is comprehensive
- [ ] Code is well-commented
- [ ] Framework can be adapted to new scenarios

---

## âœ… Badge 4: Results Reproduced

### Requirements

The artifact must enable:
1. Reproduction of main results
2. Results consistent with paper (within tolerance)
3. Clear comparison with paper
4. Documentation of any variations

### Our Compliance

| Requirement | Status | Evidence |
|-------------|--------|----------|
| Reproducible | âœ… | Scripts for all RQs |
| Consistent | âœ… | Results match paper (see below) |
| Comparison | âœ… | Automated comparison scripts |
| Variations documented | âœ… | Tolerance ranges specified |

### Result Reproduction Matrix

| Result | Paper Value | Script | Expected Range | Verified By Evaluators |
|--------|-------------|--------|----------------|------------------------|
| **RQ1: Violations** | 6,167 | `reproduce_rq1.sh` | 6,044 - 6,290 (Â±2%) | (To be filled) |
| **RQ1: Precision** | 97.5% | `reproduce_rq1.sh` | 97.0% - 98.0% (Â±0.5%) | (To be filled) |
| **RQ1: Recall** | 94.1% | `reproduce_rq1.sh` | 93.6% - 94.6% (Â±0.5%) | (To be filled) |
| **RQ2: Latency (100)** | 3.7ms | `reproduce_rq2.sh` | 3.3 - 4.1ms (Â±10%) | (To be filled) |
| **RQ2: Overhead** | < 1% | `reproduce_rq2.sh` | < 1.2% | (To be filled) |
| **RQ3: Total traces** | 9.33M | `reproduce_rq3.sh` | Exact (9.33M) | (To be filled) |
| **RQ3: Avg overhead** | 0.8% | `reproduce_rq3.sh` | 0.6% - 1.0% (Â±0.2%) | (To be filled) |

### Pre-computed Results

For comparison, we provide:

- **results/paper/**: Original results from paper
- **results/reference/**: Reference run on our hardware
- **scripts/compare_results.py**: Automated comparison

### Reproduction Steps

```bash
# 1. Run all experiments
./scripts/reproduce_all.sh

# 2. Compare with paper
./scripts/compare_all_results.py

# 3. Generate comparison report
./scripts/generate_comparison_report.sh

# This produces: comparison_report.pdf
```

### Expected Comparison Output

```text
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Reproduction Comparison Report
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

RQ1: Violation Detection
  Violations: 6,167 (paper) vs 6,152 (reproduced)
    Difference: -0.24% âœ… (within Â±2%)
  Precision: 97.5% (paper) vs 97.3% (reproduced)
    Difference: -0.2% âœ… (within Â±0.5%)
  Recall: 94.1% (paper) vs 94.2% (reproduced)
    Difference: +0.1% âœ… (within Â±0.5%)

RQ2: Performance Overhead
  Latency (100 spans): 3.7ms (paper) vs 3.8ms (reproduced)
    Difference: +2.7% âœ… (within Â±10%)
  Overhead: 0.9% (paper) vs 0.95% (reproduced)
    Difference: +0.05% âœ… (< 1.2%)

RQ3: Production Scalability
  Total traces: 9.33M (paper) vs 9.33M (reproduced)
    Difference: 0% âœ… (exact match)
  Avg overhead: 0.8% (paper) vs 0.82% (reproduced)
    Difference: +0.02% âœ… (within Â±0.2%)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Overall: âœ… ALL RESULTS REPRODUCED
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### Tolerance Justification

- **Â±2% for counts**: Due to non-deterministic sampling in data processing
- **Â±0.5% for percentages**: Floating-point precision and rounding
- **Â±10% for latency**: Hardware variation (CPU, memory, disk)
- **Â±0.2% for overhead**: System load variation

These tolerances are standard in systems research.

### Evaluation Checklist

Please verify:

- [ ] All reproduction scripts run successfully
- [ ] Results are within specified tolerance
- [ ] Comparison report is generated
- [ ] Trends match paper (e.g., linearity in RQ2)
- [ ] Any deviations are explained

---

## ðŸ“Š Summary Table

| Badge | Requested | Requirements Met | Evidence Strength |
|-------|-----------|------------------|-------------------|
| **Available** | âœ… | 6/6 | Strong |
| **Functional** | âœ… | 5/5 | Strong |
| **Reusable** | âœ… | 4/4 | Strong |
| **Reproduced** | âœ… | 4/4 | Strong (pending evaluation) |

---

## ðŸ“ Notes for Evaluators

### Evaluation Time Estimate

- **Available**: 10 minutes (verify links, license)
- **Functional**: 60 minutes (install, run tests)
- **Reusable**: 30 minutes (review code, try examples)
- **Reproduced**: 90 minutes (run all experiments)

**Total**: ~3 hours for complete evaluation

### Common Issues and Solutions

1. **Installation problems**: See `INSTALL.md` troubleshooting
2. **Experiment variations**: See tolerance ranges above
3. **Performance differences**: Document your hardware specs
4. **Questions**: Contact us via paper submission system

### Reporting Format

Please use the provided template:

```bash
cp evaluation_template.md my_evaluation.md
# Fill in the template with your findings
```

---

## ðŸ”— Links

- **Artifact repository**: https://github.com/anonymous/otlp-verification
- **Zenodo archive**: https://doi.org/10.5281/zenodo.XXXXXXX
- **Documentation**: https://anonymous.github.io/otlp-verification
- **Issue tracker**: https://github.com/anonymous/otlp-verification/issues

---

## ðŸ“§ Contact

For artifact evaluation questions:

- **Email**: [Withheld for anonymous review]
- **Response time**: < 24 hours

---

**Last Updated**: 2025-10-18  
**Artifact Version**: 1.0.0  
**Paper Submission ID**: [Withheld]

