# OpenTelemetry æ€§èƒ½ä¼˜åŒ–æŒ‡å—

> ğŸ“š **æ–‡æ¡£å¯¼èˆª**: [è¿”å›æ–‡æ¡£ç´¢å¼•](INDEX.md) | [æ¶æ„è®¾è®¡](ARCHITECTURE.md) | [é›†æˆæŒ‡å—](INTEGRATION_GUIDE.md) | [æ•…éšœæ’é™¤](TROUBLESHOOTING.md)

## æ€§èƒ½åŸºå‡†

### å®˜æ–¹åŸºå‡†æ•°æ®

| ä¼ è¾“æ–¹å¼ | ååé‡ | CPUä½¿ç”¨ | å†…å­˜ä½¿ç”¨ | ç½‘ç»œå¸¦å®½ |
|----------|--------|---------|----------|----------|
| gRPC | 200k spans/s | 1.2æ ¸ | 512MB | 280 Mb/s |
| HTTP | 60k spans/s | 1.5æ ¸ | 768MB | 310 Mb/s |

### ç¡¬ä»¶è¦æ±‚

- **CPU**: 2æ ¸ä»¥ä¸Š
- **å†…å­˜**: 1GBä»¥ä¸Š
- **ç½‘ç»œ**: 100Mbpsä»¥ä¸Š
- **å­˜å‚¨**: SSDæ¨è

## é‡‡æ ·ä¼˜åŒ–

### é‡‡æ ·ç­–ç•¥é€‰æ‹©

```yaml
# ç”Ÿäº§ç¯å¢ƒæ¨è
processors:
  probabilistic_sampler:
    sampling_percentage: 1.0  # 1%é‡‡æ ·ç‡

# å¼€å‘ç¯å¢ƒ
processors:
  probabilistic_sampler:
    sampling_percentage: 100.0  # 100%é‡‡æ ·ç‡
```

### åŠ¨æ€é‡‡æ ·

```yaml
processors:
  tail_sampling:
    decision_wait: 10s
    num_traces: 50000
    expected_new_traces_per_sec: 10
    policies:
      - name: error-rate-policy
        type: rate_limiting
        rate_limiting:
          spans_per_second: 100
      - name: latency-policy
        type: latency
        latency:
          threshold_ms: 1000
```

## æ‰¹å¤„ç†ä¼˜åŒ–

### æ‰¹å¤„ç†é…ç½®

```yaml
processors:
  batch:
    timeout: 1s
    send_batch_size: 512
    send_batch_max_size: 2048
    metadata_keys:
      - "service.name"
      - "service.version"
```

### å†…å­˜ç®¡ç†

```yaml
processors:
  memory_limiter:
    check_interval: 2s
    limit_mib: 512
    spike_limit_mib: 128
    limit_percentage: 80
    spike_limit_percentage: 95
```

## ç½‘ç»œä¼˜åŒ–

### è¿æ¥æ± é…ç½®

```yaml
exporters:
  otlp:
    sending_queue:
      enabled: true
      num_consumers: 10
      queue_size: 1000
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 300s
```

### å‹ç¼©é…ç½®

```yaml
exporters:
  otlp:
    compression: "gzip"
    timeout: 10s
```

## å­˜å‚¨ä¼˜åŒ–

### æ•°æ®ä¿ç•™ç­–ç•¥

```yaml
exporters:
  jaeger:
    endpoint: jaeger:14250
    sending_queue:
      queue_size: 1000
    retry_on_failure:
      max_elapsed_time: 300s
```

### ç´¢å¼•ä¼˜åŒ–

```yaml
exporters:
  elasticsearch:
    endpoints: ["http://elasticsearch:9200"]
    index: "otel-traces"
    mapping:
      mode: "ecs"
    bulk_actions: 1000
    bulk_size: 5242880
```

## åº”ç”¨å±‚ä¼˜åŒ–

### SDKé…ç½®

```go
// Go SDKä¼˜åŒ–
tracerProvider := sdktrace.NewTracerProvider(
    sdktrace.WithBatcher(exporter, 
        sdktrace.WithBatchTimeout(time.Second),
        sdktrace.WithMaxExportBatchSize(512),
    ),
    sdktrace.WithSpanLimits(sdktrace.SpanLimits{
        AttributeCountLimit: 128,
        EventCountLimit: 128,
        LinkCountLimit: 128,
    }),
)
```

### å¼‚æ­¥å¤„ç†

```go
// å¼‚æ­¥Spanå¤„ç†
tracerProvider := sdktrace.NewTracerProvider(
    sdktrace.WithBatcher(exporter),
    sdktrace.WithResource(resource),
)
```

## ç›‘æ§å’Œè°ƒä¼˜

### å…³é”®æŒ‡æ ‡

```yaml
# CollectoræŒ‡æ ‡
otelcol_receiver_accepted_spans
otelcol_receiver_refused_spans
otelcol_processor_batch_batch_send_size
otelcol_exporter_sent_spans
otelcol_exporter_send_failed_spans
```

### æ€§èƒ½ç›‘æ§

```yaml
service:
  telemetry:
    metrics:
      address: 0.0.0.0:8888
      readers:
        - periodic:
            interval: 10s
```

### å‘Šè­¦è§„åˆ™

```yaml
groups:
- name: performance-alerts
  rules:
  - alert: HighRefusedSpans
    expr: rate(otelcol_receiver_refused_spans[5m]) > 100
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "High number of refused spans"
  
  - alert: HighMemoryUsage
    expr: otelcol_memory_usage_bytes / otelcol_memory_limit_bytes > 0.8
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "High memory usage"
```

## å‹åŠ›æµ‹è¯•

### æµ‹è¯•è„šæœ¬

```bash
# è¿è¡Œå‹åŠ›æµ‹è¯•
./benchmarks/run-rust.ps1 -Loops 10000 -SleepMs 0

# ç›‘æ§ç³»ç»Ÿèµ„æº
top -p $(pgrep otelcol)
```

### æµ‹è¯•æŒ‡æ ‡

- ååé‡ (spans/second)
- å»¶è¿Ÿ (P50, P95, P99)
- å†…å­˜ä½¿ç”¨
- CPUä½¿ç”¨
- ç½‘ç»œå¸¦å®½

## æ•…éšœæ¢å¤

### è‡ªåŠ¨æ¢å¤

```yaml
exporters:
  otlp:
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 300s
      multiplier: 2.0
```

### é™çº§ç­–ç•¥

```yaml
processors:
  routing:
    from_attribute: "service.name"
    default_exporters: [logging]
    table:
      - value: "critical-service"
        exporters: [jaeger, prometheus]
      - value: "normal-service"
        exporters: [jaeger]
```

## é«˜çº§æ€§èƒ½ä¼˜åŒ–

### 1. å†…å­˜ä¼˜åŒ–ç­–ç•¥

#### å†…å­˜æ± ç®¡ç†

```go
// ä½¿ç”¨å†…å­˜æ± å‡å°‘GCå‹åŠ›
var spanPool = sync.Pool{
    New: func() interface{} {
        return &Span{
            Attributes: make(map[string]interface{}, 16),
            Events:     make([]Event, 0, 8),
        }
    },
}

func getSpan() *Span {
    return spanPool.Get().(*Span)
}

func putSpan(s *Span) {
    s.Reset()
    spanPool.Put(s)
}
```

#### å†…å­˜æ˜ å°„æ–‡ä»¶

```yaml
# ä½¿ç”¨å†…å­˜æ˜ å°„æ–‡ä»¶å­˜å‚¨å¤§é‡æ•°æ®
exporters:
  file:
    path: /tmp/otel-data.bin
    format: binary
    memory_mapped: true
    buffer_size: 64MB
```

### 2. CPUä¼˜åŒ–ç­–ç•¥

#### å¹¶å‘å¤„ç†ä¼˜åŒ–

```yaml
# ä¼˜åŒ–å¹¶å‘å¤„ç†
processors:
  batch:
    timeout: 100ms
    send_batch_size: 1024
    send_batch_max_size: 2048
    metadata_keys:
      - "service.name"
      - "service.version"

exporters:
  otlp:
    sending_queue:
      enabled: true
      num_consumers: 20
      queue_size: 2000
      storage: "file_storage"
```

#### CPUäº²å’Œæ€§è®¾ç½®

```bash
# è®¾ç½®CPUäº²å’Œæ€§
taskset -c 0-3 otelcol --config=config.yaml

# æˆ–ä½¿ç”¨Docker
docker run --cpuset-cpus="0-3" otel/opentelemetry-collector-contrib:latest
```

### 3. ç½‘ç»œä¼˜åŒ–ç­–ç•¥

#### è¿æ¥å¤ç”¨

```yaml
# ä¼˜åŒ–è¿æ¥å¤ç”¨
exporters:
  otlp:
    endpoint: "http://collector:4317"
    compression: "gzip"
    timeout: 10s
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 300s
    sending_queue:
      enabled: true
      num_consumers: 10
      queue_size: 1000
```

#### ç½‘ç»œç¼“å†²åŒºä¼˜åŒ–

```bash
# ä¼˜åŒ–ç½‘ç»œç¼“å†²åŒº
echo 'net.core.rmem_max = 134217728' >> /etc/sysctl.conf
echo 'net.core.wmem_max = 134217728' >> /etc/sysctl.conf
echo 'net.ipv4.tcp_rmem = 4096 87380 134217728' >> /etc/sysctl.conf
echo 'net.ipv4.tcp_wmem = 4096 65536 134217728' >> /etc/sysctl.conf
sysctl -p
```

### 4. å­˜å‚¨ä¼˜åŒ–ç­–ç•¥

#### æ•°æ®å‹ç¼©

```yaml
# å¯ç”¨æ•°æ®å‹ç¼©
exporters:
  otlp:
    compression: "gzip"
    compression_level: 6
  
  kafka:
    compression: "gzip"
    compression_level: 6
    batch_size: 1048576
```

#### å­˜å‚¨åˆ†å±‚

```yaml
# å®ç°å­˜å‚¨åˆ†å±‚
exporters:
  hot_storage:
    type: "jaeger"
    endpoint: "jaeger-hot:14250"
    storage_ttl: "24h"
  
  cold_storage:
    type: "s3"
    bucket: "otel-cold-storage"
    prefix: "traces/"
    storage_ttl: "30d"
```

### 5. ç¼“å­˜ä¼˜åŒ–ç­–ç•¥

#### å¤šçº§ç¼“å­˜

```yaml
# å®ç°å¤šçº§ç¼“å­˜
processors:
  cache:
    type: "redis"
    endpoint: "redis:6379"
    ttl: "1h"
    max_size: 10000
  
  memory_cache:
    type: "lru"
    max_size: 1000
    ttl: "10m"
```

#### ç¼“å­˜é¢„çƒ­

```go
// ç¼“å­˜é¢„çƒ­ç­–ç•¥
func warmupCache() {
    // é¢„åŠ è½½å¸¸ç”¨æ•°æ®
    commonTraces := getCommonTraces()
    for _, trace := range commonTraces {
        cache.Set(trace.ID, trace, time.Hour)
    }
}
```

## æ€§èƒ½ç›‘æ§å’Œåˆ†æ

### 1. å®æ—¶æ€§èƒ½ç›‘æ§

#### å…³é”®æŒ‡æ ‡ç›‘æ§

```yaml
# å…³é”®æ€§èƒ½æŒ‡æ ‡
groups:
- name: performance-monitoring
  rules:
  - alert: HighCPUUsage
    expr: rate(otelcol_cpu_usage_percent[5m]) > 80
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "High CPU usage detected"
  
  - alert: HighMemoryUsage
    expr: otelcol_memory_usage_bytes / otelcol_memory_limit_bytes > 0.8
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "High memory usage detected"
  
  - alert: HighQueueSize
    expr: otelcol_processor_batch_queue_size > 1000
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "High queue size detected"
```

#### æ€§èƒ½åˆ†æå·¥å…·

```bash
#!/bin/bash
# performance-analysis.sh

echo "=== OpenTelemetry æ€§èƒ½åˆ†æ ==="

# æ”¶é›†æ€§èƒ½æ•°æ®
echo "æ”¶é›†CPUä½¿ç”¨æƒ…å†µ..."
top -p $(pgrep otelcol) -n 1 -b > cpu-usage.txt

echo "æ”¶é›†å†…å­˜ä½¿ç”¨æƒ…å†µ..."
ps aux | grep otelcol > memory-usage.txt

echo "æ”¶é›†ç½‘ç»œç»Ÿè®¡..."
ss -tuln | grep 4317 > network-stats.txt

echo "æ”¶é›†CollectoræŒ‡æ ‡..."
curl -s http://localhost:13133/metrics > collector-metrics.txt

echo "åˆ†ææ€§èƒ½ç“¶é¢ˆ..."
python analyze-performance.py cpu-usage.txt memory-usage.txt network-stats.txt collector-metrics.txt

echo "=== æ€§èƒ½åˆ†æå®Œæˆ ==="
```

### 2. æ€§èƒ½åŸºå‡†æµ‹è¯•

#### è‡ªåŠ¨åŒ–åŸºå‡†æµ‹è¯•

```bash
#!/bin/bash
# benchmark-test.sh

echo "=== OpenTelemetry åŸºå‡†æµ‹è¯• ==="

# æµ‹è¯•ä¸åŒè´Ÿè½½ä¸‹çš„æ€§èƒ½
for load in 1000 5000 10000 20000; do
    echo "æµ‹è¯•è´Ÿè½½: $load spans/second"
    
    # å¯åŠ¨æµ‹è¯•
    ./benchmarks/run-rust.ps1 -Loops $load -SleepMs 0 &
    TEST_PID=$!
    
    # ç›‘æ§æ€§èƒ½
    start_time=$(date +%s)
    while kill -0 $TEST_PID 2>/dev/null; do
        cpu_usage=$(ps -p $TEST_PID -o %cpu= | tr -d ' ')
        memory_usage=$(ps -p $TEST_PID -o %mem= | tr -d ' ')
        echo "$(date): CPU: ${cpu_usage}%, Memory: ${memory_usage}%"
        sleep 1
    done
    end_time=$(date +%s)
    
    duration=$((end_time - start_time))
    echo "è´Ÿè½½ $load å®Œæˆï¼Œè€—æ—¶: ${duration}ç§’"
done

echo "=== åŸºå‡†æµ‹è¯•å®Œæˆ ==="
```

#### æ€§èƒ½å¯¹æ¯”æµ‹è¯•

```bash
#!/bin/bash
# performance-comparison.sh

echo "=== OpenTelemetry æ€§èƒ½å¯¹æ¯”æµ‹è¯• ==="

# æµ‹è¯•ä¸åŒé…ç½®çš„æ€§èƒ½
configs=("minimal.yaml" "optimized.yaml" "high-performance.yaml")

for config in "${configs[@]}"; do
    echo "æµ‹è¯•é…ç½®: $config"
    
    # å¯åŠ¨Collector
    otelcol --config="$config" &
    COLLECTOR_PID=$!
    sleep 10
    
    # è¿è¡Œæ€§èƒ½æµ‹è¯•
    ./benchmarks/run-rust.ps1 -Loops 10000 -SleepMs 0
    
    # æ”¶é›†ç»“æœ
    curl -s http://localhost:13133/metrics > "results-${config}.txt"
    
    # åœæ­¢Collector
    kill $COLLECTOR_PID
    sleep 5
done

echo "=== æ€§èƒ½å¯¹æ¯”æµ‹è¯•å®Œæˆ ==="
```

### 3. æ€§èƒ½è°ƒä¼˜å»ºè®®

#### åŸºäºè´Ÿè½½çš„è°ƒä¼˜

```yaml
# ä½è´Ÿè½½é…ç½® (< 1k spans/s)
processors:
  batch:
    timeout: 1s
    send_batch_size: 256
    send_batch_max_size: 512

# ä¸­ç­‰è´Ÿè½½é…ç½® (1k-10k spans/s)
processors:
  batch:
    timeout: 500ms
    send_batch_size: 512
    send_batch_max_size: 1024

# é«˜è´Ÿè½½é…ç½® (> 10k spans/s)
processors:
  batch:
    timeout: 100ms
    send_batch_size: 1024
    send_batch_max_size: 2048
```

#### åŸºäºç¡¬ä»¶çš„è°ƒä¼˜

```yaml
# ä½é…ç½®ç¡¬ä»¶ (2æ ¸4GB)
processors:
  memory_limiter:
    limit_mib: 512
    spike_limit_mib: 128
  batch:
    timeout: 1s
    send_batch_size: 256

# ä¸­ç­‰é…ç½®ç¡¬ä»¶ (4æ ¸8GB)
processors:
  memory_limiter:
    limit_mib: 1024
    spike_limit_mib: 256
  batch:
    timeout: 500ms
    send_batch_size: 512

# é«˜é…ç½®ç¡¬ä»¶ (8æ ¸16GB+)
processors:
  memory_limiter:
    limit_mib: 2048
    spike_limit_mib: 512
  batch:
    timeout: 100ms
    send_batch_size: 1024
```

## æ€§èƒ½ä¼˜åŒ–å·¥å…·

### 1. æ€§èƒ½åˆ†æå·¥å…·

#### Goæ€§èƒ½åˆ†æ

```bash
# å¯åŠ¨æ€§èƒ½åˆ†æ
go tool pprof http://localhost:13133/debug/pprof/profile

# å†…å­˜åˆ†æ
go tool pprof http://localhost:13133/debug/pprof/heap

# åç¨‹åˆ†æ
go tool pprof http://localhost:13133/debug/pprof/goroutine
```

#### ç³»ç»Ÿæ€§èƒ½åˆ†æ

```bash
# ä½¿ç”¨perfåˆ†æ
perf record -p $(pgrep otelcol) -g sleep 30
perf report

# ä½¿ç”¨straceè·Ÿè¸ªç³»ç»Ÿè°ƒç”¨
strace -p $(pgrep otelcol) -o otelcol-strace.log

# ä½¿ç”¨tcpdumpåˆ†æç½‘ç»œ
tcpdump -i any port 4317 -w otelcol-network.pcap
```

### 2. æ€§èƒ½ç›‘æ§å·¥å…·

#### è‡ªå®šä¹‰ç›‘æ§è„šæœ¬

```bash
#!/bin/bash
# performance-monitor.sh

while true; do
    # æ”¶é›†æ€§èƒ½æ•°æ®
    timestamp=$(date +%s)
    cpu_usage=$(ps aux | grep otelcol | grep -v grep | awk '{print $3}')
    memory_usage=$(ps aux | grep otelcol | grep -v grep | awk '{print $4}')
    network_io=$(cat /proc/net/dev | grep eth0 | awk '{print $2,$10}')
    
    # è¾“å‡ºåˆ°æ—¥å¿—
    echo "$timestamp,$cpu_usage,$memory_usage,$network_io" >> performance.log
    
    sleep 1
done
```

#### Grafanaä»ªè¡¨ç›˜

```json
{
  "dashboard": {
    "title": "OpenTelemetry Performance",
    "panels": [
      {
        "title": "CPU Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(otelcol_cpu_usage_percent[5m])",
            "legendFormat": "CPU Usage %"
          }
        ]
      },
      {
        "title": "Memory Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "otelcol_memory_usage_bytes",
            "legendFormat": "Memory Usage"
          }
        ]
      },
      {
        "title": "Throughput",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(otelcol_receiver_accepted_spans[5m])",
            "legendFormat": "Spans/sec"
          }
        ]
      }
    ]
  }
}
```

## æœ€ä½³å®è·µ

### 1. é…ç½®ä¼˜åŒ–

- æ ¹æ®è´Ÿè½½è°ƒæ•´æ‰¹å¤„ç†å¤§å°
- è®¾ç½®åˆé€‚çš„å†…å­˜é™åˆ¶
- é…ç½®åˆç†çš„é‡‡æ ·ç‡
- ä½¿ç”¨è¿æ¥æ± ç®¡ç†è¿æ¥
- å¯ç”¨æ•°æ®å‹ç¼©

### 2. ç›‘æ§è®¾ç½®

- ç›‘æ§å…³é”®æ€§èƒ½æŒ‡æ ‡
- è®¾ç½®æ€§èƒ½å‘Šè­¦
- å®šæœŸè¿›è¡Œå‹åŠ›æµ‹è¯•
- å»ºç«‹æ€§èƒ½åŸºçº¿
- æŒç»­ç›‘æ§æ€§èƒ½è¶‹åŠ¿

### 3. å®¹é‡è§„åˆ’

- æ ¹æ®ä¸šåŠ¡å¢é•¿è§„åˆ’å®¹é‡
- é¢„ç•™è¶³å¤Ÿçš„ç³»ç»Ÿèµ„æº
- å»ºç«‹æ‰©å®¹æµç¨‹
- å®šæœŸè¯„ä¼°å®¹é‡éœ€æ±‚
- å®æ–½å¼¹æ€§ä¼¸ç¼©

### 4. æ•…éšœé¢„é˜²

- å®šæœŸæ£€æŸ¥ç³»ç»Ÿå¥åº·
- å»ºç«‹æ•…éšœæ¢å¤æµç¨‹
- è¿›è¡Œç¾éš¾æ¢å¤æ¼”ç»ƒ
- å®æ–½ç›‘æ§å‘Šè­¦
- å»ºç«‹æ€§èƒ½ä¼˜åŒ–æµç¨‹

### 5. æŒç»­ä¼˜åŒ–

- å®šæœŸè¿›è¡Œæ€§èƒ½è¯„ä¼°
- å®æ–½æ€§èƒ½ä¼˜åŒ–æªæ–½
- ç›‘æ§ä¼˜åŒ–æ•ˆæœ
- å»ºç«‹æ€§èƒ½ä¼˜åŒ–çŸ¥è¯†åº“
- åˆ†äº«ä¼˜åŒ–ç»éªŒ
