#!/usr/bin/env python3
"""
æ–‡æ¡£é‡å¤åº¦åˆ†æå·¥å…·
ç”¨äºåˆ†ædoc_legacy_archive/å’Œdocs/çš„é‡å¤å†…å®¹
"""

import os
import hashlib
from collections import defaultdict
import json
from pathlib import Path

def calculate_file_hash(filepath):
    """è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼"""
    try:
        with open(filepath, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    except Exception as e:
        print(f"Error reading {filepath}: {e}")
        return None

def scan_directory(directory, extension='.md'):
    """æ‰«æç›®å½•ä¸­çš„æ‰€æœ‰æŒ‡å®šæ‰©å±•åæ–‡ä»¶"""
    files = []
    for root, dirs, filenames in os.walk(directory):
        for filename in filenames:
            if filename.endswith(extension):
                filepath = os.path.join(root, filename)
                files.append(filepath)
    return files

def analyze_duplicates(dir1, dir2, output_file):
    """åˆ†æä¸¤ä¸ªç›®å½•çš„é‡å¤æ–‡ä»¶"""
    print(f"å¼€å§‹åˆ†æé‡å¤åº¦...")
    print(f"ç›®å½•1: {dir1}")
    print(f"ç›®å½•2: {dir2}")
    
    hash_map = defaultdict(list)
    
    # æ‰«æç¬¬ä¸€ä¸ªç›®å½•
    print(f"\næ‰«æ {dir1}...")
    dir1_files = scan_directory(dir1)
    print(f"æ‰¾åˆ° {len(dir1_files)} ä¸ª.mdæ–‡ä»¶")
    
    for filepath in dir1_files:
        file_hash = calculate_file_hash(filepath)
        if file_hash:
            hash_map[file_hash].append(filepath)
    
    # æ‰«æç¬¬äºŒä¸ªç›®å½•
    print(f"\næ‰«æ {dir2}...")
    dir2_files = scan_directory(dir2)
    print(f"æ‰¾åˆ° {len(dir2_files)} ä¸ª.mdæ–‡ä»¶")
    
    for filepath in dir2_files:
        file_hash = calculate_file_hash(filepath)
        if file_hash:
            hash_map[file_hash].append(filepath)
    
    # è¯†åˆ«é‡å¤æ–‡ä»¶
    duplicates = {k: v for k, v in hash_map.items() if len(v) > 1}
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_files = len(dir1_files) + len(dir2_files)
    duplicate_files = sum(len(v) for v in duplicates.values())
    unique_files = total_files - duplicate_files + len(duplicates)
    duplicate_rate = (duplicate_files / total_files * 100) if total_files > 0 else 0
    
    stats = {
        "total_files": total_files,
        "dir1_files": len(dir1_files),
        "dir2_files": len(dir2_files),
        "duplicate_groups": len(duplicates),
        "duplicate_files": duplicate_files,
        "unique_files": unique_files,
        "duplicate_rate": f"{duplicate_rate:.1f}%"
    }
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š
    print(f"\nç”ŸæˆæŠ¥å‘Š: {output_file}")
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# æ–‡æ¡£é‡å¤åº¦åˆ†ææŠ¥å‘Š\n\n")
        f.write(f"**åˆ†ææ—¥æœŸ**: 2025-10-18\n")
        f.write(f"**åˆ†æå·¥å…·**: analyze_duplicates.py\n\n")
        
        f.write("## ğŸ“Š ç»Ÿè®¡æ¦‚è§ˆ\n\n")
        f.write(f"- **æ€»æ–‡ä»¶æ•°**: {stats['total_files']}\n")
        f.write(f"  - {dir1}: {stats['dir1_files']} ä¸ªæ–‡ä»¶\n")
        f.write(f"  - {dir2}: {stats['dir2_files']} ä¸ªæ–‡ä»¶\n")
        f.write(f"- **é‡å¤æ–‡ä»¶ç»„æ•°**: {stats['duplicate_groups']}\n")
        f.write(f"- **é‡å¤æ–‡ä»¶æ€»æ•°**: {stats['duplicate_files']}\n")
        f.write(f"- **å”¯ä¸€æ–‡ä»¶æ•°**: {stats['unique_files']}\n")
        f.write(f"- **é‡å¤ç‡**: {stats['duplicate_rate']}\n\n")
        
        f.write("## ğŸ” é‡å¤æ–‡ä»¶è¯¦æƒ…\n\n")
        
        if duplicates:
            for i, (hash_val, files) in enumerate(duplicates.items(), 1):
                f.write(f"### é‡å¤ç»„ #{i}\n\n")
                f.write(f"**Hash**: `{hash_val[:16]}...`\n\n")
                f.write(f"**æ–‡ä»¶æ•°**: {len(files)}\n\n")
                for filepath in files:
                    f.write(f"- `{filepath}`\n")
                f.write("\n")
        else:
            f.write("æœªå‘ç°é‡å¤æ–‡ä»¶ã€‚\n\n")
        
        f.write("## ğŸ’¡ å»ºè®®\n\n")
        if duplicate_rate > 30:
            f.write("ğŸ”´ **é«˜é‡å¤ç‡è­¦å‘Š**: é‡å¤ç‡è¶…è¿‡30%ï¼Œå»ºè®®ç«‹å³æ¸…ç†ã€‚\n\n")
            f.write("**æ¨èæ“ä½œ**:\n")
            f.write("1. è¯„ä¼°å“ªäº›æ–‡ä»¶æ˜¯æœ€æ–°ç‰ˆæœ¬\n")
            f.write("2. å°†æ—§ç‰ˆæœ¬ç§»åŠ¨åˆ°å½’æ¡£ç›®å½•\n")
            f.write("3. æ›´æ–°æ‰€æœ‰å¼•ç”¨é“¾æ¥\n")
        elif duplicate_rate > 10:
            f.write("ğŸŸ¡ **ä¸­ç­‰é‡å¤ç‡**: é‡å¤ç‡åœ¨10-30%ä¹‹é—´ï¼Œå»ºè®®é€‚åº¦æ¸…ç†ã€‚\n")
        else:
            f.write("ğŸŸ¢ **ä½é‡å¤ç‡**: é‡å¤ç‡ä½äº10%ï¼Œæ–‡æ¡£ç»“æ„è‰¯å¥½ã€‚\n")
    
    # ä¿å­˜JSONæ ¼å¼æ•°æ®
    json_file = output_file.replace('.md', '.json')
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump({
            "stats": stats,
            "duplicate_groups": len(duplicates),
            "duplicates_sample": {k: v for k, v in list(duplicates.items())[:10]}
        }, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… åˆ†æå®Œæˆï¼")
    print(f"ğŸ“„ MarkdownæŠ¥å‘Š: {output_file}")
    print(f"ğŸ“Š JSONæ•°æ®: {json_file}")
    print(f"\nå‘ç° {len(duplicates)} ç»„é‡å¤æ–‡ä»¶ï¼ˆå…± {duplicate_files} ä¸ªæ–‡ä»¶ï¼‰")
    print(f"é‡å¤ç‡: {stats['duplicate_rate']}")
    
    return duplicates, stats

if __name__ == "__main__":
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    Path(".progress").mkdir(exist_ok=True)
    
    # è¿è¡Œåˆ†æ
    try:
        duplicates, stats = analyze_duplicates(
            "doc_legacy_archive",
            "docs",
            ".progress/duplicate_analysis_report.md"
        )
        
        print("\n" + "="*60)
        print("åˆ†ææ‘˜è¦")
        print("="*60)
        for key, value in stats.items():
            print(f"{key}: {value}")
        
    except Exception as e:
        print(f"\nâŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

