#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OpenTelemetry 2025 çŸ¥è¯†ç”Ÿæ€ç®¡ç†ç³»ç»Ÿ
Knowledge Ecosystem Management System for OpenTelemetry 2025

åŠŸèƒ½ç‰¹æ€§:
- çŸ¥è¯†ä½“ç³»ç®¡ç†
- çŸ¥è¯†å…³ç³»å»ºç«‹
- çŸ¥è¯†æœç´¢åŠŸèƒ½
- çŸ¥è¯†å®Œæ•´æ€§æ£€æŸ¥
- çŸ¥è¯†æ›´æ–°ç›‘æ§
- çŸ¥è¯†è´¨é‡è¯„ä¼°
"""

import os
import sys
import json
import yaml
import time
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Set, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import re
import hashlib

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('knowledge_ecosystem.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class KnowledgeType(Enum):
    """çŸ¥è¯†ç±»å‹æšä¸¾"""
    THEORETICAL = "theoretical"
    STANDARD = "standard"
    TECHNICAL = "technical"
    PRACTICAL = "practical"
    QUALITY = "quality"
    COMMUNITY = "community"
    ACADEMIC = "academic"

class KnowledgeStatus(Enum):
    """çŸ¥è¯†çŠ¶æ€æšä¸¾"""
    ACTIVE = "active"
    DRAFT = "draft"
    ARCHIVED = "archived"
    DEPRECATED = "deprecated"

@dataclass
class KnowledgeNode:
    """çŸ¥è¯†èŠ‚ç‚¹æ•°æ®ç±»"""
    id: str
    title: str
    description: str
    knowledge_type: KnowledgeType
    status: KnowledgeStatus
    file_path: str
    content_hash: str
    created_at: str
    updated_at: str
    tags: List[str]
    dependencies: List[str]
    references: List[str]
    quality_score: float
    completeness_score: float

@dataclass
class KnowledgeRelationship:
    """çŸ¥è¯†å…³ç³»æ•°æ®ç±»"""
    source_id: str
    target_id: str
    relationship_type: str
    strength: float
    description: str
    created_at: str

class KnowledgeIndexer:
    """çŸ¥è¯†ç´¢å¼•å™¨"""
    
    def __init__(self, base_path: str = "doc/02_çŸ¥è¯†ä½“ç³»"):
        self.base_path = Path(base_path)
        self.knowledge_nodes: Dict[str, KnowledgeNode] = {}
        self.knowledge_relationships: List[KnowledgeRelationship] = []
        self.index_file = Path("data/knowledge_index.json")
    
    def build_knowledge_index(self) -> Dict[str, Any]:
        """æ„å»ºçŸ¥è¯†ç´¢å¼•"""
        logger.info("å¼€å§‹æ„å»ºçŸ¥è¯†ç´¢å¼•...")
        
        # æ¸…ç©ºç°æœ‰ç´¢å¼•
        self.knowledge_nodes.clear()
        self.knowledge_relationships.clear()
        
        # æ‰«æçŸ¥è¯†ä½“ç³»ç›®å½•
        self._scan_knowledge_directory()
        
        # å»ºç«‹çŸ¥è¯†å…³ç³»
        self._build_knowledge_relationships()
        
        # è®¡ç®—çŸ¥è¯†è´¨é‡åˆ†æ•°
        self._calculate_quality_scores()
        
        # ä¿å­˜ç´¢å¼•
        self._save_knowledge_index()
        
        logger.info(f"çŸ¥è¯†ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…±ç´¢å¼• {len(self.knowledge_nodes)} ä¸ªçŸ¥è¯†èŠ‚ç‚¹")
        
        return {
            'total_nodes': len(self.knowledge_nodes),
            'total_relationships': len(self.knowledge_relationships),
            'knowledge_types': self._get_knowledge_type_distribution(),
            'quality_distribution': self._get_quality_distribution()
        }
    
    def _scan_knowledge_directory(self):
        """æ‰«æçŸ¥è¯†ä½“ç³»ç›®å½•"""
        if not self.base_path.exists():
            logger.warning(f"çŸ¥è¯†ä½“ç³»ç›®å½•ä¸å­˜åœ¨: {self.base_path}")
            return
        
        # é€’å½’æ‰«ææ‰€æœ‰markdownæ–‡ä»¶
        for md_file in self.base_path.glob('**/*.md'):
            try:
                # è®¡ç®—ç›¸å¯¹è·¯å¾„
                rel_path = md_file.relative_to(self.base_path)
                
                # è¯»å–æ–‡ä»¶å†…å®¹
                content = md_file.read_text(encoding='utf-8')
                
                # è®¡ç®—å†…å®¹å“ˆå¸Œ
                content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()
                
                # è·å–æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯
                stat = md_file.stat()
                created_at = datetime.fromtimestamp(stat.st_ctime).isoformat()
                updated_at = datetime.fromtimestamp(stat.st_mtime).isoformat()
                
                # è§£ææ–‡æ¡£å…ƒæ•°æ®
                metadata = self._parse_document_metadata(content, str(rel_path))
                
                # åˆ›å»ºçŸ¥è¯†èŠ‚ç‚¹
                node_id = self._generate_node_id(str(rel_path))
                knowledge_node = KnowledgeNode(
                    id=node_id,
                    title=metadata['title'],
                    description=metadata['description'],
                    knowledge_type=metadata['knowledge_type'],
                    status=metadata['status'],
                    file_path=str(rel_path),
                    content_hash=content_hash,
                    created_at=created_at,
                    updated_at=updated_at,
                    tags=metadata['tags'],
                    dependencies=metadata['dependencies'],
                    references=metadata['references'],
                    quality_score=0.0,
                    completeness_score=0.0
                )
                
                self.knowledge_nodes[node_id] = knowledge_node
                
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™ {md_file}: {e}")
    
    def _parse_document_metadata(self, content: str, file_path: str) -> Dict[str, Any]:
        """è§£ææ–‡æ¡£å…ƒæ•°æ®"""
        # æå–æ ‡é¢˜
        title_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
        title = title_match.group(1) if title_match else Path(file_path).stem
        
        # æå–æè¿°ï¼ˆç¬¬ä¸€æ®µéæ ‡é¢˜æ–‡æœ¬ï¼‰
        description = ""
        lines = content.split('\n')
        for line in lines:
            line = line.strip()
            if line and not line.startswith('#') and not line.startswith('```'):
                description = line
                break
        
        # æ ¹æ®æ–‡ä»¶è·¯å¾„ç¡®å®šçŸ¥è¯†ç±»å‹
        knowledge_type = self._determine_knowledge_type(file_path)
        
        # æå–æ ‡ç­¾
        tags = self._extract_tags(content, file_path)
        
        # æå–ä¾èµ–å’Œå¼•ç”¨
        dependencies = self._extract_dependencies(content)
        references = self._extract_references(content)
        
        return {
            'title': title,
            'description': description,
            'knowledge_type': knowledge_type,
            'status': KnowledgeStatus.ACTIVE,
            'tags': tags,
            'dependencies': dependencies,
            'references': references
        }
    
    def _determine_knowledge_type(self, file_path: str) -> KnowledgeType:
        """æ ¹æ®æ–‡ä»¶è·¯å¾„ç¡®å®šçŸ¥è¯†ç±»å‹"""
        path_parts = file_path.lower().split('/')
        
        if '01_ç†è®ºåŸºç¡€å±‚' in path_parts:
            return KnowledgeType.THEORETICAL
        elif '02_æ ‡å‡†è§„èŒƒå±‚' in path_parts:
            return KnowledgeType.STANDARD
        elif '03_æŠ€æœ¯å®ç°å±‚' in path_parts:
            return KnowledgeType.TECHNICAL
        elif '04_åº”ç”¨å®è·µå±‚' in path_parts:
            return KnowledgeType.PRACTICAL
        elif '05_è´¨é‡ä¿è¯å±‚' in path_parts:
            return KnowledgeType.QUALITY
        elif '06_ç¤¾åŒºç”Ÿæ€å±‚' in path_parts:
            return KnowledgeType.COMMUNITY
        elif '07_å­¦æœ¯ç ”ç©¶å±‚' in path_parts:
            return KnowledgeType.ACADEMIC
        else:
            return KnowledgeType.TECHNICAL
    
    def _extract_tags(self, content: str, file_path: str) -> List[str]:
        """æå–æ ‡ç­¾"""
        tags = []
        
        # ä»æ–‡ä»¶è·¯å¾„æå–æ ‡ç­¾
        path_parts = file_path.split('/')
        for part in path_parts:
            if part.endswith('.md'):
                part = part[:-3]
            if part and part != 'README':
                tags.append(part)
        
        # ä»å†…å®¹ä¸­æå–æ ‡ç­¾
        tag_matches = re.findall(r'#(\w+)', content)
        tags.extend(tag_matches)
        
        # å»é‡å¹¶è¿”å›
        return list(set(tags))
    
    def _extract_dependencies(self, content: str) -> List[str]:
        """æå–ä¾èµ–å…³ç³»"""
        dependencies = []
        
        # æŸ¥æ‰¾ä¾èµ–å£°æ˜
        dep_matches = re.findall(r'ä¾èµ–[:ï¼š]\s*(.+)', content)
        for match in dep_matches:
            deps = [dep.strip() for dep in match.split(',')]
            dependencies.extend(deps)
        
        return dependencies
    
    def _extract_references(self, content: str) -> List[str]:
        """æå–å¼•ç”¨å…³ç³»"""
        references = []
        
        # æŸ¥æ‰¾å¼•ç”¨é“¾æ¥
        ref_matches = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content)
        for ref_text, ref_url in ref_matches:
            if ref_url.startswith('http'):
                references.append(ref_url)
            elif ref_url.endswith('.md'):
                references.append(ref_url)
        
        return references
    
    def _generate_node_id(self, file_path: str) -> str:
        """ç”ŸæˆèŠ‚ç‚¹ID"""
        # ä½¿ç”¨æ–‡ä»¶è·¯å¾„ç”Ÿæˆå”¯ä¸€ID
        path_hash = hashlib.md5(file_path.encode('utf-8')).hexdigest()[:8]
        return f"node_{path_hash}"
    
    def _build_knowledge_relationships(self):
        """å»ºç«‹çŸ¥è¯†å…³ç³»"""
        logger.info("å»ºç«‹çŸ¥è¯†å…³ç³»...")
        
        # åŸºäºæ–‡ä»¶è·¯å¾„å»ºç«‹å±‚çº§å…³ç³»
        for node_id, node in self.knowledge_nodes.items():
            # å»ºç«‹çˆ¶å­å…³ç³»
            parent_path = str(Path(node.file_path).parent)
            for other_id, other_node in self.knowledge_nodes.items():
                if other_id != node_id:
                    other_parent_path = str(Path(other_node.file_path).parent)
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯çˆ¶å­å…³ç³»
                    if parent_path == other_node.file_path:
                        relationship = KnowledgeRelationship(
                            source_id=node_id,
                            target_id=other_id,
                            relationship_type="parent",
                            strength=1.0,
                            description="çˆ¶å­å…³ç³»",
                            created_at=datetime.now().isoformat()
                        )
                        self.knowledge_relationships.append(relationship)
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å…„å¼Ÿå…³ç³»
                    elif parent_path == other_parent_path and parent_path != "":
                        relationship = KnowledgeRelationship(
                            source_id=node_id,
                            target_id=other_id,
                            relationship_type="sibling",
                            strength=0.8,
                            description="å…„å¼Ÿå…³ç³»",
                            created_at=datetime.now().isoformat()
                        )
                        self.knowledge_relationships.append(relationship)
            
            # åŸºäºå¼•ç”¨å»ºç«‹å…³ç³»
            for ref in node.references:
                for other_id, other_node in self.knowledge_nodes.items():
                    if other_id != node_id and ref in other_node.file_path:
                        relationship = KnowledgeRelationship(
                            source_id=node_id,
                            target_id=other_id,
                            relationship_type="reference",
                            strength=0.6,
                            description="å¼•ç”¨å…³ç³»",
                            created_at=datetime.now().isoformat()
                        )
                        self.knowledge_relationships.append(relationship)
    
    def _calculate_quality_scores(self):
        """è®¡ç®—çŸ¥è¯†è´¨é‡åˆ†æ•°"""
        for node_id, node in self.knowledge_nodes.items():
            # è®¡ç®—è´¨é‡åˆ†æ•°ï¼ˆåŸºäºå†…å®¹é•¿åº¦ã€ç»“æ„ç­‰ï¼‰
            quality_score = self._calculate_node_quality(node)
            node.quality_score = quality_score
            
            # è®¡ç®—å®Œæ•´æ€§åˆ†æ•°
            completeness_score = self._calculate_node_completeness(node)
            node.completeness_score = completeness_score
    
    def _calculate_node_quality(self, node: KnowledgeNode) -> float:
        """è®¡ç®—èŠ‚ç‚¹è´¨é‡åˆ†æ•°"""
        try:
            file_path = self.base_path / node.file_path
            if not file_path.exists():
                return 0.0
            
            content = file_path.read_text(encoding='utf-8')
            
            # åŸºäºå†…å®¹é•¿åº¦
            length_score = min(1.0, len(content) / 1000)
            
            # åŸºäºç»“æ„å®Œæ•´æ€§
            structure_score = 0.0
            if '# æ¦‚è¿°' in content:
                structure_score += 0.2
            if '## æ ¸å¿ƒå†…å®¹' in content:
                structure_score += 0.2
            if '## åº”ç”¨åœºæ™¯' in content:
                structure_score += 0.2
            if '## æ€§èƒ½ä¼˜åŒ–' in content:
                structure_score += 0.2
            if '## æµ‹è¯•ä¸éªŒè¯' in content:
                structure_score += 0.2
            
            # åŸºäºå¼•ç”¨æ•°é‡
            ref_score = min(1.0, len(node.references) / 5)
            
            # ç»¼åˆåˆ†æ•°
            total_score = (length_score * 0.3 + structure_score * 0.5 + ref_score * 0.2)
            return min(1.0, total_score)
            
        except Exception as e:
            logger.error(f"è®¡ç®—èŠ‚ç‚¹è´¨é‡åˆ†æ•°æ—¶å‡ºé”™: {e}")
            return 0.0
    
    def _calculate_node_completeness(self, node: KnowledgeNode) -> float:
        """è®¡ç®—èŠ‚ç‚¹å®Œæ•´æ€§åˆ†æ•°"""
        try:
            file_path = self.base_path / node.file_path
            if not file_path.exists():
                return 0.0
            
            content = file_path.read_text(encoding='utf-8')
            
            # æ£€æŸ¥å¿…éœ€å…ƒç´ 
            required_elements = [
                '# æ¦‚è¿°', '## æ ¸å¿ƒå†…å®¹', '## åº”ç”¨åœºæ™¯', 
                '## æ€§èƒ½ä¼˜åŒ–', '## æµ‹è¯•ä¸éªŒè¯', '## å‚è€ƒæ–‡çŒ®'
            ]
            
            found_elements = sum(1 for element in required_elements if element in content)
            completeness = found_elements / len(required_elements)
            
            return completeness
            
        except Exception as e:
            logger.error(f"è®¡ç®—èŠ‚ç‚¹å®Œæ•´æ€§åˆ†æ•°æ—¶å‡ºé”™: {e}")
            return 0.0
    
    def _get_knowledge_type_distribution(self) -> Dict[str, int]:
        """è·å–çŸ¥è¯†ç±»å‹åˆ†å¸ƒ"""
        distribution = {}
        for node in self.knowledge_nodes.values():
            type_name = node.knowledge_type.value
            distribution[type_name] = distribution.get(type_name, 0) + 1
        return distribution
    
    def _get_quality_distribution(self) -> Dict[str, int]:
        """è·å–è´¨é‡åˆ†å¸ƒ"""
        distribution = {
            'excellent': 0,  # >= 0.9
            'good': 0,       # >= 0.7
            'fair': 0,       # >= 0.5
            'poor': 0        # < 0.5
        }
        
        for node in self.knowledge_nodes.values():
            score = node.quality_score
            if score >= 0.9:
                distribution['excellent'] += 1
            elif score >= 0.7:
                distribution['good'] += 1
            elif score >= 0.5:
                distribution['fair'] += 1
            else:
                distribution['poor'] += 1
        
        return distribution
    
    def _save_knowledge_index(self):
        """ä¿å­˜çŸ¥è¯†ç´¢å¼•"""
        try:
            # åˆ›å»ºæ•°æ®ç›®å½•
            self.index_file.parent.mkdir(parents=True, exist_ok=True)
            
            # å‡†å¤‡ç´¢å¼•æ•°æ®
            index_data = {
                'metadata': {
                    'created_at': datetime.now().isoformat(),
                    'total_nodes': len(self.knowledge_nodes),
                    'total_relationships': len(self.knowledge_relationships)
                },
                'nodes': {node_id: asdict(node) for node_id, node in self.knowledge_nodes.items()},
                'relationships': [asdict(rel) for rel in self.knowledge_relationships]
            }
            
            # ä¿å­˜ç´¢å¼•
            with open(self.index_file, 'w', encoding='utf-8') as f:
                json.dump(index_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"çŸ¥è¯†ç´¢å¼•å·²ä¿å­˜åˆ°: {self.index_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜çŸ¥è¯†ç´¢å¼•å¤±è´¥: {e}")

class KnowledgeSearcher:
    """çŸ¥è¯†æœç´¢å™¨"""
    
    def __init__(self, index_file: str = "data/knowledge_index.json"):
        self.index_file = Path(index_file)
        self.knowledge_nodes: Dict[str, KnowledgeNode] = {}
        self.knowledge_relationships: List[KnowledgeRelationship] = []
        self._load_knowledge_index()
    
    def _load_knowledge_index(self):
        """åŠ è½½çŸ¥è¯†ç´¢å¼•"""
        try:
            if self.index_file.exists():
                with open(self.index_file, 'r', encoding='utf-8') as f:
                    index_data = json.load(f)
                
                # åŠ è½½çŸ¥è¯†èŠ‚ç‚¹
                for node_id, node_data in index_data.get('nodes', {}).items():
                    node = KnowledgeNode(**node_data)
                    self.knowledge_nodes[node_id] = node
                
                # åŠ è½½çŸ¥è¯†å…³ç³»
                for rel_data in index_data.get('relationships', []):
                    relationship = KnowledgeRelationship(**rel_data)
                    self.knowledge_relationships.append(relationship)
                
                logger.info(f"çŸ¥è¯†ç´¢å¼•åŠ è½½å®Œæˆï¼Œå…± {len(self.knowledge_nodes)} ä¸ªèŠ‚ç‚¹")
            else:
                logger.warning("çŸ¥è¯†ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨")
                
        except Exception as e:
            logger.error(f"åŠ è½½çŸ¥è¯†ç´¢å¼•å¤±è´¥: {e}")
    
    def search_knowledge(self, query: str, search_type: str = "all") -> List[Dict[str, Any]]:
        """æœç´¢çŸ¥è¯†"""
        results = []
        query_lower = query.lower()
        
        for node_id, node in self.knowledge_nodes.items():
            score = 0.0
            
            # æ ‡é¢˜åŒ¹é…
            if query_lower in node.title.lower():
                score += 0.4
            
            # æè¿°åŒ¹é…
            if query_lower in node.description.lower():
                score += 0.3
            
            # æ ‡ç­¾åŒ¹é…
            for tag in node.tags:
                if query_lower in tag.lower():
                    score += 0.2
            
            # æ–‡ä»¶è·¯å¾„åŒ¹é…
            if query_lower in node.file_path.lower():
                score += 0.1
            
            # æ ¹æ®æœç´¢ç±»å‹è¿‡æ»¤
            if search_type != "all" and node.knowledge_type.value != search_type:
                continue
            
            if score > 0:
                results.append({
                    'node_id': node_id,
                    'title': node.title,
                    'description': node.description,
                    'file_path': node.file_path,
                    'knowledge_type': node.knowledge_type.value,
                    'quality_score': node.quality_score,
                    'completeness_score': node.completeness_score,
                    'tags': node.tags,
                    'score': score
                })
        
        # æŒ‰åˆ†æ•°æ’åº
        results.sort(key=lambda x: x['score'], reverse=True)
        return results
    
    def find_related_knowledge(self, node_id: str, max_depth: int = 2) -> List[Dict[str, Any]]:
        """æŸ¥æ‰¾ç›¸å…³çŸ¥è¯†"""
        related_nodes = set()
        visited = set()
        
        def traverse_relationships(current_id: str, depth: int):
            if depth > max_depth or current_id in visited:
                return
            
            visited.add(current_id)
            
            # æŸ¥æ‰¾ç›´æ¥å…³ç³»
            for rel in self.knowledge_relationships:
                if rel.source_id == current_id:
                    related_nodes.add(rel.target_id)
                    traverse_relationships(rel.target_id, depth + 1)
                elif rel.target_id == current_id:
                    related_nodes.add(rel.source_id)
                    traverse_relationships(rel.source_id, depth + 1)
        
        traverse_relationships(node_id, 0)
        
        # è¿”å›ç›¸å…³çŸ¥è¯†èŠ‚ç‚¹
        results = []
        for related_id in related_nodes:
            if related_id in self.knowledge_nodes:
                node = self.knowledge_nodes[related_id]
                results.append({
                    'node_id': related_id,
                    'title': node.title,
                    'description': node.description,
                    'file_path': node.file_path,
                    'knowledge_type': node.knowledge_type.value,
                    'quality_score': node.quality_score
                })
        
        return results

class KnowledgeEcosystemManager:
    """çŸ¥è¯†ç”Ÿæ€ç®¡ç†ä¸»ç±»"""
    
    def __init__(self, base_path: str = "doc/02_çŸ¥è¯†ä½“ç³»"):
        self.base_path = Path(base_path)
        self.indexer = KnowledgeIndexer(base_path)
        self.searcher = KnowledgeSearcher()
    
    def run_knowledge_management(self) -> Dict[str, Any]:
        """è¿è¡ŒçŸ¥è¯†ç®¡ç†"""
        logger.info("å¼€å§‹è¿è¡ŒçŸ¥è¯†ç”Ÿæ€ç®¡ç†...")
        
        # 1. æ„å»ºçŸ¥è¯†ç´¢å¼•
        index_results = self.indexer.build_knowledge_index()
        
        # 2. é‡æ–°åŠ è½½æœç´¢å™¨
        self.searcher = KnowledgeSearcher()
        
        # 3. ç”ŸæˆçŸ¥è¯†ç”Ÿæ€æŠ¥å‘Š
        report_content = self._generate_knowledge_ecosystem_report(index_results)
        
        # 4. ä¿å­˜æŠ¥å‘Š
        self._save_knowledge_report(report_content)
        
        logger.info("çŸ¥è¯†ç”Ÿæ€ç®¡ç†å®Œæˆ")
        
        return {
            'index_results': index_results,
            'report_content': report_content
        }
    
    def _generate_knowledge_ecosystem_report(self, index_results: Dict[str, Any]) -> str:
        """ç”ŸæˆçŸ¥è¯†ç”Ÿæ€æŠ¥å‘Š"""
        report_content = f"""# OpenTelemetry 2025 çŸ¥è¯†ç”Ÿæ€ç®¡ç†æŠ¥å‘Š

## ğŸ“Š æŠ¥å‘Šæ¦‚è¿°

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**æŠ¥å‘Šç‰ˆæœ¬**: 1.0.0
**çŸ¥è¯†èŠ‚ç‚¹æ€»æ•°**: {index_results.get('total_nodes', 0)}
**çŸ¥è¯†å…³ç³»æ€»æ•°**: {index_results.get('total_relationships', 0)}

## ğŸ§  çŸ¥è¯†ä½“ç³»åˆ†æ

### çŸ¥è¯†ç±»å‹åˆ†å¸ƒ
"""
        
        # æ·»åŠ çŸ¥è¯†ç±»å‹åˆ†å¸ƒ
        type_distribution = index_results.get('knowledge_types', {})
        for type_name, count in type_distribution.items():
            report_content += f"- **{type_name}**: {count} ä¸ªèŠ‚ç‚¹\n"
        
        # æ·»åŠ è´¨é‡åˆ†å¸ƒ
        report_content += "\n### çŸ¥è¯†è´¨é‡åˆ†å¸ƒ\n"
        quality_distribution = index_results.get('quality_distribution', {})
        for quality_level, count in quality_distribution.items():
            report_content += f"- **{quality_level}**: {count} ä¸ªèŠ‚ç‚¹\n"
        
        # æ·»åŠ çŸ¥è¯†ç”Ÿæ€å»ºè®®
        report_content += "\n## ğŸ’¡ çŸ¥è¯†ç”Ÿæ€ä¼˜åŒ–å»ºè®®\n\n"
        suggestions = self._generate_optimization_suggestions(index_results)
        for i, suggestion in enumerate(suggestions, 1):
            report_content += f"{i}. {suggestion}\n"
        
        # æ·»åŠ ç»“è®º
        report_content += f"""

## ğŸ“‹ ç»“è®º

OpenTelemetry 2025çŸ¥è¯†ç”Ÿæ€ä½“ç³»æ„å»º{'æˆåŠŸ' if index_results.get('total_nodes', 0) > 0 else 'éœ€è¦å®Œå–„'}ï¼Œå…±åŒ…å« {index_results.get('total_nodes', 0)} ä¸ªçŸ¥è¯†èŠ‚ç‚¹å’Œ {index_results.get('total_relationships', 0)} ä¸ªçŸ¥è¯†å…³ç³»ã€‚

### ä¸»è¦æˆå°±
- çŸ¥è¯†ä½“ç³»ç»“æ„å®Œæ•´ï¼Œè¦†ç›–äº†ä»ç†è®ºåŸºç¡€åˆ°å®é™…åº”ç”¨çš„å„ä¸ªå±‚é¢
- çŸ¥è¯†èŠ‚ç‚¹è´¨é‡æ•´ä½“è¾ƒé«˜ï¼Œå†…å®¹è¯¦å®
- çŸ¥è¯†å…³ç³»å»ºç«‹å®Œå–„ï¼Œä¾¿äºçŸ¥è¯†å‘ç°å’Œå…³è”

### éœ€è¦å…³æ³¨çš„é—®é¢˜
- éƒ¨åˆ†çŸ¥è¯†èŠ‚ç‚¹éœ€è¦è¿›ä¸€æ­¥å®Œå–„
- æŸäº›çŸ¥è¯†å…³ç³»éœ€è¦åŠ å¼º
- éœ€è¦å»ºç«‹æ›´å®Œå–„çš„çŸ¥è¯†æ›´æ–°æœºåˆ¶

### ä¸‹ä¸€æ­¥è¡ŒåŠ¨
1. å®Œå–„ä½è´¨é‡çš„çŸ¥è¯†èŠ‚ç‚¹
2. åŠ å¼ºçŸ¥è¯†å…³ç³»çš„å»ºç«‹
3. å»ºç«‹å®šæœŸçš„çŸ¥è¯†æ›´æ–°æœºåˆ¶
4. åŠ å¼ºçŸ¥è¯†æœç´¢å’Œå‘ç°åŠŸèƒ½

---
*æœ¬æŠ¥å‘Šç”±OpenTelemetry 2025çŸ¥è¯†ç”Ÿæ€ç®¡ç†ç³»ç»Ÿç”Ÿæˆ*
"""
        
        return report_content
    
    def _generate_optimization_suggestions(self, index_results: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        # åŸºäºçŸ¥è¯†ç±»å‹åˆ†å¸ƒçš„å»ºè®®
        type_distribution = index_results.get('knowledge_types', {})
        if type_distribution.get('theoretical', 0) < 5:
            suggestions.append("å¢åŠ ç†è®ºåŸºç¡€å±‚çŸ¥è¯†èŠ‚ç‚¹ï¼Œå®Œå–„ç†è®ºä½“ç³»")
        
        if type_distribution.get('practical', 0) < 10:
            suggestions.append("å¢åŠ åº”ç”¨å®è·µå±‚çŸ¥è¯†èŠ‚ç‚¹ï¼Œæä¾›æ›´å¤šå®é™…æ¡ˆä¾‹")
        
        # åŸºäºè´¨é‡åˆ†å¸ƒçš„å»ºè®®
        quality_distribution = index_results.get('quality_distribution', {})
        if quality_distribution.get('poor', 0) > 0:
            suggestions.append(f"æ”¹è¿› {quality_distribution['poor']} ä¸ªä½è´¨é‡çŸ¥è¯†èŠ‚ç‚¹")
        
        if quality_distribution.get('excellent', 0) < 5:
            suggestions.append("æé«˜çŸ¥è¯†èŠ‚ç‚¹è´¨é‡ï¼Œå¢åŠ ä¼˜ç§€çŸ¥è¯†èŠ‚ç‚¹æ•°é‡")
        
        # åŸºäºå…³ç³»æ•°é‡çš„å»ºè®®
        total_relationships = index_results.get('total_relationships', 0)
        total_nodes = index_results.get('total_nodes', 0)
        if total_nodes > 0 and total_relationships / total_nodes < 2:
            suggestions.append("å¢åŠ çŸ¥è¯†å…³ç³»ï¼Œæé«˜çŸ¥è¯†å…³è”åº¦")
        
        return suggestions
    
    def _save_knowledge_report(self, report_content: str):
        """ä¿å­˜çŸ¥è¯†æŠ¥å‘Š"""
        try:
            # åˆ›å»ºæŠ¥å‘Šç›®å½•
            report_dir = Path("reports")
            report_dir.mkdir(exist_ok=True)
            
            # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_file = report_dir / f"knowledge_ecosystem_report_{timestamp}.md"
            
            # ä¿å­˜æŠ¥å‘Š
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            logger.info(f"çŸ¥è¯†ç”Ÿæ€æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜çŸ¥è¯†ç”Ÿæ€æŠ¥å‘Šå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºçŸ¥è¯†ç”Ÿæ€ç®¡ç†å®ä¾‹
        manager = KnowledgeEcosystemManager()
        
        # è¿è¡ŒçŸ¥è¯†ç®¡ç†
        results = manager.run_knowledge_management()
        
        # è¾“å‡ºç®€è¦ç»“æœ
        print("\n" + "="*60)
        print("OpenTelemetry 2025 çŸ¥è¯†ç”Ÿæ€ç®¡ç†ç»“æœ")
        print("="*60)
        
        index_results = results['index_results']
        print(f"ğŸ§  çŸ¥è¯†èŠ‚ç‚¹æ€»æ•°: {index_results.get('total_nodes', 0)}")
        print(f"ğŸ”— çŸ¥è¯†å…³ç³»æ€»æ•°: {index_results.get('total_relationships', 0)}")
        
        # æ˜¾ç¤ºçŸ¥è¯†ç±»å‹åˆ†å¸ƒ
        type_distribution = index_results.get('knowledge_types', {})
        print("\nğŸ“Š çŸ¥è¯†ç±»å‹åˆ†å¸ƒ:")
        for type_name, count in type_distribution.items():
            print(f"  - {type_name}: {count}")
        
        # æ˜¾ç¤ºè´¨é‡åˆ†å¸ƒ
        quality_distribution = index_results.get('quality_distribution', {})
        print("\nğŸ“ˆ è´¨é‡åˆ†å¸ƒ:")
        for quality_level, count in quality_distribution.items():
            print(f"  - {quality_level}: {count}")
        
        print("\nè¯¦ç»†æŠ¥å‘Šå·²ç”Ÿæˆï¼Œè¯·æŸ¥çœ‹ reports/ ç›®å½•")
        print("="*60)
        
    except Exception as e:
        logger.error(f"è¿è¡ŒçŸ¥è¯†ç”Ÿæ€ç®¡ç†æ—¶å‡ºé”™: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
